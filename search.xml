<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[添加linux开机启动项]]></title>
    <url>%2Flinux-boot-add%2F</url>
    <content type="text"><![CDATA[摘要：以往的服务通常使用supervisord或者crontab的方式来守护或者定时执行，最近遇到了自己写的服务所依赖到一些组件服务，在开机的没有自动启动的问题，所以这次对添加linux开机自启动的一些方式做一个总结。让自己的服务可以做到无人托管开机自启动，减少运营成本的同时可以防止有人偷偷写入开机启动项，启动一些没有必要的服务。用户自定义开机程序(/etc/rc.d/rc.local)这些就是开机的时候会自动执行的脚本和命令了。这个时候就有人问了，我以前写的是/etc/rc.local这个文件也没写过你说的/etc/rc.d/rc.local文件啊。这是因为/etc/rc.local软链接到/etc/rc.d/rc.local，所以写到哪个文件里都是一样的写在这个文件里的开机脚本都是默认后台执行的，不需要再加&amp;符号了自写脚本登陆自动执行(/etc/profile.d/)，不是开机自动执行目录中存放的是一些应用程序所需的启动脚本，其中包括了颜色、语言、less、vim及which等命令的一些附加设置。当一个用户登录Linux系统或使用su -命令切换到另一个用户时，也就是Login shell 启动时，首先要确保执行的启动脚本就是 /etc/profile 。（只有Login shell启动时才会运行 /etc/profile 这个脚本，也就是需要输入账号和密码的时候，而Non-login shell 不会调用这个脚本）这些脚本文件之所以能够 被自动执行，是因为在/etc/profile中使用一个for循环语句来调用这些脚本。而这些脚本文件是用来设置一些变量和运行一些初始化过程的。chkconfig命令设置 可设置优先级别/etc/rc.d/init.d目录为什么要介绍/etc/rc.local/init.d目录是因为要使用chkconfig来管理自动启动的脚本，首先将启动文件cp到 /etc/init.d/或者/etc/rc.d/init.d/（前者是后者的软连接）下才可以12345678910[root@VM_0_15_centos init.d]# lsabrt-ccpp atd cloud-config crond iptables lvm2-lvmetad netconsole ntpd psacct restorecond singleabrtd auditd cloud-final functions irqbalance lvm2-monitor netfs ntpdate quota_nld rsyslog sshdabrt-oops blk-availability cloud-init halt kdump mdmonitor network postfix rdisc sandbox udev-postacpid bootlocal cloud-init-local ip6tables killall messagebus nfs-rdma pptpd rdma saslauthd YDService[root@VM_0_15_centos init.d]# cp pptpd pptpd2[root@VM_0_15_centos init.d]# service pptpd2 statuspptpd (pid 1324) is running...[root@VM_0_15_centos init.d]# /etc/rc.d/init.d/pptpd2 statuspptpd (pid 1324) is running...此目录下的脚本会被提供给service或者systemctl使用一般存在以下命令start、stop、reload、restart、force-reload大多数的情况下，你会使用到start,stop,restart选项当然了要使用init.d目录下的脚本，你需要有root权限或sudo权限。每个脚本都将被作为一个命令运行，每个脚本也至少需要755权限。/etc/init.d指向/etc/rc.d/init.d目录chkconfig如何添加一个服务增加服务的步骤：服务脚本必须存放在/etc/ini.d/目录下；chkconfig --add servicename在chkconfig工具服务列表中增加此服务，此时服务会被在/etc/rc.d/rcN.d中赋予K/S入口了；chkconfig --level 35 mysqld on修改服务的默认启动等级。123456789[root@VM_0_15_centos init.d]# vim pptpd#!/bin/sh## Startup script for pptpd## chkconfig: - 85 15# description: PPTP server# processname: pptpd# config: /etc/pptpd.conf脚本文件前面务必添加如下三行代码，否侧会提示chkconfig不支持123#!/bin/sh 告诉系统使用的shell,所以的shell脚本都是这样#chkconfig: 35 20 80 分别代表运行级别，启动优先权，关闭优先权，此行代码必须#description: http server（自己随便发挥）此行代码必须chkconfig –add 脚本文件名 操作后就已经添加了使用范例：12345678chkconfig --list #列出所有的系统服务chkconfig --add httpd #增加httpd服务chkconfig --del httpd #删除httpd服务chkconfig --level httpd 2345 on #设置httpd在运行级别为2、3、4、5的情况下都是on（开启）的状态chkconfig --list #列出系统所有的服务启动情况chkconfig --list mysqld #列出mysqld服务设置情况chkconfig --level 35 mysqld on #设定mysqld在等级3和5为开机运行服务，--level 35表示操作只在等级3和5执行，on表示启动，off表示关闭chkconfig mysqld on #设定mysqld在各等级为on，“各等级”包括2、3、4、5等级运行级别代表是何等级时可以在开机时自动运行此服务1234567运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆运行级别2：多用户状态(没有联网NFS)运行级别3：完全的多用户状态(有联网NFS)，登陆后进入控制台命令行模式运行级别4：系统未使用，保留运行级别5：X11控制台，登陆后进入图形GUI模式运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动需要说明的是，level选项可以指定要查看的运行级而不一定是当前运行级。对于每个运行级，只能有一个启动脚本或者停止脚本。当切换运行级时，init不会重新启动已经启动的服务，也不会再次去停止已经停止的服务。如果默认情况下，服务不应在任何运行级别中启动，则应使用-代替运行级别列表。通常自己的服务使用等级35就可以了,最多2345也就是chkconfig servicename onntsysv：类图形界面管理模式来设置开机启动 （RedHat特有）Linux ntsysv命令用于设置系统的各种服务。这是Red Hat公司遵循GPL规则所开发的程序，它具有互动式操作界面，您可以轻易地利用方向键和空格键等，开启，关闭操作系统在每个执行等级中，所要执行的系统服务。这种方式不算通用，所以我这里就不赘述了，有兴趣可以移步Linux ntsysv：设置系统服务总结开机启动项添加脚本到或者命令 /etc/rc.d/rc.local先把脚本放到/etc/rc.d/init.d下，再使用chkconfig servername on命令添加2345运行级别到开机启动项（脚本写的规则见详情）ntsysv不常用可以忽略使用密码登陆时运行把脚本放到/etc/profile.d/下引用linux下三种开机自启动服务的方式总结linux 添加开机启动项的三种方法。Linux ntsysv命令/etc/profile 文件和/etc/profile.dchkconfig 服务的添加、顺序Linux 系统开机启动项清理]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程启动时间采集（linux篇）]]></title>
    <url>%2Fprocess-start-time-linux%2F</url>
    <content type="text"><![CDATA[摘要：这次我们来谈谈如何采集一个进程的启动时间以下内容使用go语言实现linux 进程启动时间采集方法一直接读取/proc/{pid} 文件夹的时间戳方式（不准确但效率高），以下是方法一的代码，但是我没有验证过12345var stat os.FileInfoif stat, err = os.Lstat(fmt.Sprintf(&quot;/proc/%v&quot;, pid)); err != nil &#123; return nil&#125;proc.mtime = stat.ModTime().Unix()方法二使用(现在的时间-从系统启动到现在的时间) + 进程启动时距离系统启动时的时间间隔得到。项目目录结构123456789101112update_server/|-- bin| `-- go_build_main|-- main|-- pkg| `-- darwin_amd64| `-- webtest.a`-- src |-- main | `-- main.go `-- webtest `-- webtest.gomain1234567891011package mainimport ( &quot;fmt&quot; &quot;webtest&quot;)func main() &#123; webtest.Init() fmt.Println(&quot;start time:&quot;,webtest.ProcessStartTime(1726))&#125;工具类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package webtest/*#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;pwd.h&gt;#include &lt;stdlib.h&gt;*/import &quot;C&quot;import ( &quot;fmt&quot; &quot;io/ioutil&quot; &quot;strconv&quot; &quot;strings&quot; &quot;time&quot;)var ( Uptime int64 // 系统启动时间戳 scClkTck = int64(C.sysconf(C._SC_CLK_TCK)))func Init() &#123; buf, err := ioutil.ReadFile(&quot;/proc/uptime&quot;) Uptime = time.Now().Unix() if err != nil &#123; fmt.Println(&quot;read file /proc/uptime faile 1&quot;) &#125; if fields := strings.Fields(string(buf)); len(fields) == 2 &#123; start, err := strconv.ParseFloat(fields[0], 10) if err == nil &#123; Uptime = time.Now().Unix() - int64(start)//- sys.Uptime &#125;else&#123; fmt.Println(&quot;read file /proc/uptime faile 2&quot;) &#125; &#125;&#125;func ProcessStartTime(pid int) (ts time.Time) &#123; buf, err := ioutil.ReadFile(fmt.Sprintf(&quot;/proc/%v/stat&quot;, pid)) if err != nil &#123; return time.Unix(0, 0) &#125; if fields := strings.Fields(string(buf)); len(fields) &gt; 22 &#123; start, err := strconv.ParseInt(fields[21], 10, 0) if err == nil &#123; if scClkTck &gt; 0 &#123; return time.Unix(Uptime+(start/scClkTck), 0) &#125; return time.Unix(Uptime+(start/100), 0) &#125; &#125; return time.Unix(0, 0)&#125;/proc/uptime中采集到的值是jiffies单位，用来记录自系统启动以来产生的节拍的总数。启动时，内核将该变量初始化为0，此后，每次时钟中断处理程序都会增加该变量的值。一秒内时钟中断的次数等于Hz，所以jiffies一秒内增加的值也就是Hz。输出结果总结刚刚开始接触go语言有点激动，发现go语言中没有类的概念，一时还不知道go相比python、c++、java而言比较大的优点在哪，比较好的一点就是可以直接编译成二进制可执行程序，不用安装各种库，简单看完了golang的入门教程，大概知道怎么写go语言的代码了，接下来要再把书温习一翻，再多动手写点代码以后体系的总结下。引用go 获得进程启动时间的两种方法]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>linux</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何无网络下免编译的安装程序（RPM包）]]></title>
    <url>%2Frpmbuild-01%2F</url>
    <content type="text"><![CDATA[摘要：rpm包是预先在Linux主机上编译好并打包的文件，安装起来非常快捷。不需要再进行繁琐的编译等操作，拿来就能用，区别于yum包管理，rpm可以在不需要网络环境的情况下使用。学习完大神的文章还是有一些细节不太清楚，下面我就再对编译RPM包的过程和详细知识点再梳理扩展一下留作备忘。简介redhat包管理器rpm（redhat package manager），因为非常的方便，所以这个方式被广泛的使用现在rpm的英文翻译是一种递归写法（RPM package manager）。RPM是以一种数据库记录的方式来将所需要的套件安装在Linux主机的一套管理程序。也就是说Linux系统中存在一个关于rpm的数据库，它记录了安装的包与包之间的依赖相关性。我们可以在有网的环境下载rpm包，也可以定制化的将自己的软件打成rpm包。准备rpm打包环境我这里用的操作系统是CentOS6.7，redhat系的其它发行版应该也类似。安装rpm-build1sudo yum install -y gcc make rpm-build redhat-rpm-config vim lrzsz在无网络的机器上装备环境需要在无网络的情况下解决一些依赖包的环境可以使用以下命令提前在一台有网络的机器上下载好RPM包再到无网络的机器上安装就可以了1yum install --downloadonly --downloaddir=&lt;目录&gt; &lt;packages&gt;创建必须的文件夹和文件1mkdir -p ~/rpmbuild/&#123;BUILD,RPMS,SOURCES,SPECS,SRPMS&#125;echo &apos;%\_topdir %(echo $HOME)/rpmbuild&apos; &gt; ~/.rpmmacros文件名类型说明SPECS目录包含rpm的xxx.spec文件(打包的描述文件)SOURCES目录包含源码包(如.tar包)和所有patch补丁、service启动文件等BUILD目录源码包被解压至此, 并在该目录的子目录完成编译BUILDROOT目录存放编译后的文件的临时目录(保存%install阶段安装的文件)RPMS目录经过编译成功后, 打包文件放在这个目录, 包含i386、i686、noarch等次级目录SRPMS目录包含.src.rpm的SPRM包(通常用于审核和升级软件包)制作spec文件找spec模板文件一般找一个类似的rpm源码包，将其安装，然后参照它写自己软件包的spec文件。1234mkdir ~/rpmswget -O ~/rpms/python-2.6.6-64.el6.src.rpm http://vault.centos.org/6.7/os/Source/SPackages/python-2.6.6-64.el6.src.rpmrpm -ivh ~/rpms/python-2.6.6-64.el6.src.rpmvim ~/rpmbuild/SPECS/python.spec # 参照这个文件来写自己软件包的spec文件rpmbuild内置变量rpmbuild --showrc可查看内置的变量常用的几个变量:123456789$RPM_BUILD_DIR /$HOME/rpmbuild/BUILD$RPM_BUILD_ROOT /$HOME/rpmbuild/BUILDROOT%&#123;_sysconfigdir&#125; /etc%&#123;_sbindir&#125; /usr/sbin%&#123;_bindir&#125; /usr/bin%&#123;_prefix&#125; /usr%&#123;_localstatedir&#125; /var更多的可以查看 --showrcrpmbuild spec规范说明1234567891011121314151617181920212223242526Summary: 软件包的内容概要描述Name: 软件包的名称(spec文件名与其一致)Version: 软件的实际版本号，具体和源码包一致Release: 软件包的发布实际版本号Url: 软件的主页License: 软件授权方式(GPL等)Group: 软件分组，如(Application/System)Source: 软件的来源Patch: 补丁patch file依赖的软件，安装的时候需要检查的BuildRoot: 安装或者编译时使用的&quot;虚拟目录&quot;，一般默认 BuildRequires: 编译过程中需要的软件Requires: 依赖的软件, 安装的时候需要检查的Packager: 软件的打包者Vendor: 软件发行商或者打包组织信息,如(Apache Software Foundation)%description: 软件包描述%prep: 编译前预处理。如: 1.打补丁; 2. 解压源码等%setup: 解压源码(一般是位于%prep下的macro, 自动解压源码)%build: 编译%install: 安装, 即把一些可执行文件和配置复制到目标目录中。%clean: 清理一些暂存文件%files: 定义哪些文件或者目录会放入rpm中%pre: rpm安装前执行的动作%post: rpm安装后执行的动作%preun: 卸载前执行脚本程序(preun)开始执行%postun: 卸载后执行脚本程序(postun)开始执行更多参考: How_to_create_an_RPM_packagerpmbuild spec 示例示例1 openstack-aodh.spec12345678910111213141516171819202122232425262728293031323334%global pypi_name aodh%&#123;!?upstream_version: %global upstream_version %&#123;version&#125;%&#123;?milestone&#125;&#125;Name: openstack-aodhVersion: 2.0.0Release: %&#123;es_versions&#125;%&#123;?dist&#125;Summary: OpenStack Telemetry AlarmingLicense: ASL 2.0URL: https://github.com/openstack/aodh.gitBuildArch: noarchSource0: http://tarballs.openstack.org/%&#123;pypi_name&#125;/%&#123;pypi_name&#125;-%&#123;version&#125;.tar.gz...%descriptionAodh is the alarm engine of the Ceilometer project.%prep%setup -q -n %&#123;pypi_name&#125;-%&#123;upstream_version&#125;%build%&#123;__python2&#125; setup.py build%install%&#123;__python2&#125; setup.py install --skip-buid --root %&#123;buildroot&#125;%files -n python-aodh%&#123;python2_sitelib&#125;/aodh... %files common%doc README.rst%dir %&#123;_sysconfdir&#125;/aodh%attr(-, root, aodh) %&#123;_datadir&#125;/aodh/aodh-dist.conf%config(noreplace) %attr(-, root, aodh) %&#123;_sysconfdir&#125;/aodh/aodh.conf...%changelog示例2 mariadb.spec1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283%define debug_package %&#123;nil&#125;%define install_dir /usr/local/mariadbName: mariadbVersion: 10.1.20Release: 1%&#123;?dist&#125;Summary: Mariadb client programs and shared libraries and server programsGroup: Applications/DatabasesLicense: GPLv2 with exceptionsURL: https://mariadb.org/Source0: mariadb-10.1.20.tar.gzBuildRoot: %&#123;_tmppath&#125;/%&#123;name&#125;-%&#123;version&#125;-%&#123;release&#125;-rootBuildRequires: readline-devel, openssl-develBuildRequires: gcc-c++, ncurses-devel, zlib-develBuildRequires: libtool, automake, autoconf, gawkRequires: grep, automake, autoconf, bashAutoreq: 0%descriptionMariaDB Server is one of the most popular database servers in the world. It’s made by the original developers of MySQL and guaranteed to stay open source. Notable users include Wikipedia, WordPress.com and Google.%prep%setup -q%buildcmake . -DMYSQL_UNIX_ADDR=/tmp/mariadb.sock \ -DEXTRA_CHARSETS=all \ -DMYSQL_USER=mysql \ -DCMAKE_INSTALL_PREFIX=%&#123;install_dir&#125; \ -DMYSQL_DATADIR=/data/mariadb \ -DWITH_XTRADB_STORAGE_ENGINE=1 \ -DWITH_FEDERATEDX_STORAGE_ENGINE=1 \ -DWITH_ARCHIVE_STORAGE_ENGINE=1 \ -DWITH_MYISAM_STORAGE_ENGINE=1 \ -DWITH_INNOBASE_STORAGE_ENGINE=1 \ -DWITH_ARCHIVE_STPRAGE_ENGINE=1 \ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \ -DWITH_SSL=system \ -DVITH_ZLIB=system \ -DWITH_LOBWRAP=0 \ -DDEFAULT_CHARSET=utf8 \ -DDEFAULT_COLLATION=utf8_general_ci \ -DENABLED_LOCAL_INFILE=ON \ -DWITH_PARTITION_STORAGE_ENGINE=1 \ -DWITH_PERFSCHEMA_STORAGE_ENGINE=1 \ -DWITH_READLINE=ON \ -DWITH_WSREP=ON \ -DWITH_INNODB_DISALLOW_WRITES=ON \ -DWITH_TOKUDB=1 \ -DCMAKE_CXX_COMPILER:FILEPATH=/usr/bin/g++ \ -DCMAKE_C_COMPILER:FILEPATH=/usr/bin/gcc \ -DCMAKE_BUILD_TYPE:STRING=RELEASEmake %&#123;?_smp_mflags&#125;%installrm -rf %&#123;buildroot&#125;make install DESTDIR=%&#123;buildroot&#125;%pregroupadd -r mysql 2&gt; /dev/nulluseradd -g mysql -r -M -s /sbin/nologin mysql 2&gt; /dev/nullmkdir -p /data/mariadb/data 2&gt; /dev/nullmkdir -p /data/mariadb/binlog 2&gt; /dev/nullmkdir -p /data/mariadb/undolog 2&gt; /dev/nullmkdir -p /var/run/mariadb/ 2&gt; /dev/nullmkdir -p /var/log/mariadb/ 2&gt; /dev/nullchown -R mysql.mysql /var/run/mariadb/ 2&gt; /dev/nullchown -R mysql.mysql /var/log/mariadb/ 2&gt; /dev/nullchown -R mysql.mysql /data/mariadb 2&gt; /dev/null%clean rm -rf %&#123;buildroot&#125; %files%defattr(-, root, root, 0755)%&#123;install_dir&#125;/%doc%changelogspec更加详细内容见 这里制作rpm包上传必要的source文件1cp $&#123;some_where&#125;/Python-2.7.10.tgz ~/rpmbuild/SOURCES/开始制作12cd ~/rpmbuildrpmbuild -bb --target x86_64 SPECS/python27-tstack.spec &gt; rpmbuild.log &amp;一切顺利的话，最终会在~/rpmbuild/RPMS/x86_64/目录下找到编译好的rpm包。技巧总结不打debug的rpm包:在spec文件中加入%debug\_package %{nil}即可禁止自动分析源码添加不应该加入的依赖 在spec文件中加入Autoreq: 0即可sepc文件中一些宏的用法 在spec文件中经常出现一些宏，比如%setup、%patch，这两个宏的选项较多，使用时要特别注意，参见这里安装卸载rpm包前后的动作 可以通过%pre, %post, %preun, %postun指定rpm包在安装卸载前后的动作，比如在安装前用脚本做一些准备、在安装后用脚本做一些初始化动作、在卸载前用脚本做一些准备、在卸载后用脚本做一些清理动作rpmbuild命令的选项 rpmbuild命令有不少选项，参见这里，用得比较多的有：1234567-bp 只解压源码及应用补丁-bc 只进行编译-bi 只进行安装到%&#123;buildroot&#125;-bb 只生成二进制rpm包-bs 只生成源码rpm包-ba 生成二进制rpm包和源码rpm包--target 指定生成rpm包的平台，默认会生成i686和x86_64的rpm包，但一般我只需要x86_64的rpm包RPM 常用参数备忘123456789101112rpm -ivh ***.rpm: 安装软件(并且显示安装进度 --install--verbose--hash)rpm -Uvh ***.rpm: 升级软件(--Update)rpm -e: 卸载软件rpm -q [软件名称]: 查询程序是否安装rpm -qa: 查询已经安装的所有软件(Query All)rpm -qi [软件名称]: 列出该软件的详细信息rpm -qf [指定文件名]: 查找指定文件属于哪个RPM软件包(Query Files)rpm -qc [软件名称]: 列出该软件的所有文件rpm -qpi [软件名称]: 列出RPM软件包的描述信息(Query Packages install package(s))rpm -qpl [软件名称]: 列出该软件的所有文件(Query Packages list)rpm -qRp [软件名称]: 列出该软件的依赖(Query Rely Packages)rpm -Va [软件名称]: 校验所有的RPM软件包，查找丢失的文件(View List)其他如何从python源码包构建rpm在python源码目录执行1python setup.py bdist_rpm # 即可在当前dist目录下生成rpm包定制spec:123python setup.py bdist_rpm --spec-only # 只生成dist/&lt;module&gt;.spec# 重新编辑 dist/&lt;module&gt;.specpython setup.py bdist_rpm --spec-file=dist/&lt;module&gt;.spec无污染地打rpm包方法一、使用mock来进程打包，详细的过程可以见centos下无污染地打rpm包mock编包一般是做发行版本给别人使用，当然你做的软件要能在别人机器上跑，那么在开发过程中，我们必须模拟一个纯净的用户环境（即是所谓的chroot），mock在编包前先构建一个这样的用户环境，然后里面再使用rpmbuild的机制编包。总结来说，rpmbuild编出来的包是特例，而mock编出来包在具有共性。mock相当于给rpmbuild增加了一个外壳，包装了一下。方法二、虚拟机操作建议使用虚拟机操作，不要在自己的电脑上进行这些操作，不然到时候弄一大堆的包在自己电脑上很头疼的，如果没有虚拟机可以使用 vagrant在本地虚拟化出一个os进行操作 具体用法使用 Vagrant 打造跨平台开发环境123456789mkdir some-dircd some-dirvagrant init centos/6vagrant upvagrant ssh这两种方法各有各的好处，mock感觉有点像dock，只要编译完成后直接删除该mock环境就行，更加的纯洁迅速； 而Vagrant如果没有理解错他就是在本地安装了一个虚拟机，比起mock更重一些，但是可以比较轻松的应对更多的场景，比如说在本地开发的时候代码可以在不同的时候上调试，以后有时间一定要学习下这两种方式。总结rpmbuild打包一般步骤根据rpmbuild标准，创建打包的目录结构将源码和辅助文件放到指定目录编写spec文件，放到指定目录根据需要构建rpm，或者rpm和srpm等。无网络时提前在有网络的机器上下载好依赖包就好啦引用How to create an RPM package/zh-hkCentOS6下rpm打包实战Centos 6 制作 rpm 包Linux之rpm本地打包SRPM包编译成RPM包之rpmbuild和mock]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>持续集成</tag>
        <tag>CI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis运行优化]]></title>
    <url>%2Fredis-better%2F</url>
    <content type="text"><![CDATA[摘要：redis在运行的过程中如果使用不合适的配置会使其占用内存越来越大，在此我给出我自己的一些优化策略供大家参考，如果你看到我的文章有更好的方法和策略希望也能分享给我，我们一起交流，感激不尽！代码层级去除代码里的scan操作给每个key加过期时间存储结构用hashredis策略绑定本机ip部署redis防止远程连接maxmemory-policy allkeys-lru :从数据集中(包括设置过期时间以及未设置过期时间的数据集中)，选择最近最久未使用的数据释放maxmemory 12GB 根据实际调整大小系统限制内核参数 sysctl.conf1vm.overcommit_memory=1Redis的RDB持久化实现是folk一个子进程，然后让子进程将内存镜像dump到RDB文件中。理论上来说是需要跟父进程一样的内存空间，但是由于linux很早就支持的copy-on-write技术，所以实际上并不需要这么多的物理内存的。（vm.overcommit_memory=1表示内核允许分配所有的物理内存，而不管当前的内存状态如何。）关闭透明大页功能1echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled(操作系统后台有一个叫做khugepaged的进程，它会一直扫描所有进程占用的内存，在可能的情况下会把4kpage交换为Huge Pages，在这个过程中，对于操作的内存的各种分配活动都需要各种内存锁，直接影响程序的内存访问性能，并且，这个过程对于应用是透明的，在应用层面不可控制,对于专门为4k page优化的程序来说，可能会造成随机的性能下降现象。)冷备方式redis提供两种方式,他们是 RDB和AOFRDB 将数据库的快照（snapshot）以二进制的方式保存到磁盘中。AOF 则以协议文本的方式，将所有对数据库进行过写入的命令（及其参数）记录到 AOF 文件，以此达到记录数据库状态的目的。RDB优点：节省磁盘空间、恢复速度快，就是一个镜像，适合大规模的数据恢复；对数据完整性和一致性要求不高缺点：在备份周期在一定间隔时间做一次备份，所以如果Redis意外down掉的话，就会丢失最后一次快照后的所有修改。虽然Redis在fork时使用了写时拷贝技术,但是如果数据庞大时还是会占用cpu性能。优化策略（改redis的配置）：AOF 与 RDB ，使用RDB方式打开命令1234567891011121314# 指定本地数据库文件名，一般采用默认的 dump.rdbdbfilename dump.rdb# save &lt;指定时间间隔&gt; &lt;执行指定次数更新操作&gt;，满足条件就将内存中的数据同步到硬盘中。官方出厂配置默认是 900秒内有1个更改，300秒内有10个更改以及60秒内有10000个更改，则将内存中的数据快照写入磁盘。若不想用RDB方案，可以把 save "" 的注释打开，下面三个注释。# save ""save 900 1save 300 10save 60 10000# 指定本地数据库存放目录，一般也用默认配置dir ./# 配置存储至本地数据库时是否压缩数据，默认为yes。# Redis采用LZF压缩方式，但占用了一点CPU的时间。若关闭该选项，但会导致数据库文件变的巨大。建议开启。rdbcompression yes关闭aof：appendonly no（appendfsync everysec：每一秒写入aof文件，并完成磁盘同步。appendfsync no 不即时同步，由操作系统控制何时刷写到磁盘上，这种模式速度最快，减少io，数据安全性下降,但是会产生僵尸进程appendfsync always：每个命令都写入aof文件，并完成磁盘同步）为什么不使用AOF?AOF 是redis的一种持久化方式，用来记录所有的写操作，但是随着时间增加，aof文件会越来越大，所以需要进行重写，将内存中的数据重新以命令的方式写入aof文件。在重写的过程中，由于redis还会有新的写入，为了避免数据丢失，会开辟一块内存用于存放重写期间产生的写入操作，等到重写完毕后会将这块内存中的操作再追加到aof文件中。从原理中可以了解到，如果在重写过程中redis的写入很频繁或写入量很大，就会导致占用大量额外的内存来缓存写操作，导致内存爆涨。命令行方式关闭：12345redis-03:6379&gt; config get appendonly1) "appendonly"2) "yes"redis-03:6379&gt; config set appendonly noOK总结绑定本机ip部署redis防止远程连接aof 与 rdb 比较代码层级：去除代码里的scan操作，给每个key加过期时间存储结构用hashmaxmemory-policy allkeys-lru:从数据集中(包括设置过期时间以及未设置过期时间的数据集中)，选择最近最久未使用的数据释放关闭aof：appendonly no（appendfsync everysec：每一秒写入aof文件，并完成磁盘同步。appendfsync no 不即时同步，由操作系统控制何时刷写到磁盘上，这种模式速度最快，减少io，数据安全性下降,但是为产生僵尸进程appendfsync always：每个命令都写入aof文件，并完成磁盘同步）内核参数sysctl.confvm.overcommit_memory=1Redis的RDB持久化实现是folk一个子进程，然后让子进程将内存镜像dump到RDB文件中。理论上来说是需要跟父进程一样的内存空间，但是由于linux很早就支持的copy-on-write技术，所以实际上并不需要这么多的物理内存的。（vm.overcommit_memory=1表示内核允许分配所有的物理内存，而不管当前的内存状态如何。）关闭透明大页功能 echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled(操作系统后台有一个叫做khugepaged的进程，它会一直扫描所有进程占用的内存，在可能的情况下会把4kpage交换为Huge Pages，在这个过程中，对于操作的内存的各种分配活动都需要各种内存锁，直接影响程序的内存访问性能，并且，这个过程对于应用是透明的，在应用层面不可控制,对于专门为4k page优化的程序来说，可能会造成随机的性能下降现象。)maxmemory 12GB 根据实际调整大小引用redis.conf配置详细解析redis官网v3.0配置redis官网配置]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起刷算法_动态规划01]]></title>
    <url>%2Fleetcode-20190420%2F</url>
    <content type="text"><![CDATA[摘要：本周我利用休息时间刷了两道简单、一道中等的动态规划算法题目，和大家一起分享一下，作为动态规划算法专题的开始。我的主页 https://qupzhi.com ，转载请注明出处。以下内容用go语言实现动态规划三要素三要素最优子结构性质，通俗的说法就是问题的最优解包含其子问题的最优解边界状态转移方程重复求区间和12345678910111213给定一个整数数组 nums，求出数组从索引 i 到 j (i ≤ j) 范围内元素的总和，包含 i, j 两点。示例：给定 nums = [-2, 0, 3, -5, 2, -1]，求和函数为 sumRange()sumRange(0, 2) -&gt; 1sumRange(2, 5) -&gt; -1sumRange(0, 5) -&gt; -3说明:你可以假设数组不可变。会多次调用 sumRange 方法。三种思路：直接根据每次传进来的区间进行现场计算用一个Map，把每次传进来的区间拼成一个key,二次查询的时候就有了缓存直接用一个1D的数组，每个元素存0到当前值之间的和，这样区间和就是两个下标存元素相减就可以了1234567891011121314151617181920212223242526272829type NumArray struct &#123;&#125;var sum *[]intfunc Constructor(nums []int) NumArray &#123; var obj NumArray for k, v := range nums &#123; if k &gt; 0 &#123; nums[k] = nums[k-1] + v &#125; &#125; sum = &amp;nums return obj&#125;func (this *NumArray) SumRange(i int, j int) int &#123; if i == 0 &#123; return (*sum)[j] &#125; return (*sum)[j] - (*sum)[i-1]&#125;func Test()&#123; arr := []int&#123;1,2,3,4&#125; obj := Constructor(arr) res := obj.SumRange(0,3) fmt.Println(res)&#125;经典问题爬楼梯123456789101112131415161718192021假设你正在爬楼梯。需要 n 阶你才能到达楼顶。每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？注意：给定 n 是一个正整数。示例 1：输入： 2输出： 2解释： 有两种方法可以爬到楼顶。1. 1 阶 + 1 阶2. 2 阶示例 2：输入： 3输出： 3解释： 有三种方法可以爬到楼顶。1. 1 阶 + 1 阶 + 1 阶2. 1 阶 + 2 阶3. 2 阶 + 1 阶因为每次都可以分解为1步和2步，边界就是1步2步，状态转移方程就是f(n) = f(n-1) + f(n-2) (n&gt;=3),这里提供三种思路递归字典动态规划，每次都用前一次的值推倒后一次的答案123456789101112131415161718192021222324252627282930313233343536373839404142// f(n) = f(n-1) + f(n-2) (n&gt;=3)// f(1) = 1// f(2) = 2// 方法一、递归func climbStairs1(n int)int&#123; if n &lt; 0 &#123; return 0 &#125; if n == 1 &#123; return 1 &#125; if n == 2 &#123; return 2 &#125; return climbStairs1(n-1) + climbStairs1(n-2)&#125;//方法二、动归func climbStairs(n int) int &#123; if n == 1 &#123; return 1 &#125; if n == 2 &#123; return 2 &#125; res,a,b := 0,1 ,2 for i:=3;i&lt;=n;i++&#123; res = a + b a = b b = res &#125; return res&#125;func Test1()&#123; fmt.Println(climbStairs1(10))&#125;func Test2()&#123; fmt.Println(climbStairs(100))&#125;最大正方形面积12345678910111213141516171819202122221. Maximal SquareMedium116529FavoriteShareGiven a 2D binary matrix filled with 0&apos;s and 1&apos;s, find the largest square containing only 1&apos;s and return its area.Example:Input:1 0 1 0 01 0 1 1 11 1 1 1 11 0 0 1 0Output: 4三种思路：方法一 逐渐增加连续的 ‘1’ 的长度 ，然后用一个函数去看是否构成正方形，构成的话就继续增加边长，直到找到最大的正方形为止时间复杂 O((mn)^2) 空间 O(1)方法二 用一个二维数组的的每个元素都来保存边长每次只要遍历到的点是1，就开始取他的上、左、左上三个值取最小，再加1得到当前点的值12dp[x,y] = min(min(dp[x-1][y],dp[x][y-1]) , dp[x-1][y-1])+1O(mn) O(mn)方法三12345input [1,1,0][1,1,0]000001100120每次遍历总是要知道上一行的数据，所以至少得保留一行可以发现，遍历到第三行的时候，第一行已经没有用了！每次遍历的时候，假设只保留上一行，比如在第三行，每次走一位，我们要更新当前值，当前位置的马上的覆盖值对于下一次来说就是左上角的值（2*2）对于这一次来说就是上，因为当前位置的左边已经更新了所以就是左，那我们当前就出现了左、上、左上三个值了，凑够了。初始化就全0，prev=012dp[y]=min(dp[y-1],dp[y],prev)O(mn) O(m)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// 方法二func maximalSquare(matrix [][]byte) int &#123; rows := len(matrix) cols := 0 if rows&gt;0&#123; cols = len(matrix[0]) &#125;else&#123; cols = 0 &#125; dp := make([][]int,rows+1) for i:=0;i &lt; len(dp);i++&#123; dp[i] = make([]int,cols+1) &#125; maxlen := 0 for x:=1;x &lt;= rows; x++&#123; for y:=1;y &lt;= cols;y++&#123; if matrix[x-1][y-1] == '1'&#123; tmp := min(min(dp[x-1][y],dp[x][y-1]),dp[x-1][y-1])+1 dp[x][y]=tmp maxlen = max(maxlen,tmp) &#125; &#125; &#125; return maxlen * maxlen&#125;// 方法三func maximalSquare2(matrix [][]byte) int &#123; rows := len(matrix) cols := 0 if rows&gt;0&#123; cols = len(matrix[0]) &#125;else&#123; cols = 0 &#125; dp := make([]int,cols +1) maxlen,prev := 0,0 for x:=1;x &lt;= rows; x++&#123; for y:=1;y &lt;= cols;y++&#123; tmp := dp[y] //当前值当下一次的prev if matrix[x-1][y-1] == '1'&#123; dp[y] = min(min(dp[y],dp[y-1]),prev) + 1 //分别对应上，左，左上 maxlen = max(dp[y],maxlen) &#125;else&#123; dp[y] = 0 &#125; prev = tmp &#125; &#125; return maxlen * maxlen&#125;func Test1()&#123; t1 :=[][]byte&#123;&#123;'1','0','1','0','0'&#125;,&#123;'1','0','1','1','1'&#125;,&#123;'1','1','1','1','1'&#125;,&#123;'1','0','0','1','0'&#125;&#125; t2 :=[][]byte&#123;&#125; fmt.Println(maximalSquare(t2)) fmt.Println(maximalSquare2(t1))&#125;func min(x, y int) int &#123; if x &lt; y &#123; return x &#125; return y&#125;func max(x, y int) int &#123; if x &gt; y &#123; return x &#125; return y&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>动态规划</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Clion优雅的完全远程自动同步和远程调试c++]]></title>
    <url>%2FCLion_cplusplus%2F</url>
    <content type="text"><![CDATA[摘要：在linux上用vim写C++的时候，通常用gdb进行调试，不能随心所欲的看代码和跳转代码以及加watch（也有可能是因为我还没有get正确的使用方法）。为此我发现Clion可以做到自动同步本场代码或自己下载远程代码同步到本地，以及调试在远程机器上运行的代码，为此记录下来。简介CLion 是一款专为开发 C 及 C++ 所设计的跨平台 IDE。 它是以 IntelliJ 为基础设计的，包含了许多智能功能来提高开发人员的生产力，从而提升工作效率 。环境准备操作系统： macOS / WindowsIDE: CLion源码路径： Node Master自动同步打开CLion的设置，进入到 Preferences | Build, Execution, Deployment | Deployment点加号，添加一个远程配置设置为SFTP协议，设置远程ip、port、pass然后点击Test Connection，确认是否连接成功切换到Mappings设置本地的代码根目录Local path设置远程的代码根目录 Deployment path上传一个文件Upload是上传，Download是下载，本地与远程所映射的目录我们在上面设置过了上传成功runlvm.sh文件更新成功，现在的时间是3月18 21：42在Tool - Deployment - Automatic Upload(always) 设置为自动同步自动同步的话可以在每次在本地改动代码的时候自己同步了Sync with Deployed to 就是自动下载远程代码到本地手动上传整个项目上传成功，上传了100个文件查看远程目录，确实成功了好了，现在只要在本地有任何的改动都会自动同步到远程了。远程调试CentOS的依赖依赖123456789# cmakeyum install cmake -y# gcc &amp; gdbyum install gcc-c++ -yyum install gdb -y#gdbserveryum install gdb-gdbserver -y添加GDB Remote Debug本地克利翁配置添加一个GDB远程调试的配置 GDB Remote Debug设置远程访问参数（target remote args）： tcp:xx.xx.xx.xx:1234设置远程路径和本地路径的映射（Path mappings）远程调试连接成功后，像本地调试一样，可以设置断点，单步跟踪等调试需要本地和远程的代码一致两个方法设置远程配置方法一、 远程gdbserver的启动远程调试依赖gdbserver的来支持，通过gdbserver的启动的程序，会先等待远程调试的连接，连接成功后再启动进程。假设代码的根目录：/data/pzqu/read_phy_disk_use/,执行以下代码以后编译1234cd /data/pzqu/read_phy_disk_use/buildcmake .. -DCMAKE_BUILD_TYPE=Debugmakegdbserver :1234 ./read_phy_disk_use注意：cmake的指定需要-DCMAKE_BUILD_TYPE=Debug来请获取调试方法二 远程gdbserver的动态连接gdbserver的还支持动态绑定到已经启动的进程1gdbserver :1234 --attach &lt;PID&gt;打断点开始调试点击小虫子开始调试我们可以看到代码停止到了断点处Variables 是可以自己设置的watch和正常调试方式一样啦用CLion完全的在远程工作这是一个非常非常爽的操作，以后就可以完全的用IDE来写代码了，虽然使用命令行显得比较专业，但是使用工具可以有效的提高工作效率，何乐而不为呢？进入 Preferences | Build, Execution, Deployment | Toolchains设置远程连接信息，CLion会自动监测CMake gcc g++ gdb的信息进入Preferences | Build, Execution, Deployment | CMakeToolchain 选择我们刚刚设置的上面的Automatically reload CMake project on editing 的设置勾上的时候，只要代码有修改就会自动编译现在已经配置了远程Toolchains工具链并配置了相应的CMake配置文件上图是自动编译和上传到的目录/tmp/tmp.pIdETgMIBR，然后我们只要设置为自动同步目录就成了，如下图现在已经设置成了自动同步目录，只需在运行/调试配置切换器中选择正确的CMake配置文件，即可以完全远程的方式构建，运行和调试应用程序和测试。添加Application来跑我们的程序远程运行，本地输出结果远程运行，本地自动调试总结配置CLion中的同步信息：同步到哪台机器的哪个目录。使用 Deployment 让本地和远程可以自动同步和自动下载，手动同步和手动下载代码配置GDB remote Debug设置使用gdbserver来启动程序进行调试使用gdbserver attach依附已经启动的程序进行调试完全在本地进行远程代码调试其他这种方式是不是非常的优雅，这样我们只要不断的添加工具链（Toolchains）就可以在本地应对数不清的远程环境了。这种方式应该适用于JetBrain全家桶]]></content>
      <categories>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用cmake编译]]></title>
    <url>%2Fhow-to-use-cmake%2F</url>
    <content type="text"><![CDATA[摘要：CMake是一种跨平台编译工具，比make更为高级，使用起来要方便得多。CMake主要是编写CMakeLists.txt文件，然后用cmake命令将CMakeLists.txt文件转化为make所需要的makefile文件，最后用make命令编译源码生成可执行程序或共享库CentOS的依赖依赖123456789# cmakeyum install cmake -y# gcc &amp; gdbyum install gcc-c++ -yyum install gdb -y#gdbserveryum install gdb-gdbserver -yCMake编译原理CMake是一种跨平台编译工具，比make更为高级，使用起来要方便得多。CMake主要是编写CMakeLists.txt文件，然后用cmake命令将CMakeLists.txt文件转化为make所需要的makefile文件，最后用make命令编译源码生成可执行程序或共享库（so(shared object)）。因此CMake的编译基本就两个步骤：cmakemakecompile.sh1g++ -rdynamic ../include/incl/tfc_base_config_file.cpp ../include/mq/*.cpp local_util.cpp AgentMemRpt.cpp AgentDiskRpt.cpp AgentLoadRpt.cpp AgentIoRpt.cpp AgentNetRpt.cpp AgentCpuRpt.cpp AgentProcessRpt.cpp AgentParentRpt.cpp AgentSysTop_5.cpp BaseFeatureRptMain.cpp -o rpt_main -I../include/incl -I../include/mq -I../include/rapidjson -lpthread -ldlCMake说明一般把CMakeLists.txt文件放在工程目录下，使用时，先创建一个叫build的文件夹（这个并非必须，因为cmake命令指向CMakeLists.txt所在的目录，例如cmake .. 表示CMakeLists.txt在当前目录的上一级目录。cmake后会生成很多编译的中间文件以及makefile文件，所以一般建议新建一个新的目录，专门用来编译），然后执行下列操作：123cd build cmake .. make其中cmake .. 在build里生成Makefile，make根据生成makefile文件，编译程序，make应当在有Makefile的目录下，根据Makefile生成可执行文件。编写 CMakeList.txt123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# 1. 声明要求的cmake最低版本cmake_minimum_required( VERSION 2.8 )# 2. 添加c++11标准支持#set( CMAKE_CXX_FLAGS &quot;-std=c++11&quot; )# 3. 声明一个cmake工程PROJECT(rpt_main)MESSAGE(STATUS &quot;Project: SERVER&quot;) #打印相关消息消息 # 4. 头文件include_directories($&#123;PROJECT_SOURCE_DIR&#125;/../include/mq $&#123;PROJECT_SOURCE_DIR&#125;/../include/incl $&#123;PROJECT_SOURCE_DIR&#125;/../include/rapidjson)# 5. 通过设定SRC变量，将源代码路径都给SRC，如果有多个，可以直接在后面继续添加set(SRC $&#123;PROJECT_SOURCE_DIR&#125;/../include/incl/tfc_base_config_file.cpp $&#123;PROJECT_SOURCE_DIR&#125;/../include/mq/tfc_ipc_sv.cpp $&#123;PROJECT_SOURCE_DIR&#125;/../include/mq/tfc_net_ipc_mq.cpp$&#123;PROJECT_SOURCE_DIR&#125;/../include/mq/tfc_net_open_mq.cpp $&#123;PROJECT_SOURCE_DIR&#125;/local_util.cpp$&#123;PROJECT_SOURCE_DIR&#125;/AgentMemRpt.cpp $&#123;PROJECT_SOURCE_DIR&#125;/AgentDiskRpt.cpp $&#123;PROJECT_SOURCE_DIR&#125;/AgentLoadRpt.cpp $&#123;PROJECT_SOURCE_DIR&#125;/AgentIoRpt.cpp$&#123;PROJECT_SOURCE_DIR&#125;/AgentNetRpt.cpp $&#123;PROJECT_SOURCE_DIR&#125;/AgentCpuRpt.cpp $&#123;PROJECT_SOURCE_DIR&#125;/AgentProcessRpt.cpp $&#123;PROJECT_SOURCE_DIR&#125;/AgentParentRpt.cpp$&#123;PROJECT_SOURCE_DIR&#125;/AgentSysTop_5.cpp $&#123;PROJECT_SOURCE_DIR&#125;/BaseFeatureRptMain.cpp )# 6. 创建共享库/静态库# 设置路径（下面生成共享库的路径）set(CMAKE_LIBRARY_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/lib)# 即生成的共享库在工程文件夹下的lib文件夹中 set(LIB_NAME rpt_main_lib)# 创建共享库（把工程内的cpp文件都创建成共享库文件，方便通过头文件来调用）# 这时候只需要cpp，不需要有主函数 # $&#123;PROJECT_NAME&#125;是生成的库名 表示生成的共享库文件就叫做 lib工程名.so# 也可以专门写cmakelists来编译一个没有主函数的程序来生成共享库，供其它程序使用# SHARED为生成动态库，STATIC为生成静态库add_library($&#123;LIB_NAME&#125; STATIC $&#123;SRC&#125;) # 7. 链接库文件# 把刚刚生成的$&#123;LIB_NAME&#125;库和所需的其它库链接起来# 如果需要链接其他的动态库，-l后接去除lib前缀和.so后缀的名称，以链接# libpthread.so 为例,-lpthreadtarget_link_libraries($&#123;LIB_NAME&#125; pthread dl) # 8. 编译主函数，生成可执行文件# 先设置路径set(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/bin) # 可执行文件生成add_executable($&#123;PROJECT_NAME&#125; $&#123;SRC&#125;) # 这个可执行文件所需的库（一般就是刚刚生成的工程的库咯）target_link_libraries($&#123;PROJECT_NAME&#125; pthread dl $&#123;LIB_NAME&#125;)使用 cmake进入/home/pzqu/agent/libvirt_base_feature/build目录执行命令 cmake ..查看生成的目录结构，此目录结构是中间代码，不用提交到git12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@TJSJZVM000456 /home/pzqu/agent/libvirt_base_feature/build]# tree.|-- CMakeCache.txt|-- CMakeFiles| |-- 2.8.12.2| | |-- CMakeCCompiler.cmake| | |-- CMakeCXXCompiler.cmake| | |-- CMakeDetermineCompilerABI_C.bin| | |-- CMakeDetermineCompilerABI_CXX.bin| | |-- CMakeSystem.cmake| | |-- CompilerIdC| | | |-- CMakeCCompilerId.c| | | `-- a.out| | `-- CompilerIdCXX| | |-- CMakeCXXCompilerId.cpp| | `-- a.out| |-- CMakeDirectoryInformation.cmake| |-- CMakeOutput.log| |-- CMakeTmp| |-- Makefile.cmake| |-- Makefile2| |-- TargetDirectories.txt| |-- cmake.check_cache| |-- progress.marks| |-- rpt_main.dir| | |-- DependInfo.cmake| | |-- build.make| | |-- cmake_clean.cmake| | |-- depend.make| | |-- flags.make| | |-- home| | | `-- pzqu| | | `-- agent| | | `-- include| | | |-- incl| | | `-- mq| | |-- link.txt| | `-- progress.make| `-- rpt_main_lib.dir| |-- DependInfo.cmake| |-- build.make| |-- cmake_clean.cmake| |-- cmake_clean_target.cmake| |-- depend.make| |-- flags.make| |-- home| | `-- pzqu| | `-- agent| | `-- include| | |-- incl| | `-- mq| |-- link.txt| `-- progress.make|-- Makefile`-- cmake_install.cmake使用make命令编译得到二进制文件二进制文件所在目录（CMakeLists.txt文件配置）下次教大家如何用Clion自动同步代码到服务器上，并进行debug]]></content>
      <categories>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux系统 物理硬盘监控]]></title>
    <url>%2Flinux-disk-monitor%2F</url>
    <content type="text"><![CDATA[摘要：监控系统在linux系统上获取物理磁盘IO以及使用情况的原理，让我们一起来探索一下1本文使用语言为c++物理磁盘列表和磁盘IO第一步要解决的问题是先识别物理磁盘是哪些。上图是/proc/diskstats的文件内容部分截取，我们可以通过读取/proc/diskstats获得物理磁盘列表以确认哪些是物理设备（算云硬盘）以及iops等信息1-5列：设备号、编号、设备、读完成次数、合并完成次数6-10列：读扇区次数、读操作花费毫秒数、写完成次数、合并写完成次数、写扇区次数11-14列：写操作花费的毫秒数、正在处理的输入/输出请求数、输入/输出操作花费的毫秒数、输入/输出操作花费的加权毫秒数。那这个文件内如此多的设备哪些是物理硬盘呢？只要达到下面两个限制条件就判定为物理硬盘。该行有14列，可以使用sscanf取到设备名此行中的设备名组装成/sys/block/设备名/device，然后看此文件夹是否存在，如果存在则是物理磁盘设备备注1：判断文件/文件夹是否存在使用函数access(syspath, F_OK)，存在返回0备注2：如果设备名为中含有/的话要转换成!,如下123while ((slash = strchr(name, &apos;/&apos;))) &#123; *slash = &apos;!&apos;;&#125;备注3：目录/sys/block下的所有子目录代表着系统中当前被发现的所有块设备（其中的内容已经变为了指向它们在/sys/devices/中真实设备的符号链接文件）到此我们就取到了物理硬盘的iops，接下来我们来看看使用情况和总量是如何拿到的。物理磁盘使用情况 和总量物理磁盘使用量因为我们没有办法直接取到物理硬盘的使用情况，所以我们用一种间接的方式。根据分区和物理硬盘的关系获得物理硬盘的使用情况。比如一个物理硬盘sda分了sda1 sda2等两个分区，又知道sda1的挂载点是/data,sda2的挂载点是/home，通过某种方式查出/data和home的使用情况，加起来就是sda的使用情况了。我们可以，然后再通过获取挂载点大小的方式知道这些设备的使用情况。/etc/mtab中不会直接物理硬盘的信息，所以只能通过把属于这个物理硬盘的全部分区加起来才能最后算出我们想要的值。使用情况计算逻辑通过读/etc/mtab的方式拿到各种设备和它的挂载点。（/etc/mtab文件中不会直接给出物理硬盘的使用情况）使用statfs获得所挂载的目录使用情况来确定每个设备的使用情况根据分区和物理硬盘的关系获得物理硬盘的使用情况（通常物理磁盘的名称是分区的子串，比如/dev/sda是/dev/sda1的子串）tips: 物理磁盘设备的名称列表我们已经在上一节取到了。通过/etc/mtab文件拿到各种设备和它的挂载点知道了计算逻辑，我们来看看/etc/mtab文件内容的含义上图是/etc/mtab的内容截取，可以读取/etc/mtab文件获取设备名和挂载点此文件每行有四列，分别代表的含义是：驱动器、挂载点、文件系统、读写权限/etc/mtab记载了当前系统已经装载的文件系统，包括一些操作系统虚拟文件，使用/etc/fstab也可以监控，不同的是/etc/mtab文件在mount挂载、umount卸载时都会被更新，时刻跟踪当前系统中的分区挂载情况。用到了以下核心c++函数（读取/etc/mtab）123456mount_table = setmntent(&quot;/etc/mtab&quot;, &quot;r&quot;); //打开文件系统描述文件的文件名，并且返回可以被使用的文件指针getmntent().mount_entry = getmntent(mount_table);//函数读取文件系统的下一行来自文件流的描述文件并返回指向结构的指针（即循环读取文件）device = mount_entry-&gt;mnt_fsname;mount_point = mount_entry-&gt;mnt_dir;statfs(mount_point, &amp;s) != 0 //此条件成立时获取成功endmntent(mount_table);//关闭流和与其相关联的文件系统描述文件。具体用法见 linux中getmntent、setmntent 、endmntent 函数的详细用法通过statfs函数所挂载的目录使用情况(used/total)来确定每个分区的使用情况123456789101112131415161718192021#include &lt;sys/vfs.h&gt; /* 或者 &lt;sys/statfs.h&gt; */// path: 需要查询信息的文件系统的文件路径名。 如/home// buf：以下结构体的指针变量，用于储存文件系统相关的信息int statfs(const char *path, struct statfs *buf); // fd： 需要查询信息的文件系统的文件描述词。int fstatfs(int fd, struct statfs *buf);struct statfs &#123; long f_type; /* 文件系统类型 */ long f_bsize; /* 经过优化的传输块大小 */ long f_blocks; /* 文件系统数据块总数 */ long f_bfree; /* 可用块数 */ long f_bavail; /* 非超级用户可获取的块数 */ long f_files; /* 文件结点总数 */ long f_ffree; /* 可用文件结点数 */ fsid_t f_fsid; /* 文件系统标识 */ long f_namelen; /* 文件名的最大长度 */ &#125;;返回说明：成功执行时，返回0。失败返回-1statfs结构中可用空间块数有两种f_bfree和 f_bavail，前者是硬盘所有剩余空间，后者为非root用户剩余空间，ext3文件系统给root用户分有5%的独享空间，所以这里是不同的地方。这里要强调的是每块的大小一般是4K（×这句话错误，不一定都是4k，正确做法是:总大小=sfs.f_blocks×f_bsize，即块数×每块的大小，单位是bytes，也就是要/1024/1024/1024才是GB单位）。计算分区的使用情况123456789101112131415161718192021222324252627#define M (1024*1024) blocks_used = s.f_blocks - s.f_bfree; //使用量 blocks_percent_used = 0; if (blocks_used + s.f_bavail) &#123; blocks_percent_used = blocks_used * 100 / (blocks_used + s.f_bavail); //使用率 &#125; /* GNU coreutils 6.10 skips certain mounts, try to be compatible. */ if (strcmp(device, &quot;rootfs&quot;) == 0) continue; Record record; record.disk_total_val = CalRound((blocks_used + s.f_bavail) * s.f_bsize, M); //总量 record.disk_use_val = CalRound((s.f_blocks - s.f_bfree) * s.f_bsize, M); // record.use_precent = blocks_percent_used;CalRound函数的作用是四舍五入，感兴趣可以拉到文章底部看代码。在获取使用量情况失败的时候，可能是因为没有挂载获取其他特殊的情况，我们就默认使用量为0备注1：/dev/root设备可以从/proc/cmdline中获取到真实设备名备注2：rootfs设备要忽略，此为根文件系统（内核启动时所mount的第一个文件系统）如果出现lvm格式的逻辑分区怎么计算使用量？我们根据上面的逻辑可以取到正常一般情况下的part类型的分区使用量，加到物理硬盘中去；如上图，出现lvm格式分区的时候，/etc/mtab中就没有sda2设备的信息，而且sda2也没有挂载在任意一个文件系统上。这个时候就要拿到sda2下面挂载的三个lvm分区的使用情况。下图是lsblk的输出结果：可以看到上图中,有一个逻辑卷（dm-2），同时挂在sda2和sdb1上，这是怎么回事？这就要从lvm的概念开始讲起了。什么是lvm分区？LVM的重点是可以弹性调整文件系统的容量，并不是如RAID在于对文件的读写性能或是数据的可靠性上。LVM可以将多个物理分区整合在一起，让这些分区看起来就像是一个磁盘一样，而且，还可以在将来添加其他的物理分区或将其从这个LVM管理的磁盘中删除。这样一来，整个硬盘的空间使用上，相当具有弹性。这里介绍三个概念：PV(physical volume)：物理卷在逻辑卷管理系统最底层，可为整个物理硬盘或实际物理硬盘上的分区。VG(volume group)：卷组建立在物理卷上，一卷组中至少要包括一物理卷，卷组建立后可动态的添加卷到卷组中，一个逻辑卷管理系统工程中可有多个卷组。LV(logical volume)：逻辑卷建立在卷组基础上，卷组中未分配空间可用于建立新的逻辑卷，逻辑卷建立后可以动态扩展和缩小空间。我们知道了这些就够了，怎么计算lvm格式的使用量并规到物理硬盘上呢？我们要知道他的写入方式，才能知道算法。lvm有两种写入方式LVM写入方式：线性模式（linear）：假如有/dev/sdb1,/dev/sdb2这2个分区加入到VG当中，并且整个VG只有一个LV时，那么所谓的线性模式就是当/dev/sdb1的容量用完之后，/dev/sdb2的分区才会被使用。在此模式下，使用量就按顺序算到所挂的分区上去。交错模式（triped）：将一条数据拆分成两部分，分别写入/dev/sdb1与/dev/sdb2，有点像RAID0。这样子，一份数据用两块硬盘来写入，理论上，读写性能会比较好。在此模式下，使用量就按平均到所挂的分区上去，可能会有点细微的差别，但这是相对准确的方式了。如何取到lvm类型执行lvm相关的命令之前必须要安装lvm2这个软件，不过CentOS和其他比较新的Linux发行版已经默认安装了lvm的所需软件，何况我们这里的目的是监控已经创建lvm分区的linux机器（lsblk看到的），那一定有这些软件，就不用担心这个问题了。但是比较老的版本没有这些参数，比如那我们用这种方式ps:直接解析/proc/swaps的内容有一样的效果哦现在我们取到了dm-1设备的使用情况和总量，正常来说可以结合lsblk的结果和对应到磁盘上，但是问题来了，有的lsblk输出结果不带有dm-1这种字样，那怎么办呢？不用怕，我们可以利用VG和LV的名称找到他们的软链接（符号链接）。再用c++的readlink函数取到符号链接所指向的文件ps: 大家可以看到，这里的lvm使用量都是用命令方式来采集的，如果你有读文件或者系统api等更好的方式，希望你可以留言和我交流，非常感谢！物理磁盘总量我们可以直接根据磁盘名（比如/dev/sda）来获取磁盘总量,无论是否有lvm分区，以下是核心代码123456789101112131415161718192021unsigned long long AgentDiskRpt::readDiskTotal(const string &amp;deviceName)&#123; int fd, ret; unsigned long long size; fd = open(deviceName.c_str(), O_RDONLY); if (fd == -1) &#123; close(fd); return -1; &#125; ret = ioctl(fd, BLKGETSIZE64, &amp;size); if (ret == 0) &#123; close(fd); return CalRound(size,M); &#125; close(fd); return 0;&#125;遇到nas硬盘怎么计算？NAS（Network Attached Storage：网络附属存储）按字面简单说就是连接在网络上，具备资料存储功能的装置，因此也称为“网络存储器”。它是一种专用数据存储服务器。它以数据为中心，将存储设备与服务器彻底分离，集中管理数据，从而释放带宽、提高性能、降低总拥有成本、保护投资。其成本远远低于使用服务器存储，而效率却远远高于后者。nas硬盘，采集的时候当作逻辑磁盘，不是物理硬盘，他是共享的，多个用户共享一块nas盘的时候可以共享数据，所以nas盘不应该统计成物理磁盘，我们这里就没有算作，可以算作逻辑分区，直接在/etc/mtab里就能读到啦。其他CalRound函数1234567891011121314unsigned long long CalRound(unsigned long long value, int base)&#123; unsigned long long ret = 0; if (base &lt;= 1) return value; unsigned long long tmp = base / 2; if (tmp &lt;= 0) tmp = 1; if (value % base &gt;= tmp) ret = value / base + 1; else ret = value / base; return ret;&#125;参考Linux /etc/fstab和etc/mtab有什么区别statfslinux中getmntent、setmntent 、endmntent 函数的详细用法LVM动态逻辑卷理论详解]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>linux</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq 原理、集群、基本运维操作、常见故障处理]]></title>
    <url>%2Frabbitmq-learn01%2F</url>
    <content type="text"><![CDATA[摘要:本次学习主要针对运维人员，和对rabbitmq不熟悉的开发人员。通过本次学习你将掌握rabbitmq 的基本原理、集群、基本运维操作、常见故障处理用时：25 分钟原理与概念用时：9 分钟简介AMQP，即Advanced Message Queuing Protocol，高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。消息中间件主要用于组件之间的解耦，消息的发送者无需知道消息使用者的存在，反之亦然。AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。RabbitMQ是一个开源的AMQP实现，服务器端用Erlang语言编写，支持多种客户端，如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等，支持AJAX。用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。解决的问题RabbitMQ就是当前最主流的消息中间件之一。两个（多个）系统间需要通过定时任务来同步某些数据异构系统的不同进程间相互调用、通讯的问题QueueQueue（队列）是RabbitMQ的内部对象，用于存储消息，用下图表示。RabbitMQ中的消息都只能存储在Queue中，生产者（下图中的P）生产消息并最终投递到Queue中，消费者（下图中的C）可以从Queue中获取消息并消费。多个消费者可以订阅同一个Queue，这时Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理。技术术语Broker：简单来说就是消息队列服务器实体。producer：消息生产者，就是投递消息的程序。consumer：消息消费者，就是接受消息的程序。vhost：虚拟主机，一个broker里可以开设多个vhost，用作权限分离，把不同的系统使用的rabbitmq区分开，共用一个消息队列服务器，但看上去就像各自在用不用的rabbitmq服务器一样。Connection:一个网络连接，比如TCP/IP套接字连接。channel：消息通道，是建立在真实的TCP连接内的虚拟连接（是我们与RabbitMQ打交道的最重要的一个接口）。仅仅创建了客户端到Broker之间的连接后，客户端还是不能发送消息的,需要为每一个Connection创建Channel，AMQP协议规定只有通过Channel才能执行AMQP的命令。AMQP的命令都是通过信道发送出去的（我们大部分的业务操作是在Channel这个接口中完成的，包括定义Queue、定义Exchange、绑定Queue与Exchange、发布消息等。）。每条信道都会被指派一个唯一ID。在客户端的每个连接里，可建立多个channel，每个channel代表一个会话任务,理论上无限制，减少TCP创建和销毁的开销，实现共用TCP的效果。之所以需要Channel，是因为TCP连接的建立和释放都是十分昂贵的，如果一个客户端每一个线程都需要与Broker交互，如果每一个线程都建立一个TCP连接，暂且不考虑TCP连接是否浪费，就算操作系统也无法承受每秒建立如此多的TCP连接。注1：一个生产者或一个消费者与MQ服务器之间只有一条TCP连接注2：RabbitMQ建议客户端线程之间不要共用Channel，至少要保证共用Channel的线程发送消息必须是串行的，但是建议尽量共用Connection。Exchange：消息交换机，生产者不是直接将消息投递到Queue中的，实际上是生产者将消息发送到Exchange（交换器，下图中的X），由Exchange将消息路由到一个或多个Queue中（或者丢弃）。Exchange TypesRabbitMQ常用的Exchange Type有fanout、direct、topic、headers这四种（AMQP规范里还提到两种Exchange Type，分别为system与自定义，这里不予以描述），之后会分别进行介绍。Queue：消息队列载体，每个消息都会被投入到一个或多个队列。Binding：绑定，它的作用就是把exchange和queue按照路由规则绑定起来，这样RabbitMQ就知道如何正确地将消息路由到指定的Queue了。Routing Key：路由关键字，生产者在将消息发送给Exchange的时候，一般会指定一个routing key，来指定这个消息的路由规则，而这个routing key需要与Exchange Type及binding key联合使用才能最终生效。在Exchange Type与binding key固定的情况下（在正常使用时一般这些内容都是固定配置好的），我们的生产者就可以在发送消息给Exchange时，通过指定routing key来决定消息流向哪里。Prefetch count前面我们讲到如果有多个消费者同时订阅同一个Queue中的消息，Queue中的消息会被平摊给多个消费者。这时如果每个消息的处理时间不同，就有可能会导致某些消费者一直在忙，而另外一些消费者很快就处理完手头工作并一直空闲的情况。我们可以通过设置prefetchCount来限制Queue每次发送给每个消费者的消息数，比如我们设置prefetchCount=1，则Queue每次给每个消费者发送一条消息；消费者处理完这条消息后Queue会再给该消费者发送一条消息。消息队列的使用过程在AMQP模型中，Exchange是接受生产者消息并将消息路由到消息队列的关键组件。ExchangeType和Binding决定了消息的路由规则。所以生产者想要发送消息，首先必须要声明一个Exchange和该Exchange对应的Binding。在Rabbit MQ中，声明一个Exchange需要三个参数：ExchangeName，ExchangeType和Durable。ExchangeName是该Exchange的名字，该属性在创建Binding和生产者通过publish推送消息时需要指定。ExchangeType，指Exchange的类型，在RabbitMQ中，有三种类型的Exchange：direct ，fanout和topic，不同的Exchange会表现出不同路由行为。Durable是该Exchange的持久化属性，这个会在消息持久化章节讨论。声明一个Binding需要提供一个QueueName，ExchangeName和BindingKey。下面是消息发送的过程建立连接Connection。由producer和consumer创建连接，连接到broker的物理节点上。建立消息Channel。Channel是建立在Connection之上的，一个Connection可以建立多个Channel。producer连接Virtual Host 建立Channel，Consumer连接到相应的queue上建立Channel。发送消息。由Producer发送消息到Broker中的Exchange中。路由转发。生产者Producer在发送消息时，都需要指定一个RoutingKey和Exchange，Exchange收到消息后可以看到消息中指定的RoutingKey，再根据当前Exchange的ExchangeType,按一定的规则将消息转发到相应的queue中去。消息接收。Consumer会监听相应的queue，一旦queue中有可以消费的消息，queue就将消息发送给Consumer端。消息确认。当Consumer完成某一条消息的处理之后，需要发送一条ACK消息给对应的Queue。Queue收到ACK信息后，才会认为消息处理成功，并将消息从Queue中移除；如果在对应的Channel断开后，Queue没有收到这条消息的ACK信息，该消息将被发送给另外的Channel。至此一个消息的发送接收流程走完了。消息的确认机制提高了通信的可靠性。exchange 与 Queue 的路由机制exchange 将消息发送到哪一个queue是由exchange type 和bing 规则决定的，目前常用的有3种exchange，Direct exchange, Fanout exchange, Topic exchange 。Direct exchange 直接转发路由，其实现原理是通过消息中的routkey，与queue 中的routkey 进行比对，若二者匹配，则将消息发送到这个消息队列。通常使用这个。以上图的配置为例，我们以routingKey=”error”发送消息到Exchange，则消息会路由到Queue1（amqp.gen-S9b…，这是由RabbitMQ自动生成的Queue名称）和Queue2（amqp.gen-Agl…）；如果我们以routingKey=”info”或routingKey=”warning”来发送消息，则消息只会路由到Queue2。如果我们以其他routingKey发送消息，则消息不会路由到这两个Queue中。Fanout exchange 复制分发路由，该路由不需要routkey，当exchange收到消息后，将消息复制多份转发给与自己绑定的消息队列。上图中，生产者（P）发送到Exchange（X）的所有消息都会路由到图中的两个Queue，并最终被两个消费者（C1与C2）消费。topic exchange 通配路由，是direct exchange的通配符模式，消息中的routkey可以写成通配的模式，exchange支持“#”和“*” 的通配。收到消息后，将消息转发给所有符合匹配表达式的queue。以上图中的配置为例，routingKey=”quick.orange.rabbit”的消息会同时路由到Q1与Q2，routingKey=”lazy.orange.fox”的消息会路由到Q1，routingKey=”lazy.brown.fox”的消息会路由到Q2，routingKey=”lazy.pink.rabbit”的消息会路由到Q2（只会投递给Q2一次，虽然这个routingKey与Q2的两个bindingKey都匹配）；routingKey=”quick.brown.fox”、routingKey=”orange”、routingKey=”quick.orange.male.rabbit”的消息将会被丢弃，因为它们没有匹配任何bindingKey。需要注意的一点只有queue具有保持消息的功能，exchange不能保存消息。headersheaders类型的Exchange不依赖于routing key与binding key的匹配规则来路由消息，而是根据发送的消息内容中的headers属性进行匹配。在绑定Queue与Exchange时指定一组键值对；当消息发送到Exchange时，RabbitMQ会取到该消息的headers（也是一个键值对的形式），对比其中的键值对是否完全匹配Queue与Exchange绑定时指定的键值对；如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。该类型的Exchange没有用到过（不过也应该很有用武之地），所以不做介绍。durability 持久化与非持久化队列如何识别？如上图，在Features字段里有一个D,就是持久化队列，英文durable（持久的）持久化队列和非持久化队列的区别是什么？持久化队列会被保存在磁盘中，固定并持久的存储，当Rabbit服务重启后，该队列会保持原来的状态在RabbitMQ中被管理，而非持久化队列不会被保存在磁盘中，Rabbit服务重启后队列就会消失。如何选择？如果需要队列的完整性，数据在队列中的保存是必须不允许丢失的，那么可以使用持久化。而当需要获取的信息是实时的，或者是随机的信息，不需要信息的精确性或完整性，但是追求获取性能，可以选择非持久化队列。分布式集群架构和高可用性用时：5 分钟设计集群的目的允许消费者和生产者在RabbitMQ节点崩溃的情况下继续运行通过增加更多的节点来扩展消息通信的吞吐量集群配置方式RabbitMQ可以通过三种方法来部署分布式集群系统，分别是：cluster,federation,shovelcluster:不支持跨网段，用于同一个网段内的局域网可以随意的动态增加或者减少节点之间需要运行相同版本的RabbitMQ和Erlangfederation:应用于广域网，允许单台服务器上的交换机或队列接收发布到另一台服务器上交换机或队列的消息，可以是单独机器或集群。federation队列类似于单向点对点连接，消息会在联盟队列之间转发任意次，直到被消费者接受。通常使用federation来连接internet上的中间服务器，用作订阅分发消息或工作队列。shovel:连接方式与federation的连接方式类似，但它工作在更低层次。可以应用于广域网RabbitMQ cluster 集群同步原理上面图中采用三个节点组成了一个RabbitMQ的集群，Exchange A的元数据信息在所有节点上是一致的，而Queue（存放消息的队列）的完整数据则只会存在于它所创建的那个节点上。，其他节点只知道这个queue的metadata信息和一个指向queue的owner node的指针。RabbitMQ集群元数据的同步RabbitMQ集群会始终同步四种类型的内部元数据（类似索引）：队列元数据：队列名称和它的属性；交换器元数据：交换器名称、类型和属性；绑定元数据：一张简单的表格展示了如何将消息路由到队列；vhost元数据：为vhost内的队列、交换器和绑定提供命名空间和安全属性；因此，当用户访问其中任何一个RabbitMQ节点时，通过rabbitmqctl查询到的queue／user／exchange/vhost等信息都是相同的。为何RabbitMQ集群仅采用元数据同步的方式一，存储空间，如果每个集群节点都拥有所有Queue的完全数据拷贝，那么每个节点的存储空间会非常大，集群的消息积压能力会非常弱（无法通过集群节点的扩容提高消息积压能力）；二，性能，消息的发布者需要将消息复制到每一个集群节点，对于持久化消息，网络和磁盘同步复制的开销都会明显增加。RabbitMQ cluster 集群的两种模式普通模式：默认的集群模式。镜像模式：把需要的队列做成镜像队列，存在于多个节点，属于RabbitMQ的HA方案普通模式：当消息进入A节点的Queue中后，consumer从B节点拉取时，RabbitMQ会临时在A、B间进行消息传输，把A中的消息实体取出并经过B发送给consumer，所以consumer应平均连接每一个节点，从中取消息。该模式存在一个问题就是当A节点故障后，B节点无法取到A节点中还未消费的消息实体。如果做了队列持久化或消息持久化，那么得等A节点恢复，然后才可被消费，并且在A节点恢复之前其它节点不能再创建A节点已经创建过的持久队列；如果没有持久化的话，消息就会失丢。这种模式更适合非持久化队列，只有该队列是非持久的，客户端才能重新连接到集群里的其他节点，并重新创建队列。假如该队列是持久化的，那么唯一办法是将故障节点恢复起来。为什么RabbitMQ不将队列复制到集群里每个节点呢？这与它的集群的设计本意相冲突，集群的设计目的就是增加更多节点时，能线性的增加性能（CPU、内存）和容量（内存、磁盘）。当然RabbitMQ新版本集群也支持队列复制（有个选项可以配置）。比如在有五个节点的集群里，可以指定某个队列的内容在2个节点上进行存储，从而在性能与高可用性之间取得一个平衡（应该就是指镜像模式）。镜像模式：其实质和普通模式不同之处在于，消息实体会主动在镜像节点间同步，而不是在consumer取数据时临时拉取。该模式带来的副作用也很明显，除了降低系统性能外，如果镜像队列数量过多，加之大量的消息进入，集群内部的网络带宽将会被这种同步通讯大大消耗掉。所以在对可靠性要求较高的场合中适用.节点类型RAM node:内存节点将所有的队列、交换机、绑定、用户、权限和vhost的元数据定义存储在内存中，好处是可以使得像交换机和队列声明等操作更加的快速。Disk node:将元数据存储在磁盘中，单节点系统只允许磁盘类型的节点，防止重启RabbitMQ的时候，丢失系统的配置信息。如果是内存结点这里就显示为RAM注意RabbitMQ要求在集群中至少有一个磁盘节点，所有其他节点可以是内存节点，当节点加入或者离开集群时，必须要将该变更通知到至少一个磁盘节点。如果集群中唯一的一个磁盘节点崩溃的话，集群仍然可以保持运行，但是无法进行其他操作(包括创建队列、交换器、绑定，添加用户、更改权限、添加和删除集群结点)，直到节点恢复。解决方案：设置两个磁盘节点，至少有一个是可用的，可以保存元数据的更改。Erlang CookieErlang Cookie是保证不同节点可以相互通信的密钥，要保证集群中的不同节点相互通信必须共享相同的Erlang Cookie。具体的目录存放在/var/lib/rabbitmq/.erlang.cookie。基本运维操作用时：8 分钟rabbitmq集群必要条件绑定实体ip，即ifconfig所能查询到的绑定到网卡上的ip,以下是绑定方法12#编辑配置路径 /etc/rabbitmq/rabbitmq-env.confNODE_IP_ADDRESS=172.16.136.133配置域名映射到实体ip123456789101112131415161718192021#配置文件1所在路径 /etc/rabbitmq/rabbitmq.config (如果是集群，每台机器都需要修改这个绑定本机实体ip)#其中rabbit@master是创建集群时所配置的参数，@后面的参数为主机名，示例中为master[ &#123;rabbit, [ &#123;cluster_nodes, &#123;[&apos;rabbit@master&apos;], disc&#125;&#125;, &#123;cluster_partition_handling, ignore&#125;, &#123;default_user, &lt;&lt;&quot;guest&quot;&gt;&gt;&#125;, &#123;default_pass, &lt;&lt;&quot;guest&quot;&gt;&gt;&#125;, &#123;tcp_listen_options, [binary, &#123;packet, raw&#125;, &#123;reuseaddr, true&#125;, &#123;backlog, 128&#125;, &#123;nodelay, true&#125;, &#123;exit_on_close, false&#125;, &#123;keepalive, true&#125;]&#125; ]&#125;, &#123;kernel, [ &#123;inet_dist_listen_max, 44001&#125;, &#123;inet_dist_listen_min, 44001&#125; ]&#125;].1234#配置文件2 所在路径 /etc/hosts (如果是集群，每台机器都需要修改这个绑定本机实体ip，而且hosts文件的映射不得重复，如果重复linux系统为以最下面一条记录为准)172.16.136.133 master172.16.136.134 venus172.16.136.135 venus2启动停止停止123456789#机器Aservice rabbitmq-server stopepmd -kill#机器Bservice rabbitmq-server stopepmd -kill#机器Cservice rabbitmq-server stopepmd -kill启动方式1123456#机器Aservice rabbitmq-server start#机器Bservice rabbitmq-server start#机器Cservice rabbitmq-server start方式21rabbitmq-server -detached集群重启顺序集群重启的顺序是固定的，并且是相反的。 如下所述：启动顺序：磁盘节点 =&gt; 内存节点关闭顺序：内存节点 =&gt; 磁盘节点最后关闭必须是磁盘节点，不然可能回造成集群启动失败、数据丢失等异常情况。重建集群注1：此处的mq集群重建是比较快速和有效的方法，面向的是初次安装或者可以接受mq中所存有的数据丢失的情况下，必须先有mq的.json后缀的配置文件或者有把握写入集群中exchange、queue等配置。按顺序停止所有机器中的rabbitmq123456789#机器Aservice rabbitmq-server stopepmd -kill#机器Bservice rabbitmq-server stopepmd -kill#机器Cservice rabbitmq-server stopepmd -kill移除rabbitmq配置记录与存储文件12#位于 /var/lib/rabbitmq/mensiamv /var/lib/rabbitmq/mensia /var/lib/rabbitmq/mensia.bak按顺序启动所有机器中的rabbitmq123456#机器Cservice rabbitmq-server start#机器Bservice rabbitmq-server start#机器Aservice rabbitmq-server start停止被加入集群节点app比如A、B、C三台机器，将B和C加入到A中去，需要执行以下命令1234#机器Brabbitmqctl stop_app#机器Crabbitmqctl stop_app建立集群注意此处master为唯一没有执行rabbitmqctl stop_app的机器1234#机器Brabbitmqctl join_cluster rabbit@master#机器Crabbitmqctl join_cluster rabbit@master启动集群1234#机器Brabbitmqctl start_app#机器Crabbitmqctl start_app检查集群状态在任意一台机器上执行rabbitmqctl cluster_status命令即可检查，输出包含集群中的节点与运行中的节点，兼以主机名标志添加集群配置创建用户例子中创建了两个用户添加用户add_user,设置角色set_user_tags,添加rabbitmq虚拟主机add_vhost，设置访问权限set_permissions,以下是详细用法123456789# 创建第一个用户/usr/sbin/rabbitmqctl add_user 用户名 密码/usr/sbin/rabbitmqctl set_user_tags 用户名 administrator/usr/sbin/rabbitmqctl set_permissions -p / 用户名 &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;# 创建第二个用户/usr/sbin/rabbitmqctl add_user 用户名2 密码/usr/sbin/rabbitmqctl set_user_tags 用户名2 management /usr/sbin/rabbitmqctl add_vhost sip_ext /usr/sbin/rabbitmqctl set_permissions -p sip_ext 用户名2 &apos;.*&apos; &apos;.*&apos; &apos;.*&apos;1备注：RabbitMQ 虚拟主机，RabbitMQ 通过虚拟主机（vhost）来分发消息。拥有自己独立的权限控制，不同的vhost之间是隔离的，单独的。1权限控制的基本单位：vhost。1用户只能访问与之绑定的vhost。1vhost是AMQP中唯一无法通过协议来创建的基元。只能通过rabbitmqctl工具来创建。打开15672网页管理端，访问mq/usr/sbin/rabbitmq-plugins enable rabbitmq_management备注：如果发现命令执行完毕没有打开此服务，15672端口没有监听，则是由于没有重启mq导致的在底部导入.json后缀的配置文件即可http://localhost:4000/first-blog/rabbitmq.jpg如果覆盖了用户需要使用以下命令修改mq用户密码/usr/sbin/rabbitmqctl change_password 用户名 密码修改节点类型1234567rabbitmqctl stop_apprabbitmqctl change_cluster_node_type distrabbitmqctl change_cluster_node_type ramrabbitmqctl start_app常用命令常见故障用时：3 分钟集群状态异常rabbitmqctl cluster_status检查集群健康状态，不正常节点重新加入集群分析是否节点挂掉，手动启动节点。保证网络连通正常队列阻塞、数据堆积保证网络连通正常保证消费者正常消费，消费速度大于生产速度保证服务器TCP连接限制合理脑裂按正确顺序重启集群保证网络连通正常保证磁盘空间、cpu、内存足够引用RabbitMQ简介、信道（channel）RabbitMQ基础概念详细介绍RabbitMQ的几种典型使用场景AMQP介绍RabbitMQ分布式集群架构RabbitMQ系列（六）你不知道的RabbitMQ集群架构全解消息中间件—RabbitMQ（集群原理与搭建篇)]]></content>
      <categories>
        <category>rabbitmq</category>
      </categories>
      <tags>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git合并不同url的项目]]></title>
    <url>%2Fgit-merge-remote-url%2F</url>
    <content type="text"><![CDATA[摘要：为了让项目能实现Git+Gerrit+Jenkin的持续集成，我们把项目从Git上迁移到了Gerrit上，发现有的同事在老Git提交代码，因为Gerrit做了同步，在Gerrit上有新提交的时候就会刷新老git，这样就会把他提交的代码冲掉。这个时候我就必须要在两个相似项目之间合并提交了。步骤使用命令git remote add [shortname] [url]将老Git url加到我们新Git的本地这里我把他取名为gitoa_web（随便取）使用命令git remot -v查看远程仓库的情况可以看到此处我们有三个远程仓库分别名为gerrit、 gitoa_web、origin使用命令git fetch gitoa_web刷新远程仓库到本地字符串 gitoa_web 指代对应的仓库地址了.比如说,要抓取所有 gitoa_web 有的,但本地仓库没有的信息,可以用使用命令git merge gitoa_web/master合并项目gitoa_web是指代仓库，master指代分支，当然如果有需要也可以合并别的分支过来发现不同email地址错误不能成功提交因为这个commit不是我的把email地址更新成我的再提交就成功了保留原有的commit用户在上一节我们先使用命令git remote add [shortname] [url]将老Git url加到我们新Git的本地使用命令git fetch gitoa_web刷新远程仓库到本地最后使用命令git merge gitoa_web/master将老项目合并到新项目上再提交这种在新项目的master上，合并老项目的方式会存在问题（就是如果不是自己的commit会过不了push），后来我遇到了项目进行迁移的需求，经过测试只要反过来，位于老的项目上，push到新的项目就不会出现这样的问题了。如下git clone 老项目git remote add gerrit 新项目git链接cd 项目名 此时我们就位于已有代码git push gerrit master此时就是把已有代码推于已有项目思考：为什么会出现这样的问题呢？因为在新的项目上合并老项目的代码，对于新项目来说是新的代码提交，所以只允许你一个人来提交如果在老项目上，给新项目推代码这种顺序就是已有代码推到已有仓库小结知识点：git merge还可以合并其他项目的到本项目git fetch 仓库名可以指定同步哪个仓库git remot -v查看本地有哪些远程仓库的情况，包含各个仓库url本次我们对以下命令加深了理解1234git remote #不带参数，列出已经存在的远程分支git remote -v #(-v是–verbose 的简写,取首字母)列出详细信息，在每一个名字后面列出其远程urlgit remote add [shortname] [url] #添加远程仓库git fetch origin #字符串 origin 指代对应的仓库地址了.比如说,要抓取所有 origin 有的,但本地仓库没有的信息,可以用ps: 这里git remote add以后，我认为还能用cherry-pick来加不同仓库的commit过来，有兴趣的朋友可以自己尝试。附Git常用命令]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>版本控制</tag>
        <tag>git教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络虚拟化技术——虚拟专用网 VPN]]></title>
    <url>%2FVPN%2F</url>
    <content type="text"><![CDATA[摘要：在虚拟化技术中，网络是必不可少的一个，它从上到下贯穿全局；它建网快速方便、容易对网络维护和管理、高可用、租户隔离等优点是企业管理、节约成本的秘诀，是网络安全的守门员。我们这一章说VPN，利用公用网络架设专用网络，实现在远程也可以安全的访问内部网络，可以理解为虚拟出来的企业内部专线，是一种安全性很高的技术。当然了，私人可以使用他来做科学上网的工具。定义虚拟专用网络的功能是：在公用网络上建立专用网络，进行加密通讯。在企业网络中有广泛应用。VPN网关通过对数据包的加密和数据包目标地址的转换实现远程访问。VPN有多种分类方式，主要是按协议进行分类。VPN可通过服务器、硬件、软件等多种方式实现。常见的vpn协议VPN的隧道协议主要有三种，PPTP、L2TP和IPSec，其中PPTP和L2TP协议工作在OSI模型的第二层，又称为二层隧道协议；IPSec是第三层隧道协议。使用场景远程访问虚拟网（AccessVPN）利用公用网络架设专用网络。例如某公司员工出差到外地，他想访问企业内网的服务器资源，这种访问就属于远程访问。原理是在内网中架设一台VPN服务器。外地员工在当地连上互联网后，通过互联网连接VPN服务器，然后通过VPN服务器进入企业内网。这中间为了保证数据安全，VPN服务器和客户机之间的通讯数据都进行了加密处理。有了数据加密，就可以认为数据是在一条专用的数据链路上进行安全传输，就如同专门架设了一个专用网络一样，但实际上VPN使用的是互联网上的公用链路，因此VPN称为虚拟专用网络，其实质上就是利用加密技术在公网上封装出一个数据通讯隧道。另一个场景就是我们比较常见的，由于我国对海外网络的限制及屏蔽,很多时候个人用户、外资公司、学术单位想要连回海外网站就要自行架设VPN或采用付费的VPN服务。企业内部虚拟网（IntranetVPN）Intranet VPN服务即企业的总部与分支机构间通过VPN虚拟网进行网络连接。随着企业的跨地区、国际化经营，这是绝大多数大、中型企业所必需的。如果要进行企业内部各分支机构的互联，使用Intranet VPN是很好的方式。这种VPN是通过公用因特网或者第三方专用网进行连接的，有条件的企业可以采用光纤作为传输介质。它的特点就是容易建立连接、连接速度快，最大特点就是它为各分支机构提供了整个网络的访问权限。IntranetVPN通过一个使用专用连接的共享基础设施，连接企业总部、远程办事处和分支机构。企业拥有与专用网络的相同政策，包括安全、服务质量(QoS)、可管理性和可靠性。企业扩展虚拟网（ExtranetVPN）Extranet VPN服务即企业间发生收购、兼并或企业间建立战略联盟后，使不同企业网通过公网来构筑的虚拟网。如果是需要提供B2B电子商务之间的安全访问服务，则可以考虑选用Extranet VPN。上图中下面那个人就代表客户、合作伙伴。随着信息时代的到来，各个企业越来越重视各种信息的处理。希望可以提供给客户最快捷方便的信息服务，通过各种方式了解客户的需要各个企业之间的合作关系也越来越多，信息交换日益频繁。因特网为这样的一种发展趋势提供了良好的基础，而如何利用因特网进行有效的信息管理，是企业发展中不可避免的一个关键问题。利用VPN技术可以组建安全的Extranet，既可以向客户、合作伙伴提供有效的信息服务，又可以保证自身的内部网络的安全。Extranet VPN通过一个使用专用连接的共享基础设施，将客户、供应商、合作伙伴或兴趣群体连接到企业内部网。企业拥有与专用网络的相同政策，包括安全、服务质量(QoS)、可管理性和可靠性。Extranet VPN服务对用户的吸引力在于:能容易地对外部网进行部署和管理，外部网的连接可以使用与部署内部网和远端访问VPN相同的架构和协议进行部署。主要的不同是接入许可，外部网的用户被许可只有一次机会连接到其合作人的网络，并且只拥有部分网络资源访问权限，这要求企业用户对各外部用户进行相应访问权限的设定。优点和缺点优点VPN能够让移动员工、远程员工、商务合作伙伴和其他人利用本地可用的高速宽带网连接（如DSL、有线电视或者WiFi网络）连接到企业网络。设计良好的宽带VPN是模块化的和可升级的。这种能力意味着企业不用增加额外的基础设施就可以提供大量的容量和应用。VPN能提供高水平的安全，使用高级的加密和身份识别协议保护数据避免受到窥探，阻止数据窃贼和其他非授权用户接触这种数据。完全控制，虚拟专用网使用户可以利用ISP的设施和服务，同时又完全掌握着自己网络的控制权。用户只利用ISP提供的网络资源，对于其它的安全设置、网络管理变化可由自己管理。在企业内部也可以自己建立虚拟专用网。缺点企业不能直接控制基于互联网的VPN的可靠性和性能。机构必须依靠提供VPN的互联网服务提供商保证服务的运行。这个因素使企业与互联网服务提供商签署一个服务级协议非常重要，要签署一个保证各种性能指标的协议。企业创建和部署VPN线路并不容易。这种技术需要高水平地理解网络和安全问题，需要认真的规划和配置。因此，应该大多数选择互联网服务提供商负责运行VPN。不同厂商的VPN产品和解决方案总是不兼容的，因为许多厂商不愿意或者不能遵守VPN技术标准。因此，混合使用不同厂商的产品可能会出现技术问题。另一方面，使用一家供应商的设备可能会提高成本。当使用无线设备时，VPN有安全风险。在接入点之间漫游特别容易出问题。当用户在接入点之间漫游的时候，任何使用高级加密技术的解决方案都可能被攻破。实现方式VPN服务器：在大型局域网中，可以通过在网络中心搭建VPN服务器的方法实现VPN。（vpn服务上使用openvpn easy-rsa或其他方式 ）软件VPN：可以通过专用的软件实现VPN。硬件VPN：可以通过专用的硬件实现VPN。集成VPN：某些硬件设备，如交换机、路由器、防火墙等，都含有VPN功能，但是一般拥有VPN功能的硬件设备通常都比没有这一功能的要贵。安全性L2TP协议和PPTP协议在数据传输过程中都存在一定安全隐患，基于IPSec协议的VPN技术可有效防止黑客入侵，保护用户的通信信息。因此，对于保密性要求较高的重要部门，建议选择使用基于IPSec协议实现的VPN，保证通信安全。腾讯公有云产品 与 VPN上图是腾讯云使用vpn连接私有云和公有云实现的弹性混合云服务，可以根据业务压力大小弹性伸缩。通过 VPN 连接同步云端和本地数据中心的数据，实现云上容灾，保证业务数据安全。法律上按我国法律规定不能以盈利为目的搭建vpn来连接internet使用vpn上外网不能违背中国社会主义的法律或者违规道德规范引用wiki 虛擬私人網路百度百科 虚拟专用网络虚拟专用网络(VPN)的安全性Difference between Intranet VPN and Extranet VPN | Intranet VPN vs Extranet VPN简析三种VPN服务类型]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>vpn</tag>
        <tag>虚拟网络</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[qq邮箱高频率邮件来源自动屏蔽的信任办法]]></title>
    <url>%2Fqqmail-filter%2F</url>
    <content type="text"><![CDATA[摘要：我们在使用QQ邮箱作为告警接收邮箱的时候，用了一段时间发现，告警邮箱再也没办法发出来了，非常的困惑，检查了全部的网络策略、dns、smtp服务器配置都没有问题，原来是qq邮箱的自动策略，在此记录。python 测试smtp脚本我们使用python的测试smtp脚本，无密码时使用，有密码时去掉注释1234567891011121314151617181920212223242526272829# -*- coding: utf-8 -*-from email import encodersfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.utils import parseaddr, formataddrimport smtplibdef _format_addr(s): name, addr = parseaddr(s) return formataddr(( \ Header(name, 'utf-8').encode(), \ addr.encode('utf-8') if isinstance(addr, unicode) else addr))from_addr = 'root@mymail.com'to_addr = ['xxxx@163.com','xxx.qq.com'] #password = ''smtp_server = 'mymail'msg = MIMEText('hello, send by Python...', 'plain', 'utf-8')msg['From'] = _format_addr(u'monitorcloud &lt;%s&gt;' % from_addr)msg['To'] = _format_addr(u'管理员 &lt;%s&gt;' % to_addr)msg['Subject'] = Header(u'来自SMTP的问候……', 'utf-8').encode()server = smtplib.SMTP(smtp_server, 25)server.set_debuglevel(1)#server.starttls()#server.login(from_addr, password)server.sendmail(from_addr, to_addr, msg.as_string())server.quit()发送情景发送到163邮箱是成功的但是发送到qq邮箱不行了查/var/log/maillog 发现有如上报错信息查QQ邮箱没有收到邮件有如上拦截信息解决办法进入反垃圾设置地址白名单和域名白名单测试成功附SMTP错误码1234567891011121314151617181920212223242526·The server rejected the message: 554 DT:SPM smtp2 Cannot send message 451 MI:SFQ 0,smtp9,DcCowLD735x2fG1MgWLhAA--.11609S2 1282243邮件服务器拒绝发送邮件，判断为发送垃圾邮件。建议您检查邮件内容，是否包含一些比较敏感的内容。·Cannot send message data: 550 5.4.5 Daily sending quota exceeded. q31sm4299413ybk.1不能发送邮件，每日发送额度用完。建议您控制此邮箱每天的发送量，今天发送量已经用完，需要明天才能继续发送。·Recipient rejected: &lt;xxx@yahoo.com&gt;: 553 5.7.1 &lt; xx@xx.com &gt;: Sender address rejected: not owned by user xx@xx.com发送邮箱地址被拒绝。建议您换个邮箱发送。·Recipient rejected: &lt;xxx@yahoo.co.uk&gt;: 550 Your mailbox is full？您的邮箱空间已满不能再发送邮件。建议您整理邮箱中的邮件。·Cannot send message: 451 sorry, server closed？不能发送，服务器已经关闭。建议您稍候再尝试邮件发送操作 。·Recipient rejected: &lt;xxx@yahoo.com&gt;: 500 Error: bad syntax收件人地址格式不正确。请核实收件人地址是否正确。·421 Service not available, closing transmission channel (This may be a reply to any command if the service knows it must shut down)服务暂时不可用！建议您稍候再尝试发送。·450 Requested mail action not taken: mailbox unavailable (E.g., mailbox busy)邮件发送请求没有发生：邮箱不可用（例如，邮箱忙）！建议您稍候再尝试！·451 Requested action aborted: local error in processing请求的邮件操作被中止：本地进程错误。建议您重新启动邮件营销助手来尝试发送·550 Requested action not taken: mailbox unavailable (E.g., mailbox not found, no access)？请求的操作未被执行：邮箱不可用（例如，邮箱未找到，不能访问）。建议您稍候再尝试。·552 Requested mail action aborted: exceeded storage allocation请求的邮件操作中止：超出存储分配。建议您整理邮箱中的邮件。· 553 Requested action not taken: mailbox name not allowed (E.g., mailbox syntax incorrect)请求的操作未被执行：不允许的邮箱名称（例如，邮箱的语法不正确的）；·550 5.3.4 Requested action not taken; This account is currently blocked from sending messages. If you don't think you've violated the Windows Live Terms of Use, please contact customer support请求不采取行动，这是目前从帐户发送的邮件被阻止。如果您不认为您已经违反了Windows Live的使用条款，请联系其客户支持。引用常见SMTP发送失败原因列表SMTP发送邮件]]></content>
      <categories>
        <category>qq邮箱策略</category>
      </categories>
      <tags>
        <tag>smtp</tag>
        <tag>qq邮箱</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用RSS订阅？]]></title>
    <url>%2Fhow_to_subscribe_RSS%2F</url>
    <content type="text"><![CDATA[摘要：我们常常会有订阅别人文章的需求，有更新的时候希望能有提醒的功能，RSS就是这样一个订阅的方式。很多网站上看到RSS的入口，点进去以后总是显示一堆的XML代码，我们来看看怎么使用这个功能。在本次的学习过后你将学会使用RSS来订阅别人的网站，而且你还能学会给不能用RSS网站的创建订阅，比如学校的教务网站发布重要的通知、新闻通知、公司网站通知、包括好看的电视剧更新就都能被你订阅了。定义RSS（简易信息聚合）是一种消息来源格式规范，用以聚合经常发布更新数据的网站，例如博客文章、新闻、音频或视频的网摘。RSS文件（或称做摘要、网络摘要、或频更新，提供到频道）包含全文或是节录的文字，再加上发布者所订阅之网摘数据和授权的元数据。Really Simple Syndication“简易信息聚合”就是RSS的英文原意。把新闻标题、摘要（Feed）、内容按照用户的要求，“送”到用户的桌面就是RSS的目的。RSS一词有时候大体上意为社会性书签，包括各种RSS的不同格式。例如，Blogspace对使用网摘于一集成器内之动作标为RSS info和RSS reader。虽然它的第一个句子就包含明确的Atom格式：“RSS和Atom文件能够用简单的格式从网站更新消息至你的计算机！”特点可以有选择地浏览您感兴趣的以及与您的工作相关的新闻。通过使用 RSS，您可以把需要的信息从不需要的信息（兜售信息，垃圾邮件等）中分离出来。通过使用 RSS，您可以创建自己的新闻频道，并将之发布到因特网。使用客户端，可以在不打开网站内容页面的情况下阅读支持RSS输出的网站内容。如何订阅浏览器方式这里介绍chrome的扩展程序，其他的浏览器大同小异。打开Chrome网上商店,并按装feeder插件安装完成后会自动提示注册一个账号，按照提示注册完成登录即可。完成插件安装之后，在浏览博客或者其他网站时，点击RSS订阅图标，就可以正常订阅成功。当有文章更新的时候，就会在浏览器右上角提示,打开后就可以看到全部订阅者了可以在reader看到比较舒服的阅读方式，也可以直接右上角打开文章来看在feeder的设置页面可以针对每个订阅者设置,可以选择刷新周期，提示方式（不过Email提示是要收费的），还可以导出你的订阅让好友来导入客户端方式阅读器推荐因为我觉得浏览器端的就够我用了，就没用过客户端的，下面是网上摘录的客户端阅读器irreade最好用，支持多平台。NewsGator Online一个免费的在线 RSS 阅读器。包含 Outlook 同步，通过 Media Center Edition 查看电视内容，以及 blog 和标题的发布。RssReade基于 Windows 的免费 RSS 阅读器。支持 RSS versions 0.9x、1.0 以及 2.0 和 Atom 0.1, 0.2 以及 0.3。FeedDemon基于 Windows 的 RSS 阅读器。使用很简便，界面很有条理。可以免费下载！blogbot一个针对 Outlook 或 Internet Explorer 的 RSS 阅读器插件。针对 Internet Explorer 的简化版是免费的。我已经有一个 RSS 阅读器了，接下来怎么做呢？点击您希望阅读的 RSS feed 旁边的橙色小图标 或 ，把浏览器窗口的 URL 拷贝粘贴到您的 RSS 阅读器即可。语法事实上我们根本不用关心RSS的语法是什么，因为多的是自动RSS的工具，如果使用的开源blog通常也会提供RSS自动生成的插件，以下是2.0版本语法12345678910111213141516&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;rss version="2.0"&gt;&lt;channel&gt; &lt;title&gt;pzqu的自言自语&lt;/title&gt; &lt;link&gt;https://qupzhi.com&lt;/link&gt; &lt;description&gt;我的描述&lt;/description&gt; &lt;item&gt; &lt;title&gt;如何用RSS订阅？&lt;/title&gt; &lt;link&gt;https://qupzhi.com/how_to_subscribe_RSS&lt;/link&gt; &lt;description&gt;摘要.....&lt;/description&gt; &lt;/item&gt; &lt;item&gt;...略&lt;/item&gt;&lt;/channel&gt;&lt;/rss&gt;感兴趣的同学可以看看官网,不同的版本语法可能略有差异。如何让你的网站支持RSS建议使用一些直接就支持自动生成RSS的开源blog，像WordPress、Blogger、Radio、Hexo，这里提供一个自动给网站生成RSS的方法，有了这种神器以后任何东西你都可以订阅了，比如电影网站，新闻网站，公司网站，学校通知。注意科学上网http://www.feed43.com提取HTML中关键内容进入创建页面，输入你想生成RSS的网址，这样可以拿到这个网站的HTML找到内容所在 HTML 字段规律。不同的文章会有相同的代码段落，比如我这里(为了好看我格式化了下)标题和url12345&lt;h1 class=&quot;post-title&quot; itemprop=&quot;name headline&quot;&gt; &lt;a class=&quot;post-title-link&quot; href=&quot;https://qupzhi.com/mariadb-better/&quot; itemprop=&quot;url&quot;&gt; mariadb 内存占用优化 &lt;/a&gt;&lt;/h1&gt;创建时间123&lt;time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-09T22:25:00+08:00"&gt; 2019-01-09&lt;/time&gt;摘要12345&lt;div class="post-body" itemprop="articleBody"&gt; &lt;p&gt; 摘要：我们在使用mariadb的时候发现有时候不能启动起来，在使用过程中mariadb占用的内存很大，在这里学习下mariadb与内存相关的配置项，对mariadb进行调优。 &lt;/p&gt;&lt;/div&gt;我们取到了标题、创建时间、摘要等比较关键的东西精简提取规则代码定义规则找到网页中你想要看的项目的列表代码(也就是文章的代码)，将你想要的字段用&#123;%&#125;代替为参数，不需要的可以用&#123;*&#125;代替，点击Extract，改到满意为止。这里的规则让我试到头疼，但是慢慢试就是能试出来的要注意一行一个规则比较清晰，这个系统也支持每行结束加一个&#123;*&#125;来忽略规则间不要的东西，如果有空格回车的也加上一个规则试好了再试另一个规则可以借助HTML格式化工具我的匹配结果如上图所示，每一篇文章的标题、链接、摘要、时间都已经成功抓取了。完善RSS源格式我们要完善好源格式，才能正确的输出源，在上面的语法那一节我们也说过了，一个完整的源要有网站title，link，description还要有每个文章的tile,link,time,description。来看文章的要怎么填变化的字段用 &#123;%+数字&#125; 的形式标示出来,也就是上面我的匹配结果里展示的内容，不同的 &#123;%+数字&#125; 之间可以填写随意的文字、符号过渡，如下ok的话点Preview就可以生成了看看结果获得你的RSS URLFeed URL就是你的RSS地址了，你可以放在任何地方Edit URL就是以后用来编辑你RSS的页面了下面可以把这个url的好记一些好了，这个url放到文章你的rss订阅器里就可以订阅了限制你可以免费使用 Feed43 做 RSS 源免费版不限制创建多少个rss源但是会有广告，每个源最大只能有250KB，更新后6小时才能刷新一次，最多100篇文章，匹配规则在30个内。其实大部分都够用了，这个是付费页面提供RSS在希望向外界提供 RSS的页面放一个RSS的图标，然后向这个按钮添加一个指向 RSS 文件的链接。代码应该类似这样：123&lt;a href="https://qupzhi.com/atom.xml"&gt;&lt;img src="https://user-gold-cdn.xitu.io/2019/1/11/1683b8918abb8f90?w=36&amp;h=14&amp;f=gif&amp;s=1036" width="36" height="14"&gt;&lt;/a&gt;附Hexo匹配规则123&lt;a class="post-title-link" href="&#123;%&#125;" itemprop="url"&gt;&#123;%&#125;&lt;/a&gt;&#123;*&#125;&lt;time title="创建于" itemprop="dateCreated datePublished" datetime="&#123;*&#125;"&gt;&#123;%&#125;&lt;/time&gt;&#123;*&#125;&lt;div class="post-body" itemprop="articleBody"&gt;&#123;*&#125;&lt;p&gt;&#123;%&#125;&lt;/p&gt;&#123;*&#125;&lt;/div&gt;推荐找到一个非常非常好用的，rss源基本所有主要的网站都能用了rsshub引用wiki RSSRSS 教程使用RSS订阅自动生成RSS利用 Feed43，将任意网页制作成 RSS 订阅源 | 一日一技]]></content>
      <categories>
        <category>RSS</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>RSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mariadb 内存占用优化]]></title>
    <url>%2Fmariadb-better%2F</url>
    <content type="text"><![CDATA[摘要：我们在使用mariadb的时候发现有时候不能启动起来，在使用过程中mariadb占用的内存很大，在这里学习下mariadb与内存相关的配置项，对mariadb进行调优。查询最高内存占用使用以下命令可以知道mysql的配置使用多少 RAM1234567891011121314SELECT ( @@key_buffer_size+ @@query_cache_size+ @@innodb_buffer_pool_size+ @@innodb_additional_mem_pool_size+ @@innodb_log_buffer_size+ @@max_connections * ( @@read_buffer_size+ @@read_rnd_buffer_size+ @@sort_buffer_size+ @@join_buffer_size+ @@binlog_cache_size+ @@thread_stack+ @@tmp_table_size)) / (1024 * 1024 * 1024) AS MAX_MEMORY_GB;可以使用mysql计算器来计算内存使用下面是理论，可以直接到推荐配置如何调整配置key_buffer_size（MyISAM索引用）指定索引缓冲区的大小，它决定索引处理的速度，尤其是索引读的速度。为了最小化磁盘的 I/O ， MyISAM 存储引擎的表使用键高速缓存来缓存索引，这个键高速缓存的大小则通过 key-buffer-size 参数来设置。如果应用系统中使用的表以 MyISAM 存储引擎为主，则应该适当增加该参数的值，以便尽可能的缓存索引，提高访问的速度。怎么设12345678show global status like 'key_read%';+------------------------+-------------+| Variable_name | Value |+------------------------+-------------+| Key_read_requests | 27813678764 || Key_reads | 6798830 |---------------------key_buffer_size通过检查状态值Key_read_requests和Key_reads，可以知道key_buffer_size设置是否合理。比例key_reads / key_read_requests应该尽可能的低，至少是1:100，1:1000更好。1show global status like '%created_tmp_disk_tables%';key_buffer_size只对MyISAM表起作用。即使你不使用MyISAM表，但是内部的临时磁盘表是MyISAM表，也要使用该值。可以使用检查状态值created_tmp_disk_tables得知详情。对于1G内存的机器，如果不使用MyISAM表，推荐值是16M（8-64M）另一个参考如下1234567show global status like 'key_blocks_u%';+------------------------+-------------+| Variable_name | Value |+------------------------+-------------+| Key_blocks_unused | 0 || Key_blocks_used | 413543 |+------------------------+-------------+Key_blocks_unused表示未使用的缓存簇(blocks)数，Key_blocks_used表示曾经用到的最大的blocks数，比如这台服务器，所有的缓存都用到了，要么增加key_buffer_size，要么就是过渡索引了，把缓存占满了。比较理想的设置：可以根据此工式来动态的调整Key_blocks_used / (Key_blocks_unused + Key_blocks_used) * 100% ≈ 80%1show engines;查询存储引擎innodb_buffer_pool_size （innodb索引用）这个参数和MyISAM的key_buffer_size有相似之处，但也是有差别的。这个参数主要缓存innodb表的索引，数据，插入数据时的缓冲。为Innodb加速优化首要参数。该参数分配内存的原则：这个参数默认分配只有8M，可以说是非常小的一个值。如果是专用的DB服务器，且以InnoDB引擎为主的场景，通常可设置物理内存的50%，这个参数不能动态更改，所以分配需多考虑。分配过大，会使Swap占用过多，致使Mysql的查询特慢。如果是非专用DB服务器，可以先尝试设置成内存的1/4，如果有问题再调整query_cache_size（查询缓存）缓存机制简单的说就是缓存sql文本及查询结果，如果运行相同的sql，服务器直接从缓存中取到结果，而不需要再去解析和执行sql。如果表更改了，那么使用这个表的所有缓冲查询将不再有效，查询缓存值的相关条目被清空。更改指的是表中任何数据或是结构的改变，包括INSERT、UPDATE、DELETE、TRUNCATE、ALTER TABLE、DROP TABLE或DROP DATABASE等，也包括那些映射到改变了的表的使用MERGE表的查询。显然，这对于频繁更新的表，查询缓存是不适合的，而对于一些不常改变数据且有大量相同sql查询的表，查询缓存会节约很大的性能。注意：如果你查询的表更新比较频繁，而且很少有相同的查询，最好不要使用查询缓存。因为这样会消耗很大的系统性能还没有任何的效果要不要打开？先设置成这样跑一段时间12query_cache_size=128M query_cache_type=1看看命中结果来进行进一步的判断1234567891011121314mysql&gt; show status like '%Qcache%';+-------------------------+-----------+| Variable_name | Value |+-------------------------+-----------+| Qcache_free_blocks | 669 || Qcache_free_memory | 132519160 || Qcache_hits | 1158 || Qcache_inserts | 284824 || Qcache_lowmem_prunes | 2741 || Qcache_not_cached | 1755767 || Qcache_queries_in_cache | 579 || Qcache_total_blocks | 1853 |+-------------------------+-----------+8 rows in set (0.00 sec)Qcache_free_blocks:表示查询缓存中目前还有多少剩余的blocks，如果该值显示较大，则说明查询缓存中的内存碎片过多了，可能在一定的时间进行整理。Qcache_free_memory:查询缓存的内存大小，通过这个参数可以很清晰的知道当前系统的查询内存是否够用，是多了，还是不够用，DBA可以根据实际情况做出调整。Qcache_hits:表示有多少次命中缓存。我们主要可以通过该值来验证我们的查询缓存的效果。数字越大，缓存效果越理想。Qcache_inserts: 表示多少次未命中然后插入，意思是新来的SQL请求在缓存中未找到，不得不执行查询处理，执行查询处理后把结果insert到查询缓存中。这样的情况的次数，次数越多，表示查询缓存应用到的比较少，效果也就不理想。当然系统刚启动后，查询缓存是空的，这很正常。Qcache_lowmem_prunes:该参数记录有多少条查询因为内存不足而被移除出查询缓存。通过这个值，用户可以适当的调整缓存大小。Qcache_not_cached: 表示因为query_cache_type的设置而没有被缓存的查询数量。Qcache_queries_in_cache:当前缓存中缓存的查询数量。Qcache_total_blocks:当前缓存的block数量。我们可以看到现网命中1158，未缓存的有1755767次，说明我们这个系统命中的太少了，表变动比较多，不什么开启这个功能涉及参数query_cache_limit：允许 Cache 的单条 Query 结果集的最大容量，默认是1MB，超过此参数设置的 Query 结果集将不会被 Cachequery_cache_min_res_unit：设置 Query Cache 中每次分配内存的最小空间大小，也就是每个 Query 的 Cache 最小占用的内存空间大小query_cache_size：设置 Query Cache 所使用的内存大小，默认值为0，大小必须是1024的整数倍，如果不是整数倍，MySQL 会自动调整降低最小量以达到1024的倍数query_cache_type：控制 Query Cache 功能的开关，可以设置为0(OFF),1(ON)和2(DEMAND)三种，意义分别如下：0(OFF)：关闭 Query Cache 功能，任何情况下都不会使用 Query Cache1(ON)：开启 Query Cache 功能，但是当 SELECT 语句中使用的 SQL_NO_CACHE 提示后，将不使用Query Cache2(DEMAND)：开启 Query Cache 功能，但是只有当 SELECT 语句中使用了 SQL_CACHE 提示后，才使用 Query Cachequery_cache_wlock_invalidate：控制当有写锁定发生在表上的时刻是否先失效该表相关的 Query Cache，如果设置为 1(TRUE)，则在写锁定的同时将失效该表相关的所有 Query Cache，如果设置为0(FALSE)则在锁定时刻仍然允许读取该表相关的 Query Cache。innodb_additional_mem_pool_size（InnoDB内部目录大小）InnoDB 字典信息缓存主要用来存放 InnoDB 存储引擎的字典信息以及一些 internal 的共享数据结构信息，也就是存放Innodb的内部目录，所以其大小也与系统中所使用的 InnoDB 存储引擎表的数量有较大关系。这个值不用分配太大，通常设置16Ｍ够用了，默认8M，如果设置的内存大小不够，InnoDB 会自动申请更多的内存，并在 MySQL 的 Error Log 中记录警告信息。innodb_log_buffer_size （日志缓冲）表示InnoDB写入到磁盘上的日志文件时使用的缓冲区的字节数，默认值为16M。一个大的日志缓冲区允许大量的事务在提交之前不用写日志到磁盘，所以如果有更新，插入或删除许多行的事务，则使日志缓冲区更大一些可以节省磁盘IO通常最大设为64M足够max_connections (最大并发连接)MySQL的max_connections参数用来设置最大连接（用户）数。每个连接MySQL的用户均算作一个连接，max_connections的默认值为100。这个参数实际起作用的最大值（实际最大可连接数）为16384，即该参数最大值不能超过16384，即使超过也以16384为准；增加max_connections参数的值，不会占用太多系统资源。系统资源（CPU、内存）的占用主要取决于查询的密度、效率等；该参数设置过小的最明显特征是出现”Too many connections”错误1234567891011121314151617181920mysql&gt; show variables like '%max_connect%';+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| extra_max_connections | 1 || max_connect_errors | 100 || max_connections | 2048 |+-----------------------+-------+3 rows in set (0.00 sec)mysql&gt; show status like 'Threads%';+-------------------+---------+| Variable_name | Value |+-------------------+---------+| Threads_cached | 0 || Threads_connected | 1 || Threads_created | 9626717 || Threads_running | 1 |+-------------------+---------+4 rows in set (0.00 sec)可以看到此时的并发数也就是Threads_connected=1，还远远达不到20481234567mysql&gt; show variables like 'open_files_limit';+------------------+-------+| Variable_name | Value |+------------------+-------+| open_files_limit | 65535 |+------------------+-------+1 row in set (0.00 sec)max_connections 还取决于操作系统对单进程允许打开最大文件数的限制也就是说如果操作系统限制单个进程最大可以打开100个文件那么 max_connections 设置为200也没什么用MySQL 的 open_files_limit 参数值是在MySQL启动时记录的操作系统对单进程打开最大文件数限制的值可以使用 show variables like ‘open_files_limit’; 查看 open_files_limit 值12ulimit -n65535或者直接在 Linux 下通过ulimit -n命令查看操作系统对单进程打开最大文件数限制 ( 默认为1024 )connection级内存参数(线程独享)connection级参数，是在每个connection第一次需要使用这个buffer的时候，一次性分配设置的内存。排序性能mysql对于排序,使用了两个变量来控制sort_buffer_size和 max_length_for_sort_data, 不象oracle使用SGA控制. 这种方式的缺点是要单独控制,容易出现排序性能问题.1234567891011mysql&gt; SHOW GLOBAL STATUS like '%sort%';+---------------------------+--------+| Variable_name | Value |+---------------------------+--------+| Sort_merge_passes | 0 || Sort_priority_queue_sorts | 1409 || Sort_range | 0 || Sort_rows | 843479 || Sort_scan | 13053 |+---------------------------+--------+5 rows in set (0.00 sec)如果发现Sort_merge_passes的值比较大，你可以考虑增加sort_buffer_size 来加速ORDER BY 或者GROUP BY 操作,不能通过查询或者索引优化的。我们这为0，那就没必要设置那么大。读取缓存read_buffer_size = 128K(默认128K)为需要全表扫描的MYISAM数据表线程指定缓存read_rnd_buffer_size = 4M：(默认256K)首先，该变量可以被任何存储引擎使用，当从一个已经排序的键值表中读取行时，会先从该缓冲区中获取而不再从磁盘上获取。大事务binlog12345678mysql&gt; show global status like 'binlog_cache%';+-----------------------+----------+| Variable_name | Value |+-----------------------+----------+| Binlog_cache_disk_use | 220840 || Binlog_cache_use | 67604667 |+-----------------------+----------+2 rows in set (0.00 sec)Binlog_cache_disk_use表示因为我们binlog_cache_size设计的内存不足导致缓存二进制日志用到了临时文件的次数Binlog_cache_use 表示 用binlog_cache_size缓存的次数当对应的Binlog_cache_disk_use 值比较大的时候 我们可以考虑适当的调高 binlog_cache_size 对应的值如上图，现网是32K，我们加到64Kjoin语句内存影响如果应用中，很少出现join语句，则可以不用太在乎join_buffer_size参数的设置大小。如果join语句不是很少的话，个人建议可以适当增大join_buffer_size到1MB左右，如果内存充足可以设置为2MB。线程内存影响Thread_stack：每个连接线程被创建时，MySQL给它分配的内存大小。当MySQL创建一个新的连接线程时，需要给它分配一定大小的内存堆栈空间，以便存放客户端的请求的Query及自身的各种状态和处理信息。12345678910111213141516171819202122mysql&gt; show status like '%threads%';+-------------------------+---------+| Variable_name | Value |+-------------------------+---------+| Delayed_insert_threads | 0 || Slow_launch_threads | 0 || Threadpool_idle_threads | 0 || Threadpool_threads | 0 || Threads_cached | 0 || Threads_connected | 1 || Threads_created | 9649301 || Threads_running | 1 |+-------------------------+---------+8 rows in set (0.00 sec)mysql&gt; show status like 'connections';+---------------+---------+| Variable_name | Value |+---------------+---------+| Connections | 9649311 |+---------------+---------+1 row in set (0.00 sec)如上：系统启动到现在共接受到客户端的连接9649311次，共创建了9649301个连接线程，当前有1个连接线程处于和客户端连接的状态。而在Thread Cache池中共缓存了0个连接线程(Threads_cached)。Thread Cache 命中率：1Thread_Cache_Hit = (Connections - Threads_created) / Connections * 100%;一般在系统稳定运行一段时间后，Thread Cache命中率应该保持在90%左右才算正常。内存临时表tmp_table_size 控制内存临时表的最大值,超过限值后就往硬盘写，写的位置由变量 tmpdir 决定max_heap_table_size 用户可以创建的内存表(memory table)的大小.这个值用来计算内存表的最大行数值。Order By 或者Group By操作多的话，加大这两个值，默认16M123456789mysql&gt; show status like 'Created_tmp_%';+-------------------------+-------+| Variable_name | Value |+-------------------------+-------+| Created_tmp_disk_tables | 0 || Created_tmp_files | 626 || Created_tmp_tables | 3 |+-------------------------+-------+3 rows in set (0.00 sec)如上图，写入硬盘的为0，3次中间表，说明我们的默认值足够用了mariadb 推荐配置注意这里只推荐innodb引擎内存配置只关注有注释的行12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockdefault-storage-engine=INNODBcharacter-set-server=utf8collation-server=utf8_general_ciuser=mysqlsymbolic-links=0# global settingstable_cache=65535table_definition_cache=65535max_allowed_packet=4Mnet_buffer_length=1Mbulk_insert_buffer_size=16Mquery_cache_type=0 #是否使用查询缓冲,0关闭query_cache_size=0 #0关闭，因为改表操作多，命中低，开启消耗cpu# sharedkey_buffer_size=8M #保持8M MyISAM索引用innodb_buffer_pool_size=4G #DB专用mem*50%，非DB专用mem*15%到25%myisam_sort_buffer_size=32Mmax_heap_table_size=16M #最大中间表大小tmp_table_size=16M #中间表大小# per-threadsort_buffer_size=256K #加速排序缓存大小read_buffer_size=128k #为需要全表扫描的MYISAM数据表线程指定缓存read_rnd_buffer_size=4M #已排序的表读取时缓存，如果比较大内存就到6Mjoin_buffer_size=1M #join语句多时加大，1-2Mthread_stack=256k #线程空间，256K or 512Kbinlog_cache_size=64K #大事务binlog# big-tablesinnodb_file_per_table = 1skip-external-lockingmax_connections=2048 #最大连接数skip-name-resolve# slow_query_logslow_query_log_file = /var/log/mysql-slow.loglong_query_time = 30group_concat_max_len=65536# according to tuning-primer.shthread_cache_size = 8thread_concurrency = 16# set variablesconcurrent_insert=2运行时修改使用以下命令来修改变量123set global &#123;要改的key&#125; = &#123;值&#125;; （立即生效重启后失效）set @@&#123;要改的key&#125; = &#123;值&#125;; （立即生效重启后失效）set @@global.&#123;要改的key&#125; = &#123;值&#125;; （立即生效重启后失效）试验12345678910111213141516mysql&gt; set @@global.innodb_buffer_pool_size=4294967296;ERROR 1238 (HY000): Variable 'innodb_buffer_pool_size' is a read only variablemysql&gt; set @@global.thread_stack=262144;ERROR 1238 (HY000): Variable 'thread_stack' is a read only variablemysql&gt; set @@global.binlog_cache_size=65536;Query OK, 0 rows affected (0.00 sec)mysql&gt; set @@join_buffer_size=1048576;Query OK, 0 rows affected (0.00 sec)mysql&gt; set @@read_rnd_buffer_size=4194304;Query OK, 0 rows affected (0.00 sec)mysql&gt; set @@sort_buffer_size=262144;Query OK, 0 rows affected (0.00 sec)mysql&gt; set @@read_buffer_size=131072;Query OK, 0 rows affected (0.00 sec)mysql&gt; set global key_buffer_size=8388608;Query OK, 0 rows affected (0.39 sec)我们可以看到innodb_buffer_pool_size和thread_stack报错了，他们只能改配置文件，在运行时是只读的。以下直接复制使用123456set @@global.binlog_cache_size=65536;set @@join_buffer_size=1048576;set @@read_rnd_buffer_size=4194304;set @@sort_buffer_size=262144;set @@read_buffer_size=131072;set global key_buffer_size=8388608;引用记一次Mysql占用内存过高的优化过程mysql 优化技巧心得一(key_buffer_size设置)mysql内存计算mysql计算器mariadb官网]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vimdiff]]></title>
    <url>%2Fvimdiff%2F</url>
    <content type="text"><![CDATA[摘要：我们在windows平台上用过beyond compare来进行文件比较，在linux平台上也有类似的东西，还是免费的。那就是vimdiff，它是vim的diff模式，依赖于diff命令。在文件比较方便比diff要强大的多，它有简单明了的界面以及对比结果一目了然的特点，容易对多处差异进行对比和合并。启动方法保证安装了vim和diff命令使用以下方法启动12vimdiff file1 file2 #垂直vimdiff -o file1 file2 #水平切换视角12345Ctrl-w w #在不同窗口间跳转Ctrl-w K #把当前窗口移到最上边Ctrl-w H #把当前窗口移到最左边Ctrl-w J #把当前窗口移到最下边Ctrl-w L #把当前窗口移到最右边其中K和J两个操作会把窗口改成水平分割方式。对比差异只在某一文件中存在的行的背景色被设置为蓝色，而在另一文件中的对应位置被显示为绿色。两个文件中都存在，但是包含差异的行显示为粉色背景，引起差异的文字用红色背景加以突出。+-- 7 lines: #include &lt;stdio.h&gt;------------------- 表示折叠的行 可以用zo（open）可以把折叠的行打开,使用zc(close)可以把折叠的行关闭------------------------------------------- 表示删除的行上下文的展开和查看：比较和合并文件的时候经常需要结合上下文来确定最终要采取的操作。Vimdiff 缺省是会把不同之处上下各 6 行的文本都显示出来以供参考。其他的相同的文本行被自动折叠。如果希望修改缺省的上下文行数，可以这样设置(设置上下文为3行)：1:set diffopt=context:3光标可以使用快捷键在各个差异点之间快速移动。]c 跳转到下一个差异点[c 跳转到上一个差异点2]c如果在命令前加上数字的话，可以跳过一个或数个差异点，从而实现跳的更远。比如如果在位于第一个差异点的行输入2]c，将越过下一个差异点，跳转到第三个差异点。合并用到的命令：dp （diff “put”）如果希望把另一个文件的内容复制到当前行中，可以使用命令do (diff “get”，之所以不用dg，是因为dg已经被另一个命令占用了):diffupdate 在修改一个或两个文件之后，vimdiff会试图自动来重新比较文件，来实时反映比较结果。但是也会有处理失败的情况，这个时候需要手工来刷新比较结果&lt;ESC&gt;, u 如果希望撤销修改，可以和平常用vim编辑一样，直接但是要注意一定要将光标移动到需要撤销修改的文件窗口中。备注： 如果有多个窗口的话只要在dp、do命令前加数字代表把当前行复制到哪个窗口中或者把哪个窗口中的复制到当前窗口引用vimdiff的常用命令技巧：Vimdiff 使用]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用腾讯云服务器搭建vpn]]></title>
    <url>%2Fcreate_vpn%2F</url>
    <content type="text"><![CDATA[摘要：最近腾讯云发了优惠券那就给自己搭建一个vpn吧，便于随时上外网。服务器环境腾讯云centos7.2 64位服务器上配置首先查看系统是否支持pptpd服务：1modprobe ppp-compress-18 &amp;&amp; echo yes安装ppp , pptpd，iptables123yum install -y ppp pptpd iptablessystemctl mask firewalldsystemctl stop firewalld修改配制123vim /etc/pptpd.conf #找到配制文件中默认的值，去掉注释即可localip 192.168.0.2-238,192.168.0.245remoteip 192.168.1.2-238,192.168.1.245需要注意的是：remoteip最好不用和VPN client本身所在的局域网的ip冲突。修改DNS123vim /etc/ppp/options.pptpd #末尾添加dnsms-dns 8.8.8.8ms-dns 114.114.114.114添加vpn账户123vim /etc/ppp/chap-secrets# client server secret IP addresses user pptpd passwd *开启路由转发123vim /etc/sysctl.confnet.ipv4.ip_forward = 1 #添加在配制文件的末尾即可sysctl -p #运行这个命令会输出上面添加的那一行信息，意思是使内核修改生效在防火墙上开启nat转发1iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -o eth0 -j MASQUERADE #IP和网口根据实际情况修改即可开启服务123service iptables save #正常来说要做这个来使iptables生效systemctl restart iptablessystemctl restart pptpd检查是否成功12netstat -anlpt | grep pptptcp 0 0 0.0.0.0:1723 0.0.0.0:* LISTEN 27202/pptpd如果不能访问的话，看系统日志位置安全组加一个入站规则手机上连接手机上找到vpn新建，pptp协议服务器ip: 你的服务器ip用户名：你刚刚设置的用户名/etc/ppp/chap-secrets下密码: 你刚刚设置的密码/etc/ppp/chap-secrets下mac上连接我发现新的MacOS不支持PPTP协议，先用shimo，等试用期到了再看要不要加L2TP]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>vpn</tag>
        <tag>PPTP</tag>
        <tag>虚拟网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[base64]]></title>
    <url>%2Fbase64%2F</url>
    <content type="text"><![CDATA[欢迎来到我的博客, 请输入密码来访问这篇文章. Incorrect Password! No content to display! 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]]></content>
      <categories>
        <category>base64</category>
      </categories>
      <tags>
        <tag>base64</tag>
        <tag>编码</tag>
        <tag>加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[版本控制工具——Git常用命令]]></title>
    <url>%2Fgit-start2%2F</url>
    <content type="text"><![CDATA[摘要：我们说过了git的几乎全部的常用情况，相信基本已经可以在平时团队协作开发的过程中游刃有余了。我熟悉了git的使用以后，这里给出git常用的命令，以下的命令几乎覆盖了所有的git常用操作，在此记录便于快速查找使用。配置相关123456789101112131415161718git init # 初始化本地git仓库（创建新仓库）git config --global user.name "xxx" # 配置用户名git config --global user.email "xxx@xxx.com" # 配置邮件// git status等命令自动着色git config --global color.ui true git config --global color.status autogit config --global color.diff autogit config --global color.branch autogit config --global color.interactive auto// 查看当前代理设置git config --global http.proxy// 设置当前代理为 http://127.0.0.1:1080 或 socket5://127.0.0.1:1080git config --global http.proxy 'http://127.0.0.1:1080'git config --global https.proxy 'http://127.0.0.1:1080'git config --global http.proxy 'socks5://127.0.0.1:1080'git config --global https.proxy 'socks5://127.0.0.1:1080' git config --global --unset http.proxy #删除 proxy git config代码文件与提交相关1234567891011121314151617181920git status # 查看当前版本状态（是否修改）git add xyz # 添加xyz文件至indexgit add . # 增加当前子目录下所有更改过的文件至indexgit commit -m 'xxx' # 提交git commit --amend -m 'xxx' # 合并上一次提交（用于反复修改）git commit -am 'xxx' # 将add和commit合为一步git rm xxx # 删除index中的文件git rm -r * # 递归删除git log # 显示提交日志git log -1 # 显示1行日志 -n为n行git log -5git log --stat # 显示提交日志及相关变动文件git log -p -mgit log -- filename # 查看文件的修改日志 git show dfb02e6e4f2f7b573337763e5c0013802e392818 # 显示某个提交的详细内容git show dfb02 # 可只用commitid的前几位git show HEAD # 显示HEAD提交日志git show HEAD^ # 显示HEAD的父（上一个版本）的提交日志 ^^为上两个版本 ^5为上5个版本git whatchanged # 显示提交历史对应的文件修改git revert dfb02e6e4f2f7b573337763e5c0013802e392818 # 撤销提交dfb02e6e4f2f7b573337763e5c0013802e392818tag相关1234567git tag # 显示已存在的taggit tag -a v2.0 -m 'xxx' # 增加v2.0的taggit show v2.0 # 显示v2.0的日志及详细内容git log v2.0 # 显示v2.0的日志git push --tags # 把所有tag推送到远程仓库git tag -d tag_name # 本地删除名为tag_name的taggit push origin :refs/tags/tag_name # 远程删除名为tag_name的tag差异比较相关123456git diff # 显示所有未添加至index的变更git diff --cached # 显示所有已添加index但还未commit的变更git diff HEAD^ # 比较与上一个版本的差异git diff HEAD -- ./lib # 比较与HEAD版本lib目录的差异git diff origin/master..master # 比较远程分支master上有本地分支master上没有的git diff origin/master..master --stat # 只显示差异的文件，不显示具体内容分支相关12345678910111213141516171819202122232425262728293031git clone git+ssh://git@192.168.53.168/VT.git # clone远程仓库git remote add origin git+ssh://git@192.168.53.168/VT.git # 增加远程定义（用于push/pull/fetch）git branch # 显示本地分支git branch --contains 50089 # 显示包含提交50089的分支git branch -a # 显示所有分支git branch -r # 显示所有原创分支git branch --merged # 显示所有已合并到当前分支的分支git branch --no-merged # 显示所有未合并到当前分支的分支git branch -m master master_copy # 本地分支改名git checkout -b master_copy # 从当前分支创建新分支master_copy并检出git checkout -b master master_copy # 上面的完整版git checkout features/performance # 检出已存在的features/performance分支git checkout --track hotfixes/BJVEP933 # 检出远程分支hotfixes/BJVEP933并创建本地跟踪分支git checkout v2.0 # 检出版本v2.0git checkout -b devel origin/develop # 从远程分支develop创建新本地分支devel并检出git checkout -- README # 检出head版本的README文件（可用于修改错误回退）git merge origin/master # 合并远程master分支至当前分支git cherry-pick ff44785404a8e # 合并提交ff44785404a8e的修改git push origin master # 将当前分支push到远程master分支git push origin :hotfixes/BJVEP933 # 删除远程仓库的hotfixes/BJVEP933分支git fetch # 获取所有远程分支（不更新本地分支，另需merge）git fetch --prune # 获取所有原创分支并清除服务器上已删掉的分支git pull origin master # 获取远程分支master并merge到当前分支git mv README README2 # 重命名文件README为README2git reset --hard HEAD # 将当前版本重置为HEAD（通常用于merge失败回退）git rebasegit branch -d hotfixes/BJVEP933 # 删除分支hotfixes/BJVEP933（本分支修改已合并到其他分支）git branch -D hotfixes/BJVEP933 # 强制删除分支hotfixes/BJVEP933，小心操作git ls-files # 列出git index包含的文件git show-branch # 图示当前分支历史git show-branch --all # 图示所有分支历史图示命令123456789git ls-tree HEAD # 内部命令：显示某个git对象git rev-parse v2.0 # 内部命令：显示某个ref对于的SHA1 HASHgit reflog # 显示所有提交，包括孤立节点git show HEAD@&#123;5&#125;git show master@&#123;yesterday&#125; # 显示master分支昨天的状态git log --pretty=format:'%h %s' --graph # 图示提交日志git show HEAD~3git show -s --pretty=raw 2be7fcb476暂存相关1234git stash # 暂存当前修改，将所有至为HEAD状态git stash list # 查看所有暂存git stash show -p stash@&#123;0&#125; # 参考第一次暂存git stash apply stash@&#123;0&#125; # 应用第一次暂存查找12git grep "delete from" #查找当前分支下的文件内容，可以git grep --help看具体用法 git grep "delete from" v2.0 #指定tag来查找git index操作1234git update-index —assume-unchanged 文件名 #取消本地跟踪git update-index —no-assume-unchanged 文件名 #恢复本地跟踪git ls-files -v| grep '^h\ ' #可以看到本地不跟踪的文件管理远程分支1234git remote #不带参数，列出已经存在的远程分支git remote -v #(-v是–verbose 的简写,取首字母)列出详细信息，在每一个名字后面列出其远程urlgit remote add [shortname] [url] #添加远程仓库git fetch origin #字符串 origin 指代对应的仓库地址了.比如说,要抓取所有 origin 有的,但本地仓库没有的信息,可以用注意事项理论上，git日常用到的命令是 diff show fetch rebase pull push checkout commit status 等，这些命令都不会导致代码丢失，假如害怕代码丢失，可以预先commit一次，再进行修改，但切记不可使用自己不熟悉的命令任何命令，不要加上-f的强制参数，否则可能导致代码丢失建议多使用命令行，不要使用图形界面操作引用git官网廖雪峰的官方网站-git篇hexo博客部署到vps]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>版本控制</tag>
        <tag>git教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[版本控制工具——Git常用操作（下）]]></title>
    <url>%2Fgit-start3%2F</url>
    <content type="text"><![CDATA[摘要：上一集我们一起入门学习了git的基本概念和git常用的操作，包括提交和同步代码、使用分支、出现代码冲突的解决办法、紧急保存现场和恢复现场的操作。学会以后已经足够我们使用Git参加协作开发了，但是在开发的过程中难免会出错，本文主要介绍版本控制的过程中出错了的场景，以及Git开发的一些技巧，让我们用的更流畅。上集回顾：Git的基本概念一个人使用Git时的代码版本控制–（提交、拉代码、分支操作）多人合作时的代码版本控制–（合并冲突、暂存代码）上集传送门：版本控制工具——Git常用操作（上）本文核心：后悔药-各种后悔操作（撤消commit,回滚，回退远程仓库等）哎呀，提交的时候漏了文件tag操作git忽略不想提交的文件后悔药撤消当前commit如果你发现刚刚的操作一不小心commit了，所幸你还没有推送到远程仓库，你可以用reset命令来撤消你的这次提交。reset命令的作用：重置HEAD(当前分支的版本顶端）到另外一个commit。我们的撤消当前提交的时候往往不希望我们此次提交的代码发生任何丢失，只是撤消掉commit的操作，以便我们继续修改文件。如果我们是想直接不要了这次commit的全部内容的任何修改我们将在下一小节讨论。来，我们先说一句蠢话来diss老板12345678910111213141516171819$ touch to_boss.txt$ echo 'my boss is a bad guy!' &gt; to_boss.txt$ git add to_boss.txt$ git statusOn branch masterYour branch is up to date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: to_boss.txt$ git commit -m "[+]骂了我的boss"[master 3d113a7] [+]骂了我的boss 1 file changed, 1 insertion(+) create mode 100644 to_boss.txt创建to_boss.txt文件，并向其写入了my boss is a bad guy!add然后status查看新文件已经加入跟踪commit提交了这次的修改好了，刚刚我们“不小心”diss了我们的老板，要是被发现就完了，所幸还没有push，要快点撤消这些提交，再换成一些好话才行。我们使用以下命令：123456789101112131415161718192021222324252627282930313233343536373839$ git reset --soft head^$ git statusOn branch masterYour branch is behind 'origin/master' by 1 commit, and can be fast-forwarded. (use "git pull" to update your local branch)Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: to_boss.txt$ cat to_boss.txtmy boss is a bad guy!$ echo 'my boss is a good boy!'my boss is a good boy!$ echo 'my boss is a good boy!' &gt; to_boss.txt$ cat to_boss.txtmy boss is a good boy!$ git add to_boss.txt$ git statusOn branch masterYour branch is behind 'origin/master' by 1 commit, and can be fast-forwarded. (use "git pull" to update your local branch)Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: to_boss.txt $ git commit -m "[*]夸了我的boss"[master 8be46aa] [*]夸了我的boss 1 file changed, 1 insertion(+) create mode 100644 to_boss.txtgit reset --soft head^撤消了本次提交，将工作区恢复到了提交前但是已经add的状态将to_boss.txt的内容改成了my boss is a good boy!add然后commit提交好了，有惊无险，这就是撤消commit的操作。另一种情况是如果你想撤消commit的时候支持舍弃这次全部的修改就把git reset --soft head^改成git reset --hard head^，这样你本地修改就彻底丢掉了(慎用)，如果真用了想找回来怎么办？见救命的后悔药。当然了，你只要开心不加soft或hard参数也是安全的(相当于使用了--mixed参数)，只不过是撤消以后你的本次修改就会回到add之前的状态，你可以重新检视然后再做修改和commit。回退远程仓库要是我们做的更过分一点，直接把这次commit直接给push怎么办？要是被发现就全完了,我们来看看github上的远程仓库。完了，真的提交了（我刚刚push的）让我们冷静下来，用撤消当前commit的方法先撤消本地的commit,这次我们来试试用hard参数来撤消1234567891011121314$ git reset --hard head^HEAD is now at 3f22a06 [+]add file time.txt$ git statusOn branch masterYour branch is behind 'origin/master' by 1 commit, and can be fast-forwarded. (use "git pull" to update your local branch)nothing to commit, working tree clean$ git push origin master --forceTotal 0 (delta 0), reused 0 (delta 0)To github.com:pzqu/git_test.git + 3d113a7...3f22a06 master -&gt; master (forced update)使用git reset --hard head^回滚到上一个commit使用git status查看现在的工作区情况，提示Your branch is behind &#39;origin/master&#39; by 1 commit,代表成功表了上一次的提示状态，nothing to commit, working tree clean代表这次的修改全没了，清理的算是一个彻底。如果还想找回来怎么办，我们还真是有办法让你找回来的，见救命的后悔药。git push origin master --force 命令强制提交到远程仓库(注意，如果是在团队合作的情况下，不到迫不得已不要给命令加–force参数)让我们看看github真的撤消了远程仓库，长舒一口气。暂存区（Stage）到工作区（Working Directory）如果我们刚刚执行了git reset --soft或者add等的操作，把一些东西加到了我们的暂存区，比如日志文件,我们就要把他们从暂存区拿出来。123456789101112131415161718192021$ git statusOn branch masterYour branch is up to date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: mysql.log $ git reset -- mysql.log$ git statusOn branch masterYour branch is up to date with 'origin/master'.Untracked files: (use "git add &lt;file&gt;..." to include in what will be committed) mysql.lognothing added to commit but untracked files present (use "git add" to track)status查看暂存区，里面有一个mysql.log被放进去了git reset -- mysql.log把mysql.log取出来status可以看到真的取出来了然后如果不要想这个文件的话再rm掉就好啦,但是如果这些文件每次自动生成都要用这种方式取出暂存区真的好累，我们可以用 git忽略不想提交的文件回滚文件到某个提交当我们想要把某个文件任意的回滚到某次提交上，而不改变其他文件的状态我们要怎么做呢？我们有两种情况，一种是，只是想在工作区有修改的文件，直接丢弃掉他现在的修改；第二种是想把这个文件回滚到以前的某一次提交。我们先来说第一种：取消文件在工作区的修改123456789101112131415161718192021222324$ cat time.txt10:41$ echo 18:51 &gt; time.txt$ git statusOn branch masterYour branch is up to date with 'origin/master'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: time.txtno changes added to commit (use "git add" and/or "git commit -a")$ cat time.txt18:51$ git checkout -- time.txt$ cat time.txt10:41更新time.txt的内容，可以status看到他发生了变化git checkout -- time.txt , 取消这次在工作区的修改，如果他已经被add加到了暂存区，那么这个命令就没有用了，他的意思是取消本次在工作区的修改，去上一次保存的地方。如果没有add就回到和版本库一样的状态；如果已经加到了暂存区，又做了修改，那么就回加到暂存区后的状态将文件回滚到任意的版本我们这里说的把文件回滚到以前的某个版本的状态，完整的含义是保持其他文件的内容不变，改变这个文件到以前的某个版本，然后修改到自己满意的样子和做下一次的提交。核心命令1git checkout [&lt;options&gt;] [&lt;branch&gt;] -- &lt;file&gt;...我们还是用time.txt这个文件来做试验,先搞三个版本出来，在这里我已经搞好了，来看看：版本1，time.txt内容00:501234commit 35b66ed8e3ae2c63cc4ebf323831e3b917d2b1d4 (HEAD -&gt; master, origin/master, origin/HEAD)Author: pzqu &lt;pzqu@example.com&gt;Date: Sun Dec 23 00:51:54 2018 +0800 [*]update time to 00:50版本2，time.txt内容18:511234commit 856a74084bbf9b678467b2615b6c1f6bd686ecffAuthor: pzqu &lt;pzqu@example.com&gt;Date: Sat Dec 22 19:39:19 2018 +0800 [*]update time to 18:51版本3，time.txt内容10:411234commit 3f22a0639f8d79bd4e329442f181342465dbf0b6Author: pzqu &lt;pzqu@example.com&gt;Date: Tue Dec 18 10:42:29 2018 +0800 [+]add file time.txt现在的是版本1，我们把版本3检出试试。12345678910111213$ git checkout 3f22a0639f8d -- time.txt$ cat time.txt10:41$ git statusOn branch masterYour branch is up to date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) modified: time.txt使用checkout+commit id+-- filename的组合，横跨版本2把历史版本3的time.txt搞出来了查看状态,time.txt被改变了我们来把time.txt恢复到版本1，同样的方法，因为版本1是上一次提交我们可以省略掉版本号1234$ git checkout -- time.txt$ cat time.txt00:50看到了吧！只要用git checkout commit_id -- filename的组合，想搞出哪个文件历史版本就搞出哪个。到了这里，你可能会很懵比,reset和checkout命令真的好像啊！都可以用来做撤消checkout语义上是把什么东西取出来，所以此命令用于从历史提交（或者暂存区域）中拷贝文件到工作目录，也可用于切换分支。reset语义上是重新设置，所以此命令把当前分支指向另一个位置，并且有选择的变动工作目录和索引。也用来在从历史仓库中复制文件到索引，而不动工作目录。还想不通可以给我发邮件：pzqu@qq.com救命的后悔药来到这里我已经很清楚的你的现况了，你的代码丢了现在一定非常的着急，不要慌，总是有办法找回他们的。但是前提是要保证你的项目根目录下.git文件夹是完整的，要是手动删除了里面的一些东西那就真完了。还要保证一点，你的代码以前是有过git追踪的，最少add过找回你丢失的历史记录Git提供了一个命令git reflog用来记录你的每一次命令，贴个图吧直观点：有没有发现，git reflog里的全部都是和改变目录树有关的，比如commit rebase reset merge，也就是说一定要有改变目录树的操作才恢复的回来像add这种操作就不能恢复了吗？那肯定不是，只是要用更麻烦点的方式来恢复git log是一样的，也可以看到所有分支的历史提交，不一样的是看不到已经被删除的 commit 记录和 reset rebase merge 的操作我们可以看到git reflog前面的就是commit id，现在我们就可以用之前介绍过的方法来回滚版本了，撤消当前commit12345678910111213141516171819202122232425$ git reset --hard 856a740HEAD is now at 856a740 [*]update time to 18:51$ git log -1commit 856a74084bbf9b678467b2615b6c1f6bd686ecff (HEAD -&gt; master)Author: pzqu &lt;pzqu@example.com&gt;Date: Sat Dec 22 19:39:19 2018 +0800 [*]update time to 18:51 $ git reset --hard 35b66edHEAD is now at 35b66ed [*]update time to 00:50$ git log -2commit 35b66ed8e3ae2c63cc4ebf323831e3b917d2b1d4 (HEAD -&gt; master, origin/master, origin/HEAD)Author: pzqu &lt;pzqu@example.com&gt;Date: Sun Dec 23 00:51:54 2018 +0800 [*]update time to 00:50commit 856a74084bbf9b678467b2615b6c1f6bd686ecffAuthor: pzqu &lt;pzqu@example.com&gt;Date: Sat Dec 22 19:39:19 2018 +0800 [*]update time to 18:51根据git reflog返回的结果，用git reset --hard commit_id回退到856a740这个版本git log -1看近一行的日志，可以看到目前就在这了再根据git reflog的结果，用git reset --hard 35b66ed跑到这次提交git log -2看到两次提交的日志，我们就这么再穿梭过来了，就是这么爽但是我们如果只是想把此提交给找回来，恢复他，那还是不要用reset的方式，可以用cherry-pick或者merge来做合并找回忘记提交的历史记录你之前没有commit过的文件，被删除掉了，或者被reset --hard的时候搞没了,这种情况可以说是相当的难搞了，所幸你以前做过add的操作把他放到过暂存区，那我们来试试找回来,先来创建一个灾难现场123456789101112131415161718192021222324$ echo 'my lose message' &gt; lose_file.txt$ git add lose_file.txt$ git statusOn branch masterYour branch is up to date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: lose_file.txt$ git reset --hard 35b66ed8HEAD is now at 35b66ed [*]update time to 00:50$ git statusOn branch masterYour branch is up to date with 'origin/master'.nothing to commit, working tree clean$ lsREADME.md need_stash.txt share_file.txt time.txt创建一个叫lose_file.txt的文件并写入内容my lose message，并把他加到暂存区用git reset --hard 35b66ed8用丢弃一切修改的方式来使现在的工作区恢复到35b66ed8版本，因为还没提交所以也就是恢复到当前的（head）版本。我们用status和ls再看，这个叫lose_file.txt的文件真的没了，完蛋了,第一反应用刚刚学到的命令git reflow会发现根本就不好使核心命令：git fsck --lost-found,他会通过一些神奇的方式把历史操作过的文件以某种算法算出来加到.git/lost-found文件夹里12345678910111213$ git fsck --lost-foundChecking object directories: 100% (256/256), done.Checking objects: 100% (3/3), done.dangling blob 7f5965523d2b9e850b39eb46e8e0f7c5755f6719dangling commit fdbb19cf4c5177003ea6610afd35cda117a41109dangling commit 8be46aa83f0fe90317b0c6b9c201ad994f8caeafdangling blob 11400c1d56142615deba941a7577d18f830f4d85dangling tree 3bd4c055afedc51df0326def49cf85af15994323dangling commit 3d113a773771c09b7c3bf34b9e974a697e04210adangling commit bfdc065df8adc44c8b69fa6826e75c5991e6cad0dangling tree c96ff73cb25b57ac49666a3e1e45e0abb8913296dangling blob d6d03143986adf15c806df227389947cf46bc6dedangling commit 7aa21bc382cdebe6371278d1af1041028b8a2b09这里涉及到git的一些低层的知识，我们可以看到这里有blob、commit、tree类型的数据，还有tag等类型的。他们是什么含义呢？blob组件并不会对文件信息进行存储，而是对文件的内容进行记录commit组件在每次提交之后都会生成，当我们进行commit之后，首先会创建一个commit组件，之后把所有的文件信息创建一个tree组件,所以哪个blob代表什么文件都可以在tree 里找到我们来看看怎么恢复刚刚不见了的lose_file.txt文件，在上面执行完git fsck --lost-found命令，返回的第一行blob我们看看他的内容1234567git show 7f5965523d2b9e850b39eb46e8e0f7c5755f6719my lose messagegit show 7f5965523d2b9e850b39eb46e8e0f7c5755f6719 &gt; lose_file.txt$ lsREADME.md lose_file.txt need_stash.txt share_file.txt time.txt看到没有，就是我们丢失的文件内容，这样就找回来了！我们再来看看commit tree的内容1234567891011$ git cat-file -p fdbb19cf4c5177003ea6610afd35cda117a41109tree 673f696143eb74ac5e82a46ca61438b2b2d3bbf4parent e278392ccbf4361f27dc338c854c8a03daab8c49parent 7b54a8ae74be7192586568c6e36dc5a813ff47cfauthor pzqu &lt;pzqu@example.com&gt; 1544951197 +0800committer pzqu &lt;pzqu@example.com&gt; 1544951197 +0800Merge branch 'master' of github.com:pzqu/git_test$ git ls-tree 3bd4c055afedc51df0326def49cf85af15994323100644 blob c44be63b27a3ef835a0386a62ed168c91e680e87 share_file.txt用git cat-file -p可以看到commit的内容，可以选择把这个commit合并到我们的分支里，还是reset merge rebase cherry-pick这些命令来合commitgit ls-tree列出tree下面的文件名和id的记录信息，然后就可以根据这些来恢复文件了后记：如果你发现执行git fsck --lost-found的输出找不到你想要的，那么在执行完git fsck --lost-found后会出现一堆文件 在 .git/lost-found 文件夹里,我们不管他。可以用以下命令来输出近期修改的文件12345678910111213141516171819$ find .git/objects -type f | xargs ls -lt | sed 3q-r--r--r-- 1 pzqu staff 32 12 23 12:19 .git/objects/7f/5965523d2b9e850b39eb46e8e0f7c5755f6719-r--r--r-- 1 pzqu staff 15 12 23 01:51 .git/objects/e6/9de29bb2d1d6434b8b29ae775ad8c2e48c5391-r--r--r-- 1 pzqu staff 162 12 23 00:51 .git/objects/35/b66ed8e3ae2c63cc4ebf323831e3b917d2b1d4$ git cat-file -t 7f5965523d2b9e850b39eb46e8e0f7c5755f6719blob$ git cat-file -p 7f5965523d2b9e850b39eb46e8e0f7c5755f6719my lose message$ git cat-file -t b2484b5ab58c5cb6ecd92dacc09b41b78e9b0001tree$ git cat-file -p b2484b5ab58c5cb6ecd92dacc09b41b78e9b0001100644 blob f9894f4195f4854cfc3e3c55960200adebbc3ac5 README.md100644 blob e69de29bb2d1d6434b8b29ae775ad8c2e48c5391 need_stash.txt100644 blob 83f50ec84c00f5935da8089bac192171cfda8621 share_file.txt100644 blob f0664bd6a49e268d3db47c508b08d865bc25f7bb time.txt这里用find .git/objects -type f | xargs ls -lt | sed 3q返回了近3个修改的文件,想要更多就改3q这个数值，比如你想输出100个就用100qgit cat-file -t 7f5965523d2b9e850b39eb46e8e0f7c5755f6719 就能看见文件类型 把最后一个/去掉 复制从objects/ 后面的所有东西放在-t后面git cat-file -p id就能看见文件内容，是不是很爽漏提交有时候会碰到我们已经commit但是有修改忘记了提交，想把他们放在刚刚的commit里面，这种时候怎么做呢？1234567891011121314151617181920212223242526$ git log --name-status --pretty=oneline -135b66ed8e3ae2c63cc4ebf323831e3b917d2b1d4 (HEAD -&gt; master, origin/master, origin/HEAD) [*]update time to 00:50M time.txt$ git statusOn branch masterYour branch is up to date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: lose_file.txt new file: test_amend.txt $ git commit --amend --no-edit[master 31cc277] [*]update time to 00:50 Date: Sun Dec 23 00:51:54 2018 +0800 3 files changed, 2 insertions(+), 1 deletion(-) create mode 100644 lose_file.txt create mode 100644 test_amend.txt $ git log --name-status --pretty=oneline -131cc2774f0668b5b7c049a404284b19e9b40dc5d (HEAD -&gt; master) [*]update time to 00:50A lose_file.txtA test_amend.txtM time.txt查看文件提交日志只有time.txtstage里还有新的修改在使用git commit --amend --no-edit合并到上一个提交里，如果不加--no-edit参数的话，会提示你来修改commit提示信息(这个命令也可以用在重复编辑commit message)。查看日志，合并提交成功！tag标签创建一个tag标签是一个类似于快照的东西，常常用于测试和发布版本。所以我们常常把tag名以版本号来命名，比如：v1.0beat1这样我们怎么创建标签呢？首先先切换到想打标签的分支，然后直接打就可以了。1234567891011121314151617181920$ git branch dev/pzqu master* release_v1.0$ git tag -a release_v1.0 -m "release v1.0"$ git tag release_v1.1$ git tagrelease_v1.0release_v1.1$ git push --tagsCounting objects: 2, done.Writing objects: 100% (2/2), 158 bytes | 158.00 KiB/s, done.Total 2 (delta 0), reused 0 (delta 0)To github.com:pzqu/git_test.git * [new tag] release_v1.0 -&gt; release_v1.0 * [new tag] release_v1.1 -&gt; release_v1.1切换到想打tag的分支创建名为release_v1.0带有信息release v1.0的tag创建的不带有tag的提交信息的release_v1.1git tag查看tag推送本地全部tag也可以推送单个tag1234$ git push origin release_v1.1Total 0 (delta 0), reused 0 (delta 0)To github.com:pzqu/git_test.git * [new tag] release_v1.1 -&gt; release_v1.1我们来删除tag123456789$ git tag -d release_v1.0Deleted tag 'release_v1.0' (was eb5d177)$ git push origin :refs/tags/release_v1.0To github.com:pzqu/git_test.git - [deleted] release_v1.0$ git tagrelease_v1.1本地删除名为release_v1.0的tag远程删除名为release_v1.0的tag对历史提交打tag先看看当前的log1234531cc277 (HEAD -&gt; release_v1.0, tag: release_v1.1, origin/release_v1.0, master) [*]update time to 00:50856a740 [*]update time to 18:513f22a06 [+]add file time.txt4558a25 (origin/dev/pzqu, dev/pzqu) [*]test stashd9e018e [*]merge master to dev/pzqu比方说要对[*]update time to 18:51这次提交打标签，它对应的commit id是856a740，敲入命令：12345$ git tag v.9 856a740$ git log --pretty=oneline --abbrev-commit31cc277 (HEAD -&gt; release_v1.0, tag: release_v1.1, origin/release_v1.0, master) [*]update time to 00:50856a740 (tag: v0.9) [*]update time to 18:51成功打上git忽略不想提交的文件我们有两种情况，一种是我们根本就不想这些文件出现在git库里比如日志文件；另一种是git远程仓库里有这些文件，就像通用的配置文件，我们必须要在本地修改配置来适应运行环境，这种情况下我们不想每次提交的时候都去跟踪这些文件。忽略自动生成的垃圾文件、中间文件、敏感信息文件忽略文件的原则是：忽略操作系统自动生成的文件，比如缩略图等；忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件；忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。我们要怎么做呢？在Git工作区的根目录下创建一个特殊的.gitignore文件，然后把要忽略的文件名填进去，Git就会自动忽略这些文件。12345678910111213$ echo "*.log" &gt; .gitignore$ touch test.log$ touch test2.log$ ls -a. .git README.md need_stash.txt test.log test_amend.txt.. .gitignore lose_file.txt share_file.txt test2.log time.txt$ git statusOn branch release_v1.0nothing to commit, working tree clean创建并写入忽略规则*.log忽略全部以.log为后缀的文件创建了test.log和test2.logstatus查看，真是工作区是clean，新创建的文件没有被跟踪忽略远程存在，本地不想与远程同步的文件添加跟踪忽略核心命令：1git update-index —assume-unchanged 文件名创建time.txt文件并写入10:41,提交到远程仓库使用命令git update-index —assume-unchanged加time.txt加到忽略名单里修改time.txt的内容为10:43status查看确实没有被跟踪看远程仓库取消跟踪忽略核心命令：1git update-index —no-assume-unchanged 文件名pull同步远程仓库，真的没有更新刚刚被添加跟踪忽略的文件git update-index —no-assume-unchanged取消跟踪忽略status查看，出现文件的跟踪查看跟踪记录如果忘记了哪些文件被自己本地跟踪使用命令git update-index —assume-unchanged加time.txt加到忽略名单里使用git ls-files -v| grep &#39;^h\ &#39;命令可以看到小写h代表本地不跟踪的文件小结学完本文章，你将学会撤消commit,回滚暂存区，回滚工作区、回退远程仓库两种方法找回不小心丢失的文件提交的时候漏了文件，修改commit的提交信息tag操作，创建、创建有描述信息的tag、删除tag、删除远程tag、推送本地单个tag和全部taggit忽略自动生成的垃圾文件、中间文件、敏感信息文件；忽略远程存在，本地不想与远程同步的文件并恢复跟踪和查看哪些文件被跟踪注意事项理论上，git日常用到的命令是 diff show fetch rebase pull push checkout commit status 等，这些命令都不会导致代码丢失，假如害怕代码丢失，可以预先commit一次，再进行修改，但切记不可使用自己不熟悉的命令任何命令，不要加上-f的强制参数，否则可能导致代码丢失建议多使用命令行，不要使用图形界面操作下集引用git官网廖雪峰的官方网站-git篇hexo博客部署到vps关于git reset –hard这个命令的惨痛教训Git 基础再学习之：git checkout – file如何理解git checkout – file和git reset HEAD – file]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>版本控制</tag>
        <tag>git教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[版本控制工具——Git常用操作（上）]]></title>
    <url>%2Fgit-start%2F</url>
    <content type="text"><![CDATA[摘要：用了很久的Git和svn,由于总是眼高手低，没能静下心来写这些程序员日常开发最常用的知识点。现在准备开一个专题，专门来总结一下版本控制工具，让我们从git开始。完成本系列博客的阅读以后，你将掌握git的基本概念与git的基本命令，可以在本地随心所欲的完成代码的提交撤销保存修改等操作、可以流畅的参与多人协作，本文致力于快速的入门，如果涉及到更高级的功能需要进行更深一步的学习。本文核心点：Git的基本概念一个人使用Git时的代码版本控制–（提交、拉代码、分支操作）多人合作时的代码版本控制–（合并冲突、暂存代码）什么是Git简介git是世界上目前最先进的分布式版本控制系统,致力于团队、个人进行项目版本管理，完美的解决难以比较代码、难以合并代码、难以取消修改、难以在写当前代码的过程中保存未完成的修改去修改线上版本的bug等的痛点。git是一个非常强大的工具，但作为一个git使用者来说，不用完全学习Git的知识点与命令，因为有的命令的使用频率非常的低甚至数年都不会用到，让我们来由浅入深进行学习。git的历史git是linux的创始人linus，在付费版本控制工具BitMover收回对Linux社区免费使用权利的时候，一怒之下花费两个星期的时间写出来的。（牛笔的人）开始安装git选择自己的操作系统对应的git版本安装，安装成功后运行git version后，输出git版本则安装正确。git 官方： https://git-scm.com/downloads配置用户信息使用git config命令来配置用户名和邮箱12git config --global user.name "pzqu" git config --global user.email pzqu@example.com如果用了 –global 选项，那么更改的配置文件就是位于你用户主目录下的那个，以后你所有的项目都会默认使用这里配置的用户信息。如果要在某个特定的项目中使用其他名字或者电邮，只要去掉 –global选项重新配置即可，新的设定保存在当前项目的 .git/config 文件里。使用git config user.name和git config user.email来检查是否成功，也可以直接用git config --list来列出全部git配置信息来查看创建git托管的项目假如我们创建一个项目叫make_money，先创建一个文件夹叫make_money，再使用git init命令创建git项目。1234567891011121314151617181920# pzqu @ pzqu-pc in ~/Documents/code/test [0:05:29]$ mkdir make_money# pzqu @ pzqu-pc in ~/Documents/code/test [0:06:24]$ lsmake_money# pzqu @ pzqu-pc in ~/Documents/code/test [0:06:29]$ cd make_money# pzqu @ pzqu-pc in ~/Documents/code/test/make_money [0:07:10]$ git initInitialized empty Git repository in /Users/pzqu/Documents/code/test/make_money/.git/# pzqu @ pzqu-pc in ~/Documents/code/test/make_money on git:master o [0:07:12]$ ls -altotal 0drwxr-xr-x 3 pzqu staff 96 11 7 00:07 .drwxr-xr-x 3 pzqu staff 96 11 7 00:06 ..drwxr-xr-x 9 pzqu staff 288 11 7 00:07 .git创建成功以后，会出现一个叫.git的隐藏文件夹，这个就是你的git仓库，以后所有的git操作历史提交记录信息就全部记录在此了，只要这个文件夹在就可以记住我们的全部git操作工作区和暂存区在使用git的时候还要清楚暂存区和工作区的含义，参考廖雪峰的官方网站-git篇-工作区和暂存区常见情况提交代码新文件与修改1234567891011121314151617181920212223242526# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master o [11:37:50]$ lsREADME.md# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master o [11:42:02]$ touch file1.txt# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master x [11:42:15]$ git add file1.txt# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master x [11:42:23]$ git statusOn branch masterYour branch is up to date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: file1.txt# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master x [11:56:38]$ git commit -m "[+]add new file1.txt"[master 66cc488] [+]add new file1.txt 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 file1.txt上图操作包含:创建新文件file1.txtadd 添加修改的内容到索引status 查看修改的内容commit 把索引提交到本地分支git add . ：监控工作区的状态树，此命令会把工作时的所有变化提交到暂存区，包括文件内容修改(modified)以及新文件(new)，但不包括被删除的文件。git add -u：他仅监控已经被add的文件（即tracked file），他会将被修改的文件提交到暂存区。add -u 不会提交新文件（untracked file）。（git add –update的缩写）git add -A ：是上面两个功能的合集（git add –all的缩写）git show 列出最近一次的提交 对于commit：像这样，你不断对文件进行修改，然后不断提交修改到版本库里，就好比玩RPG游戏时，每通过一关就会自动把游戏状态存盘，如果某一关没过去，你还可以选择读取前一关的状态。有些时候，在打Boss之前，你会手动存盘，以便万一打Boss失败了，可以从最近的地方重新开始。Git也是一样，每当你觉得文件修改到一定程度的时候，就可以“保存一个快照”，这个快照在Git中被称为commit。一旦你把文件改乱了，或者误删了文件，还可以从最近的一个commit恢复，然后继续工作，而不是把几个月的工作成果全部丢失。删除文件12345678910111213141516171819202122232425262728# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master o [12:55:24]$ lsREADME.md file1.txt# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master o [12:55:25]$ git rm file1.txtrm 'file1.txt'# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master x [12:55:30]$ lsREADME.md# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master x [12:55:32]$ git statusOn branch masterYour branch is ahead of 'origin/master' by 1 commit. (use "git push" to publish your local commits)Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) deleted: file1.txt# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master x [12:55:40] C:128$ git commit -m "[-]delete file1.txt"[master e278392] [-]delete file1.txt 1 file changed, 0 insertions(+), 0 deletions(-) delete mode 100644 file1.txt上图操作包含:创建新文件file1.txtgit rm 删除file1.txt文件status 查看修改的内容commit 把索引提交到本地分支tip1: 如果没有用git rm删除文件，在本地删除文件后，git add一下再提交可以达到同样的效果tip2: 要是你加班太晚，头晕不小心删除了不想删除的文件怎么办？见版本控制工具——Git常用操作（下）-后悔药拉代码方法一 pull12345678910111213# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master o [17:01:13]$ git pullremote: Enumerating objects: 4, done.remote: Counting objects: 100% (4/4), done.remote: Compressing objects: 100% (2/2), done.remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0Unpacking objects: 100% (3/3), done.From github.com:pzqu/git_test 5fd4d8f..7b54a8a master -&gt; origin/masterMerge made by the 'recursive' strategy. share_file.txt | 1 + 1 file changed, 1 insertion(+) create mode 100644 share_file.txt上图命令：git pull查看本地仓库变化git log上图可以看到向远程仓库pull的时候，出现了两个新的commit，commit 7b54a8ae74...的提交信息为Create share_file.txt,另一个commit fdbb19cf4c51770的提交信息为Merge branch &#39;master&#39; of github.com:pzqu/git_test。事实上主线只有一个提交，为什么会出现这种情况? 是因为pull其实会做两个操作拉远程仓库代码到本地自动与当前分支合并并生成一个合并成功的提交注意这里的第二个个步骤如果远程有人和你改了同一个文件就会出现一个冲突，这个时候git会提示你哪些文件有冲突，手动改了再提交一次就可以了。详情见合并冲突方法二 fetch我在远程修改了文件，向share_file.txt加了一行内容tom modify，此时拉代码。12345678# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master o [21:07:21]$ git fetch# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:master o [21:08:43]$ git rebase origin/masterFirst, rewinding head to replay your work on top of it...Applying: [+]add new file1.txtApplying: [-]delete file1.txt上图所示有以下两个操作fetch 拉取远端代码到本地rebase 把本地代码提交基于远端分支重新replay效果如下：上图是git log所输出的提交内容，刚刚pull的时候忘记把pull自动产生的merge提交到远程，rebase的时候把本地的提交放到了远程提交之后，看起来就是一条直线，比较优雅，也是推荐的方式。同样的，如果产生了冲突，详情见合并冲突分支操作创建分支分支是多人协同最经典的地方所在，我们来创建一个分支1234567$ git checkout -b dev/pzqu origin/masterBranch 'dev/pzqu' set up to track remote branch 'master' from 'origin'.Switched to a new branch 'dev/pzqu'$ git branch* dev/pzqu mastergit checkout -b 分支名 其他分支,-b代表创建并切换到新建的分支，分支名代表新创建的分支叫什么名字，这里叫dev/pzqu ，其他分支代表基于哪一个分支来创建，这里基于远程的master分支origin/master，如果省略则代表基于当前分支git branch展示本地的分支情况，加-a参数可以展示全部的分支，包括远程分支*在分支前，指明了现在所在的分支是dev/pzqu切换分支12345678910111213141516$ git checkout -b dev/pzqu2Switched to a new branch 'dev/pzqu2'$ git branch dev/pzqu* dev/pzqu2 master$ git checkout dev/pzquSwitched to branch 'dev/pzqu'Your branch is up to date with 'origin/master'.$ git branch* dev/pzqu dev/pzqu2 master基于当前分支创建了一个新的分支并自动切换过去dev/pzqu2git checkout 已存在的分支名切换分支回到dev/pzqu删除分支1234567891011$ git branch* dev/pzqu dev/pzqu2 master $ git branch -D dev/pzqu2Deleted branch dev/pzqu2 (was 7c9be37).$ git branch* dev/pzqu master位于dev/pzqu，删除了dev/pzqu2分支合并冲突合并同一个分支的冲突（常见）为了产生一个冲突，我在另一个地方向远程仓库提交了代码，更改share_file.txt文件，加了一行内容tom add for merge，本地修改同一个文件加了一行pzqu add for merge，并提交到本地，这样一来，本地和远程仓库的同一个文件就不一样了，一会拉代码一定会产生一个冲突。效果如下：一般rebase或pull冲突的时候，都会出现提示，然后git status会出现上图图示这个时候不可以进行任何分支切换和commit操作，按照他提示进行处理git status提示哪个文件是都被修改的，both modified，然后使用编辑器修改该文件，解决冲突解决完成后，git add 添加该冲突文件git rebase –continue，并更新commit message，完成整个rebase流程我们来看看这个冲突的文件：Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，我们修改如下后保存：git add再git rebase --continue后完成rebase，效果如下，再push的远程仓库即可合并不同分支的代码产生冲突关于怎么创建分支与切换分支见创建分支和切换分支,这里只讨论合并时产生的冲突的情况，我们已经基于master分支创建了一个dev/pzqu分支123$ git branch* dev/pzqu master切换到master分支，加一行master add for merge并提交，文件内容如下：123456$ cat share_file.txttom addtom modifytom add for mergepzqu add for mergemaster add for merge切换到dev/pzqu分支，向share_file.txt加入一行dev/pzqu add for merge并提交，现在share_file.txt内容如下：123456$ cat share_file.txttom addtom modifytom add for mergepzqu add for mergedev/pzqu add for merge现在两个分支的同一个文件内容不一样了，现在我们在dev/pzqu分支上进行合并：1234567891011121314151617181920212223242526272829303132$ git merge masterAuto-merging share_file.txtCONFLICT (content): Merge conflict in share_file.txtAutomatic merge failed; fix conflicts and then commit the result.# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:dev/pzqu x [11:17:31] C:1$ git statusOn branch dev/pzquYour branch is ahead of 'origin/master' by 1 commit. (use "git push" to publish your local commits)You have unmerged paths. (fix conflicts and run "git commit") (use "git merge --abort" to abort the merge)Unmerged paths: (use "git add &lt;file&gt;..." to mark resolution) both modified: share_file.txtno changes added to commit (use "git add" and/or "git commit -a")$ cat share_file.txttom addtom modifytom add for mergepzqu add for merge&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADdev/pzqu add for merge=======master add for merge&gt;&gt;&gt;&gt;&gt;&gt;&gt; master上图出现了一个冲突，是我们意料之中的，修改share_file.txt文件，解决此冲突：123456789101112131415161718192021$ cat share_file.txttom addtom modifytom add for mergepzqu add for mergedev/pzqu add for mergemaster add for merge$ git add share_file.txt# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:dev/pzqu x [11:22:40]$ git commit -m "[*]merge master to dev/pzqu"[dev/pzqu d9e018e] [*]merge master to dev/pzqu# pzqu @ pzqu-pc in ~/Documents/code/test/git_test on git:dev/pzqu o [11:23:00]$ git statusOn branch dev/pzquYour branch is ahead of 'origin/master' by 3 commits. (use "git push" to publish your local commits)nothing to commit, working tree clean冲突解决也提交了，看看我们现在的分支内容：上图我们可以看到：master分支比远程origin/master分支多一次提交，dev/pzqu分支由于是基于origin/master分支，合并了master分支的提交和当前dev/pzqu分支的提交，超出本地master两个提交，致此我们把master合并到dev/pzqu的操作就完成了。通常我们开一个新的开发分支是为了在自己的分支上写代码，方便提交也不会把主线弄乱，现在我们用同样的方法将dev/pzqu合并到master分支，然后把两个分支都提交到远程。1234567891011121314151617181920212223242526272829$ git checkout masterSwitched to branch 'master'Your branch is ahead of 'origin/master' by 1 commit. (use "git push" to publish your local commits)$ git merge dev/pzquUpdating 58f047a..d9e018eFast-forward share_file.txt | 1 + 1 file changed, 1 insertion(+)$ git push origin masterTotal 0 (delta 0), reused 0 (delta 0)To github.com:pzqu/git_test.git 7c9be37..d9e018e master -&gt; master $ git push origin dev/pzquCounting objects: 9, done.Delta compression using up to 8 threads.Compressing objects: 100% (9/9), done.Writing objects: 100% (9/9), 887 bytes | 887.00 KiB/s, done.Total 9 (delta 2), reused 0 (delta 0)remote: Resolving deltas: 100% (2/2), done.remote:remote: Create a pull request for 'dev/pzqu' on GitHub by visiting:remote: https://github.com/pzqu/git_test/pull/new/dev/pzquremote:To github.com:pzqu/git_test.git * [new branch] dev/pzqu -&gt; dev/pzqu切换到master分支合并dev/pzqu到master分支master推到远程仓库如果dev/pzqu要保留，就可以推送到远程仓库。现在我们可以看到全部的分支都在一起了，强迫症都舒服了。暂存代码保存现场这种情况一般是出现在你正在完成一个功能，但是忽然线上发现了一个Bug，必须马上开一个新的分支来修复bug，但是现在的功能没写完不打算提交(commit)，现在怎么办？？不用怕暂存代码来帮助你。123456789101112131415161718192021222324252627282930313233343536373839404142$ git statusOn branch dev/pzquYour branch is up to date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: need_stash.txt modified: share_file.txt$ git stashSaved working directory and index state WIP on dev/pzqu: d9e018e [*]merge master to dev/pzqu$ git stash liststash@&#123;0&#125;: WIP on dev/pzqu: d9e018e [*]merge master to dev/pzqu$ git statusOn branch dev/pzquYour branch is up to date with 'origin/master'.nothing to commit, working tree clean//省略操作：去创建一个Bug分支，修复他并完成与主线的合并，删除Bug分支。//省略操作：切回来当前分支继续开发//下面来恢复现场$ git stash apply stash@&#123;0&#125;On branch dev/pzquYour branch is up to date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: need_stash.txtChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: share_file.txtstatus查看到有2个文件修改没有提交stash把修改放到暂存区，并生成一个idstash list列出暂存区所有内容stash apply重新把暂存区内容放到本地这里的stash apply成功的把暂存区的一次暂存恢复到了本地，但是暂存区还有会保存这次暂存，如果想删除这次暂存要用git stash drop来删除；也可以用git stash pop，恢复最后一次暂存的同时把stash内容也删了。1234$ git stash drop stash@&#123;0&#125;Dropped stash@&#123;0&#125; (bfdc065df8adc44c8b69fa6826e75c5991e6cad0)$ git stash list好了，暂存区清干净了。注意：要放到暂存区的文件一定要先通过git add加到index 小结本文阅读结束以后，我们学会了Git的基本概念，知道git的作用、历史；学会安装配置Git，使用Git创建项目托管以及工作区和暂存区的概念学会Git的本地操作，提交、拉代码、创建切换删除分支操作，多人合作时的代码版本控制，学会了不同情况下的合并冲突、暂存代码操作下集预告Git常用操作（下）我计划给大家介绍以下点：后悔药-各种后悔操作（撤消commit,回滚，回退远程仓库等）哎呀，提交的时候漏了文件tag操作git忽略不想提交的文件下集传送门：版本控制工具——Git常用操作（下）注意事项理论上，git日常用到的命令是 diff show fetch rebase pull push checkout commit status 等，这些命令都不会导致代码丢失，假如害怕代码丢失，可以预先commit一次，再进行修改，但切记不可使用自己不熟悉的命令任何命令，不要加上-f的强制参数，否则可能导致代码丢失建议多使用命令行，不要使用图形界面操作引用git官网廖雪峰的官方网站-git篇hexo博客部署到vps]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>版本控制</tag>
        <tag>git教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql备份还原方案xtrabackup]]></title>
    <url>%2Fmysql-backup-xtrabackup%2F</url>
    <content type="text"><![CDATA[摘要：mysql当数据库过大的时候，使用mysqldump的方式进行备份是一种非常慢的操作，500G的数据就够你备份一天一夜，我发现了一种mysql快速备份的方案，它使用文件存储的方式进行备份，支持全量和增量备份，这里所写为全量方式（如果可以接受备份开始到下次恢复之间的数据丢失时使用）。xtrabackup的备份速度很快，不管有多少的数据，备份速度完全是依赖于磁盘的读写速度，还支持压缩、不打断正在执行的事务、自动实现备份检验（用mysqldump会锁表，要加上可重复读–single-transaction才不会影响线上的程序写表，但是写表后的东西在还原的时候就会丢了，这也是全量备份的痛点）特点准备mysql备份组件需要的安装包安装备份工具1. 上传并解压2. 安装rpm包3. 检查是否安装成功开始备份1. 执行命令开始备份2. 检查是否备份成功还原备份1. 事务日志应用到备份2. 恢复数据3. 设置属主属组为mysql并启动引用特点(1)备份过程快速、可靠(2)备份过程不会打断正在执行的事务(3)能够基于压缩等功能节约磁盘空间和流量(4)自动实现备份检验(5)还原速度快准备mysql备份组件需要的安装包检查服务器是centos6版本还是centos7+版本。选择安装包12centos6/percona-xtrabackup/Percona-XtraBackup-2.4.12-r170eb8c-el6-x86_64-bundle.tarcentos7/percona-xtrabackup/Percona-XtraBackup-2.4.12-r170eb8c-el7-x86_64-bundle.tar安装包可以在此下载 ： https://www.percona.com/downloads/XtraBackup/LATEST/安装备份工具以下所有操作如果是在集群下，要在一个主节点上操作，操作一次即可，启动时设置主节点为被同步节点，集群的管理我们以后再讨论。1. 上传并解压假设当前系统是centos6+,使用Percona-XtraBackup-2.4.12-r170eb8c-el6-x86_64-bundle.tar包，拷贝到系统/tmp/backup_mariadb20181127目录下(没有则创建,日期写当天)，使用tar xvf Percona-XtraBackup-2.4.12-r170eb8c-el6-x86_64-bundle.tar命令解压,你可以得到以下文件。1234567# pwd/tmp/backup_mariadb20181127# lsPercona-XtraBackup-2.4.12-r170eb8c-el6-x86_64-bundle.tarpercona-xtrabackup-24-2.4.12-1.el6.x86_64.rpmpercona-xtrabackup-24-debuginfo-2.4.12-1.el6.x86_64.rpm percona-xtrabackup-test-24-2.4.12-1.el6.x86_64.rpm2. 安装rpm包执行以下命令123rpm -ivh --force --nodeps percona-xtrabackup-24-debuginfo-2.4.12-1.el6.x86_64.rpm rpm -ivh --force --nodeps percona-xtrabackup-24-2.4.12-1.el6.x86_64.rpmrpm -ivh --force --nodeps percona-xtrabackup-test-24-2.4.12-1.el6.x86_64.rpm3. 检查是否安装成功按以下显示则安装成功1234# rpm -qa | grep perconapercona-xtrabackup-test-24-2.4.12-1.el6.x86_64percona-xtrabackup-24-2.4.12-1.el6.x86_64percona-xtrabackup-24-debuginfo-2.4.12-1.el6.x86_64开始备份1. 执行命令开始备份执行以下命令开始备份，其中/etc/my.cnf为mysql配置文件位置，10.123.2.4为mysql绑定的ip（写当前机器的ip）,user1为用户名，123456Abc为密码，/tmp/backup_mariadb20181127为备份文件所在目录，所有按实际环境填写。此处我们只备份cloud库所以--databases库就不用改动了1innobackupex --defaults-file=/etc/my.cnf --host=10.123.2.4 --databases="cloud" --use-memory=500M --user=user1 --password=123456Abc /tmp/backup_mariadb20181127如果只需要备份其中一个或多个数据库，可以加参数--databases=&quot;cloud test&quot;,其中cloud和test是库名可以使用–use-memory= (例如： 1MB, 1M, 1GB, 1G)选项加速，在不指定内存大小的情况下，默认会占用100MB的内存。2. 检查是否备份成功最后一行显示completed OK！ 则备份成功，在所执行的目录下（此处是/tmp/backup_mariadb20181127）会出现备份的文件1181127 11:56:48 completed OK!可以看到文件结构，我们此处自动生成的备份文件夹名为2018-11-27_11-52-48，是一个以时间命名的文件夹12345# ls2018-11-27_11-52-48 Percona-XtraBackup-2.4.12-r170eb8c-el6-x86_64-bundle.tar percona-xtrabackup-24-2.4.12-1.el6.x86_64.rpm percona-xtrabackup-24-debuginfo-2.4.12-1.el6.x86_64.rpm percona-xtrabackup-test-24-2.4.12-1.el6.x86_64.rpm# pwd/tmp/backup_mariadb20181127还原备份1. 事务日志应用到备份备份出的数据并不能直接使用，因为备份出的数据是不一致的，我们还需要将同时备份出的事务日志应用到备份中，才能得到一份完整、一致、可用的数据，xtrabackup称这一步操作为prepare，也就是还原数据前的”准备”工作。1innobackupex --apply-log 2018-11-27_11-52-48/在事务日志容量很大的情况下，可以使用–use-memory= (例如： 1MB, 1M, 1GB, 1G)选项加速，在不指定内存大小的情况下，默认会占用100MB的内存。输出最后如下就为正确1181127 11:56:10 completed OK!2. 恢复数据方法一、此处使用该方法，适用于备份部分数据库的方法数据目录在/data/mariadb/data，我们备份的数据库为cloud库。进入mysql命令行mysql -A，删除cloud库drop database cloud;(如果无法进入命令行则到数据目录下直接干掉cloud文件夹，集群操作的话必须通过drop或者先停止集群，确定好主从模式)执行命令1234567cd /data/mariadb/datarm ib* -frm -f cloud/etc/init.d/mysqld stop #关闭数据库cd /tmp/backup_mariadb20181127/2018-11-27_11-52-48 #进入备份目录cp ib* /data/mariadb/datacp -R cloud /data/mariadb/data方法二、先停止数据库服务/etc/init.d/mysqld stop，且对应的数据目录(此处是/data/mariadb/data)为空,如果不为空，手动删除，一般此方法针对全量备份的方法。1innobackupex --datadir=/data/mariadb/data --copy-back /tmp/backup_mariadb20181127/2018-11-27_11-52-48–copy-back：对应的目录就是我们准备好的可用数据的目录。此处为/tmp/backup_mariadb20181127/2018-11-27_11-52-48–datadir：指定的目录就是还原后数据要存放的目录，如果my.cnf设置了datadir，可以省略–datadir，执行copyback时会读取my.cnf中的配置，datadir目录必须为空目录,如果不为空，手动删除。3. 设置属主属组为mysql并启动此时我们还不能启动mysql，因为我们是使用root用户拷贝的数据，所以数据目录中的数据文件的属主属组仍然为root，我们需要将这些文件的属主属组设置为mysql。1234cd /data/mariadb/datachown -R mysql.mysql *chown -R mysql.mysql /data/mariadb/binlog/etc/inid.d/mysqld start引用Xtrabackup 安装使用xtrabackup 原理]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>备份mysql数据库</tag>
        <tag>mysql教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq管理利器——rabbitmqadmin]]></title>
    <url>%2Frabbitmqadmin%2F</url>
    <content type="text"><![CDATA[摘要：在运维rabbitmq集群的过程中，发生了队列的严重堆积，我们在可以容忍mq消息丢失的情况下，使用常用的purge_queue queue命令等好长时间都清理不成功，在管理页面上直接purge导致页面卡住不动，最终都没有办法达到清理成功的效果。这个时候发现了一个python的rabbitmq管理工具，非常的好用且迅速，在此记录。如何获取rabbitmqadmin常用命令查看系统操作用户管理操作队列操作其他指定输出格式用户角色rabbitmqctl 命令参考如何获取rabbitmqadmin方法1. 直接复制出来1234567891011121314151617181920212223242526cp -a /var/lib/rabbitmq/mnesia/rabbit@localhost-plugins-expand/rabbitmq_management-3.3.5/priv/www/cli/rabbitmqadmin /usr/local/bin/rabbitmqadmin``` **方法2** 从管理页面获取1. 打开`rabbitmq_management`，访问15672管理页面，方法见&lt;a href="https://qupzhi.com/first-blog" target="_blank"&gt;rabbitmq集群的各种运维操作 4.2 打开15672网页管理端，访问mq &lt;/a&gt;2. 访问 ip:15672/rabbitmqadmin下载页面，另存为`rabbitmqadmin.py`,放到此目录：`/usr/local/bin/rabbitmqadmin`，授权`chmod +x /usr/local/bin/rabbitmqadmin`多一句废话：可以使用wget直接下载页面上的东西# 常用命令## 查看```bashrabbitmqadmin list users #查看用户列表rabbitmqadmin list vhosts #查看vhostsrabbitmqadmin list connections ###查看 connectionsrabbitmqadmin list exchanges ##查看 exchangesrabbitmqadmin list bindings ##查看 bindingsrabbitmqadmin list permissions ##查看 permissionsrabbitmqadmin list channels ##查看 channelsrabbitmqadmin list parameters ##查看 parametersrabbitmqadmin list consumers ##查看consumersrabbitmqadmin list queues ##查看queuesrabbitmqadmin list policies ##查看policiesrabbitmqadmin list nodes ##查看nodesrabbitmqadmin show overview ##查看overview系统操作用户管理操作新增一个用户12rabbitmqctl add_user Username Passwordrabbitmqadmin declare user name=wyl password=password tags=administrator删除一个用户1rabbitmqctl delete_user Username修改用户的密码1rabbitmqctl change_password Username Newpassword查看当前用户列表123rabbitmqctl list_usersrabbitmqadmin list users # 查看 usersrabbitmqadmin list users name # 查看 users的时候限制字段设置用户角色1rabbitmqctl set_user_tags User TagUser为用户名， Tag为角色名(对应于administrator，monitoring，policymaker，management，或其他自定义名称见用户角色)。也可以给同一用户设置多个角色，例如1rabbitmqctl set_user_tags hncscwc monitoring policymaker队列操作添加queue12rabbitmqadmin declare queue name=test durable=true ## durable=true 代表持久化打开 declare是宣布的意思rabbitmqadmin --vhost=test --username=admin --password=admin declare queue name=test durable=true #指定vhost添加队列查看queues123[root@rabbitmq1 sbin]# rabbitmqadmin list queues#查看bindings[root@rabbitmq1 sbin]# rabbitmqadmin list bindings添加消息到test queue1rabbitmqadmin publish routing_key=test payload="this is a testing" ##未指定exchange默认 exchange name为空再次查看对列发现test有一条消息1[root@rabbitmq1 sbin]# rabbitmqadmin list queues从test queue消费一条信息1rabbitmqadmin get queue=test requeue=true #requeue=true 这条消息消费后还在，反之如果为false消费后消息就不在了。删除队列1rabbitmqadmin delete queue name=test清除队列消息内容1rabbitmqadmin purge queue name=队列名其他指定输出格式使用 -f 可以指定格式有如下几种格式 raw_json, long, pretty_json, kvp, tsv, table, bash 默认为 table,具体自己试用户角色超级管理员(administrator)可登陆管理控制台(启用management plugin的情况下)，可查看所有的信息，并且可以对用户，策略(policy)进行操作。监控者(monitoring)可登陆管理控制台(启用management plugin的情况下)，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等)策略制定者(policymaker)可登陆管理控制台(启用management plugin的情况下), 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。与administrator的对比，administrator能看到这些内容普通管理者(management)仅可登陆管理控制台(启用management plugin的情况下)，无法看到节点信息，也无法对策略进行管理。其他无法登陆管理控制台，通常就是普通的生产者和消费者。了解了这些后，就可以根据需要给不同的用户设置不同的角色，以便按需管理。rabbitmqctl 命令1234567891011121314151617181920212223242526272829303132333435rabbitmqctl list_queues：查看所有队列信息rabbitmqctl stop_app：关闭应用（关闭当前启动的节点）rabbitmqctl start_app：启动应用，和上述关闭命令配合使用，达到清空队列的目的rabbitmqctl reset：从管理数据库中移除所有数据，例如配置过的用户和虚拟宿主, 删除所有持久化的消息（这个命令要在rabbitmqctl stop_app之后使用）rabbitmqctl force_reset：作用和rabbitmqctl reset一样，区别是无条件重置节点，不管当前管理数据库状态以及集群的配置。如果数据库或者集群配置发生错误才使用这个最后的手段rabbitmqctl status：节点状态rabbitmqctl add_user username password：添加用户rabbitmqctl list_users：列出所有用户rabbitmqctl list_user_permissions username：列出用户权限rabbitmqctl change_password username newpassword：修改密码rabbitmqctl add_vhost vhostpath：创建虚拟主机rabbitmqctl list_vhosts：列出所有虚拟主机rabbitmqctl set_permissions -p vhostpath username ".*" ".*" ".*"：设置用户权限rabbitmqctl list_permissions -p vhostpath：列出虚拟主机上的所有权限 rabbitmqctl clear_permissions -p vhostpath username：清除用户权限rabbitmqctl -p vhostpath purge_queue blue：清除队列里的消息rabbitmqctl delete_user username：删除用户rabbitmqctl delete_vhost vhostpath：删除虚拟主机未完待续-催更 pzqu@qq.com参考通过rabbitmqadmin管理rabbitmq,【吴业亮】云计算开发工程师RabbitMQ学习笔记四：RabbitMQ命令（附疑难问题解决）]]></content>
      <categories>
        <category>rabbitmq</category>
      </categories>
      <tags>
        <tag>rabbitmq集群</tag>
        <tag>工具</tag>
        <tag>rabbitmq工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq之Federation配置]]></title>
    <url>%2Frabbitmq-federation%2F</url>
    <content type="text"><![CDATA[摘要：当我们有多个rabbitmq集群的时候，如果想要单向的同步集群的消息，也就是说把新集群当作老集群的镜像集群，实时的同步老集群的消息，在老集群消息被消费的时候不会影响同步到新集群的消息。在外部看上去就像每次写入消息的时候，同时向新老两个集群写入一样,不论mq的跨版本，不论mq的用户。一般我们会将这种情况应用于存在两个不同的系统，但是老数据来源只能向一个队列写入数据，此时为了在新系统上也可以实时同步到老系统队列中的数据的时候。Federation介绍特点松耦合性（Loose coupling）WAN 友好性（WAN-friendly）扩展性（Scalability）federation能做什么？配置的种类身份验证操作步骤说明1. 在集群的每一个node开启federation插件(同步和被同步集群都需要)2. 登录到同步集群的管理界面::http://x.x.x.:15672/#/3. 创建upstream4. 创建policy5. 查看状态图6. 查看连接高级参考Federation介绍federation 插件的最终目标是，在不同 broker 之间进行消息传递而无需建立集群；该功能在很多场景下非常有用：注意:当你在一个cluster中使用federation插件，所有在集群中 的nodes都需要安装federation插件特点松耦合性（Loose coupling）federation 插件能够在分属不同管理域的 broker 或 cluster 之间传递消息：他们可能设置了不同的 user 和 vhost ；他们可能运行在不同版本的 RabbitMQ 和 Erlang 上；WAN 友好性（WAN-friendly）federation 插件基于 AMQP 0-9-1 协议在不同 broker 之间进行通信，并设计成能够容忍不稳定的网络连通情况；扩展性（Scalability）federation 不需要在 n 个 broker 之间建立 O(n^2) 个连接（尽管这是最简单的使用模式），这也就意味着 federation 在使用时更容易扩展federation能做什么？federation 插件允许你将多个 exchange 或多个 queue 进行 federate ；federated exchange 或 federated queue 能够从一个或多个 upstream 接收到消息；也就是说，你的队列可以和其他集群的队列建立一种关系，他们之间可以相互的同步数据，可以是我同步给你，也可以是你同步给我，不过这种关系有两个角色一个是上游一个是下游，数据流向是上游流向下流。这里有三个名词，federation 插件允许你将多个 exchange 或多个 queue 进行 federate：upstream： 上游，是指位于其他 broker 上的、远端 exchange 和 queue ；federated exchange： 到exchange的关系，能够将发给 upstream 的消息路由到本地的某个 queue 中；federated queue： 到queue的关系，则允许一个本地消费者接收到来自 upstream queue 的消息；配置的种类关于 federation upstream 的信息全都保存在 RabbitMQ 的数据库中，其中包括了 user 信息、permission 信息、queue 信息等等；在 federation 中存在 3 种界别的配置：Upstreams - 每一个 upstream 用于定义如何与另外的 broker 建立连接；Upstream sets - 每一个 upstream set 用于针对一系列使用 federation 功能 upstream 进行了分组；Policies - 每一种 policy 会限定（过滤）出一组 exchange ，或者一组 queue ，或者同时针对两者进行限定；policy 最终将作用于一个单独的 upstream 上，或者一个 upstream set 上，并对其他对象发挥作用；实际上，在最简单的使用情况下，你可以忽略已经存在的upstream设置，因为有一个隐含的默认upstream叫做“all”，他会添加所有的upstream。身份验证我们讨论的是免身份验证的方式，如果有身份难的需求请参考官网：http://www.rabbitmq.com/authentication.html操作步骤说明parameter 和 policy 可以通过 3 种方式进行设置：通过 rabbitmqctl 脚本；通过 management 插件提供的 HTTP API ；通过 rabbitmq_federation_management 插件提供的 Web UI（更通用的方式,我们也是通过页面来配置就可以了）；注意：基于 Web UI 的方式不能提供全部功能，尤其无法针对 upstream set 进行管理；1. 在集群的每一个node开启federation插件(同步和被同步集群都需要)参考命令：12rabbitmq-plugins enable rabbitmq_federationrabbitmq-plugins enable rabbitmq_federation_management2. 登录到同步集群的管理界面::http://x.x.x.:15672/#/3. 创建upstreamtips:在下游，也就是新队列（被同步队列）上操作12UI操作:Admin-&gt;Federation Upstreams-&gt;Add a new upstream Name:随意填写 URI:填被同步集群(例如:amqp://user1:xxx@x.x.x.x,xxx为连接密码) Expires:默认填写3600000 单位ms其余字段可不用填写Expires：是代表缓存时间，如果说网络连通性不好的时候，消息会在上游的队列中缓存的时间，超时丢弃，设置为空则表示，永远缓存不会丢弃数据（但是如果长时候不恢复内存会占用越来越大，建议设置上）Acknowledgement Mode: 代表消息确认方式，用来防止消息在传输过程中丢失，有三个值，on-confirm、on-publish、no-ack，对传输速度的影响是从慢速到快速，对安全性是不会丢失到可能会丢失。通常使用on-publish，不然on-confirm太慢了。4. 创建policytips:在下游，也就是新队列（被同步队列）上操作1234UI操作:Admin-&gt;Policies-&gt;Add / update a policyName:随意填写(sync_data)Pattern:匹配表达式(例如:^(?!amq.).* 剔除系统队列后的所有队列)Apply to: 默认选择Exchange and queues Definition:federation-upstream-set = all (选定federation规则)5. 查看状态图现在，所有内置的 exchange 都应该建立了 federation ，因为他们都能匹配上面的 policy，可以通过页面查看状态12UI操作:Admin &gt; Federation Status &gt; Running Links 查看针对每个 exchange 的 federation 连接。配置成功可以看到匹配的Exchange / Queue， state:running也可以通过下面的命令查看状态图：1rabbitmqctl eval 'rabbit_federation_status:status().'也可以通过 management 插件中的 exchange 列表，或者下面的命令输出，确认上述 policy 已经作用到了 exchange 上；1rabbitmqctl list_exchanges name policy | grep federate-me通常情况下，针对每个 upstream 都会有一条 federation 连接，该 federation 连接对应到一个 exchange 上；例如 3 个 exchange 与 2 个 upstream 分别建立 federation 的情况下，会有 6 条连接。6. 查看连接登录到被同步集群（上游）的管理界面::http://x.x.x.:15672/#/ 前往 Connections选项 配置成功可以看到来自同步集群的连接 高级更复杂的配置：https://www.rabbitmq.com/federation-reference.html参考官网]]></content>
      <categories>
        <category>rabbitmq</category>
      </categories>
      <tags>
        <tag>rabbitmq集群</tag>
        <tag>rabbitmq教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ansible_roles]]></title>
    <url>%2Fansible-roles%2F</url>
    <content type="text"><![CDATA[摘要：本节主要总结ansiblerroles中常用的语法以及高级特性、例如变量、下载、解压、修改文件内容等,便于使用ansible协同开发，做更加庞大的任务。常用远程执行脚本设置某个参数供下文使用文件是否存在和变量是否声明解压下载修改文件内容，直接替换方式配置文件执行命令时指定脚本执行目录高级委托统一目录结构引用常用远程执行脚本脚本放到template里12345678910- name: init rbac rights template: src: 'init_rbac_privilege.sh' dest: '/tmp/init_rbac_privilege.sh' mode: 0755- name: init rbac rights shell: "bash /tmp/init_rbac_privilege.sh" run_once: true ignore_errors: yes脚本放files文件里12345678- name: copy script file to remote server copy: src: "generate_uuid.py" dest: "&#123;&#123; dest_script_file &#125;&#125;" mode: 0755- name: set to execute script file and args shell: "python &#123;&#123; dest_script_file &#125;&#125;"设置某个参数供下文使用123456789- name: set to execute script file path set_fact: dest_script_file: '/tmp/generate_uuid.py'- name: copy script file to remote server copy: src: "generate_uuid.py" dest: "&#123;&#123; dest_script_file &#125;&#125;" mode: 0755文件是否存在和变量是否声明12345678- stat: path: "&#123;&#123; dest_script_file &#125;&#125;" register: file_result- name: set to execute script file and args shell: "python &#123;&#123; dest_script_file &#125;&#125; &#123;&#123; inventory_hostname &#125;&#125;" register: p_new_host_uuid when: host_uuid is not defined and file_result.stat.exists解压解压tar.gz，desc所指定的目录需要提前创建12345- name: Unarchive ceph_report program package unarchive: remote_src: yes src: "/tmp/ceph_report/ceph_report.tar.gz" dest: "/data/monitorcloud/script/"下载12345- name: download ceph_report package get_url: url: "http://&#123;&#123; tstack_repo_address &#125;&#125;:&#123;&#123; tstack_repo_port &#125;&#125;/tstack/tstack-tars/ceph_report.tar.gz" dest: "/tmp/ceph_report/ceph_report.tar.gz" mode: 0644修改文件内容，直接替换方式1234567891011- name: insert mons connect message lineinfile: path: "/data/monitorcloud/script/ceph_report/host_dic" state: present insertafter: EOF backrefs: no line: "&#123;&#123;item[0]&#125;&#125; &#123;&#123;item[1]&#125;&#125;" with_nested: - "&#123;&#123; groups['mons'] | union( groups['osds'] )&#125;&#125;" - ["&#123;&#123;ansible_ssh_pass&#125;&#125;"] ignore_errors: true配置文件ansible中指定配置文件，.j2中可映射ansible变量12345- name: Prepare ceph_report_http program configuration file template: src: "opts.py.j2" dest: "/data/monitorcloud/script/ceph_report/opts.py" mode: 0644执行命令时指定脚本执行目录12345- name: execute ceph_report_http install script shell: "/usr/bin/python ceph_report_http.py install" args: chdir: '/data/monitorcloud/script/ceph_report/' ignore_errors: yes高级委托在当前运行ansible的机器上，委托其他机器运行123- name: add host record to center server shell: 'echo "192.168.1.100 test.xyz.com " &gt;&gt; /etc/hosts' delegate_to: 192.168.1.1也可以委托ansible服务端运行123- name: add host record to center server shell: 'echo hello' delegate_to: localhost统一目录结构1234567891011121314151617181920212223242526272829303132project/├── filter_plugins # 自定义 filter 插件存放目录├── fooapp # Fooapp 片色目录 ( 与 common 角色目录平级)├── group_vars │ ├── group1 # group1 自定义变量文件│ └── group2 # group2 自定义变量文件├── host_vars│ ├── hostname1 # hostname1 自定义变量文件│ └── hostname2 # hostname1 自定义变量文件├── library # 自定义模块存放目录├── monitoring # Monitoring 角色目录 ( 与 common 角色目录平级)├── roles # Role 存放目录│ └── common # common 角色目录│ ├── defaults │ │ └── main.yml # common 角色自定义文件 (优先级低)│ ├── files│ │ ├── bar.txt # common 角色 files 资源文件│ │ └── foo.sh # common 角色 files 资源文件│ ├── handlers│ │ └── main.yml # common 角色 handlers 入口文件│ ├── meta│ │ └── main.yml # common 角色 依赖文件│ ├── tasks│ │ └── main.yml # common 角色 task 入口文件│ ├── template│ │ └── ntp.conf.j2 # common 角色 template 文件│ └── vars│ └── main.yml # common 角色 变量定义文件├── site.yaml # Playbook 统一入口文件├── stage # stage 环境的 inventory 文件├── webservers.yml # 特殊 Playbook 文件└── webtier # webtier 角色目录 ( 与 common 角色目录平级)引用ansible进阶技巧 https://www.ibm.com/developerworks/cn/linux/1608_lih_ansible/index.htmlshilei ansible 文档 https://wiki.shileizcc.com/confluence/display/AN/Ansible骏马金龙 ansible系列文章 http://www.cnblogs.com/f-ck-need-u/p/7576137.html#ansible]]></content>
      <categories>
        <category>ansible</category>
      </categories>
      <tags>
        <tag>自动化</tag>
        <tag>部署</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进入自动化部署的大门-ansible]]></title>
    <url>%2Fansible%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[摘要：ansible是运维与实施人员的福音，其优雅的结构，丰富的模块库，简单的编程方式都让其成为自动化部署的不二语言。现在就让我们了解ansible,并使用ansible来做一些简单的任务一、基本概念1.1 如何安装及安装要求1.2 专有名词解释1.3 原理图1.4 连接机制1.5 常用文件及配置位置二、简单的例子2.1 配置被管控机器的连接信息2.2 执行命令2.3 讨论执行命令的四个模块有什么区别三、 完成复杂的ansible操作引用资源一、基本概念Ansible是一款开源软件，可自动执行软件供应，配置管理和应用程序部署。Ansible通过SSH，远程PowerShell或其他远程API连接。—-选自wikipedia简介：ansible有两种服务器类型，控制机器与节点。控制机器就是安装ansible服务的机器，我们在上面编写和运行ansible的程序代码，并在控制机器上通过ssh远程操作机器（下发上传、执行命令）。当ansible脚本不运行的时候，不会占用任何资源（比如saltstack在运行的时候就会启动进程来监控4505/4506端口），也正由于ansible的这个特点，每次ansible发布更新的时候，只需要更新控制机器就可以了。1.1 如何安装及安装要求要求：控制机是linux系统，需要有python2.6/2.7linux系统打开文件数量设置大一些（方法自行百度）节点要求，python2.4以上版本安装方法（有很多，在此只列出常用的方法，具体安装步骤不做赘述）：源码安装 git://github.com/ansible/ansible.gityum 方式 / apt 方式 / emerge 方式 或其他linux系统的系统安装方式pip 方式1.2 专有名词解释关键字释义playbook剧本，将要执行的步骤全部放到playbook里面modules（通常指core modules）核心模块，用于执行某些任务的已有内置插件roles角色，像演戏一样，剧本中指定在什么时候哪个角色来做什么操作，所以剧本包含角色，角色就有自己的台词，不过ansible里面的角色特殊的地方是，剧本只知识角色是干什么的，具体操作步骤是在角色本身管理（无厘头）custom modules自定义模块，在核心模块不够用的时候可以自定义编写模块plugins插件，常用于ansible的日志系统与邮件系统，还有的插件有扩展内置连接方式、扩展变量定义方式、扩展内部循环语法、扩展新的过滤器等InventoryAnsible 管理主机的清单1.3 原理图1.4 连接机制ansible通常使用的是ssh协议（或者Kerberos、LDAP）来进行连接（在openssh不支持的操作系统或ansible比较老的版本，ansible使用paramiko），ansible支持的连接方式有三种SSH、Local、ZeroMQ，在规模比较大的情况下使用ZeroMQ连接方式对执行速度有显著提高1.5 常用文件及配置位置文件位置ansible应用程序的主配置文件/etc/ansible/ansible.cfg定义管控主机/etc/ansible/hosts二、简单的例子这里我们来实现在所有被管控机器上执行hostname命令输出主机名2.1 配置被管控机器的连接信息备注1：在/etc/ansible/hosts位置写入一个主机组（分组名为test），以后针对这个分组操作就视为对分组内的所有主机操作。备注2：[test:vars] 标签下为test分组的变量，ansible_ssh_user和ansible_ssh_pass是ansible的内置变量，意思为该分组下所有主机的用户和和密码，当前了也可以单独指定某台主机只需要将这两个参数追加到ip的旁边就好。这里三台主机都是同一个用户名和密码，所以像这样配置。备注3：默认路径是在/etc/ansible/hosts,如果你不喜欢是可以修改的(修改配置文件/etc/ansible/ansible.cfg的inventory=/etc/ansible/hosts即可)2.2 执行命令执行命令，输出test分组下的所有主机的主机名，-i指定主机或分组,-m指定使用的模块，-a指定传给模块的参数，这里command模块就是执行linux命令的模块缺省时默认使用该模块，相同作用的还有shell模块、raw模块、script模块（使用场景与部分细节上不同，后面探讨）2.3 讨论执行命令的四个模块有什么区别每次在使用的时候都会有一些迷惑，为什么非要弄四个执行命令的模块出来，一开始决定没有必要，但是到后来才明白存在既合理，这里和大家探讨一下模块解释command模块是为了安全的执行linux命令，所以不支持`”&lt;”, “&gt;”, ““, 和 “&amp;”`等符号（没有shell注入风险），如果要一定要使用这些，则使用shell模块shell模块通过/bin/sh来执行，其他都和command一样raw模块用来执行低版本的linux命令，可以不需要python来执行命令，甚至支持windows命令，带来的问题是很多很特性是不能用的script模块其原理是先将脚本，复制到远程主机，再在远程主机上执行，所以要指定脚本路径以及操作方法结论：要安全用command，要方便用shell，要操作写好的shell脚本或者其他脚本就用script、要是以上操作都跑不了的机器就用raw三、 完成复杂的ansible操作未完待续–引用Ansible插件扩展 https://blog.csdn.net/yongchaocsdn/article/details/79271870资源插件 https://github.com/ansible/ansible/tree/stable-2.4/lib/ansible/plugins]]></content>
      <categories>
        <category>ansible</category>
      </categories>
      <tags>
        <tag>自动化</tag>
        <tag>部署</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置像我一样的hexo博客]]></title>
    <url>%2F%E9%85%8D%E7%BD%AE%E5%83%8F%E6%88%91%E4%B8%80%E6%A0%B7%E7%9A%84hexo%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[摘要：csdn和博客园虽然用的人多、技术氛围好，但是由于csdn的广告和灌水以及博客园的简陋页面让我决定搭建一套属于自己的博客，此处用到比较火的hexo搭建，而且免费就能获得自己的网站，如果不买域名的话完全够用了一、搭建本地 hexo1.1 安装nodejs1.2 安装git1.3 使用npm安装Hexo1.4 创建本地服务1.5 主题配置二、 配置你的github2.1 创建一个新项目2.2 填写正确的项目名称2.3 配置你的项目三、激动人心的博客3.1 修改hexo配置文件3.2 部署成功四、个性化配置你的博客4.1 配置博客名，博客描述等信息4.2 添加头像4.3 添加日志列表4.2 其他五、配置你的hexo可以插入图片附：加速npm引用一、搭建本地 hexo1.1 安装nodejs到nodejs官网下载安装 http://nodejs.cn/1.2 安装git到git官网下载安装 https://git-scm.com/downloads1.3 使用npm安装Hexo1npm install -g hexo-cli备注： 如果npm下载比较慢可以跳到 附：加速npm1.4 创建本地服务创建一个文件夹，此处为blog，执行以下命令1234hexo i blog //init的缩写 blog是项目名cd blog //切换到站点根目录hexo g //generetor的缩写hexo s //server的缩写访问本地服务localhost:4000证明成功,为什么和我的不一样，因为我修改了hexo的主题配置可以跳到 四、个性化配置你的博客有页面出来证明你的配置已经成功了，原始的主题不是很喜欢，我这里也使用了最火的nexT主题1.5 主题配置在站点根目录输入1git clone https://github.com/iissnan/hexo-theme-next themes/next完成后，打开根目录下的_config.yml， 找到 theme 字段，并将其值更改为 nextnext有三种主题，我选择的是双栏 Scheme，找到 站点根目录/themes/next/_congig.yml 文件，再找到schme字段，将其值改为Pisces,其他两种可以参考引用里所写的使配置生效123hexo clean //清除缓存hexo g //重新生成代码hexo s //部署到本地这样你的会有一套和我一样主题的博客了，如果你不喜欢这套主题，或者想自己来开发一套定制自己的主题参考官方文档,下面让我们把他推到github上，变成独一无二的网站二、 配置你的github2.1 创建一个新项目如果没有注册github帐号到&gt;官网注册点此创建项目（如果你不是程序员建议勾选上图中Initalize this repository with a README选项）：2.2 填写正确的项目名称项目名可以为任意英文.github.io，public设置为公开项目,点击绿色创建按钮2.3 配置你的项目在项目里创建一个readme.md即可，这下可以在页面上访问到这个免费的网站了我的网站 https://pzqu.github.io/三、激动人心的博客3.1 修改hexo配置文件修改根目录的配置文件_config.yml，以下部分如果没有需要手动创建，建议直接拷贝我的，repo为自己刚刚创建的那个项目的git链接（注意冒号与值之间必须有空格）1234deploy: type: git repo: https://github.com/pzqu/pzqu.github.io.git branch: master3.2 部署成功直接执行一套命令,再刷新刚刚的域名即可12npm install hexo-deployer-git --savehexo d // 部署的命令,会生成代码并推送到github上去四、个性化配置你的博客4.1 配置博客名，博客描述等信息参考官方网站 https://hexo.io/zh-cn/docs/configuration.html4.2 添加头像在主题配置文件里修改avatar: images/avatar.gif4.3 添加日志列表4.2 其他添加顶部加载条,修改主题配置文件，pace_theme有好几款，自己选一款12pace: truepace_theme: pace-theme-flash添加打赏功能，修改主题配置文件，图片可以上传到themes\next\source\images123reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！wechatpay: /images/wxpay.jpgalipay: /images/alipay.jpg文章阅读量参考 https://www.jianshu.com/p/702a7aec4d00评论系统http://www.zhaojun.im/hexo-valine/删除底部强力驱动、统计站点网上自己查添加分享添加评论五、配置你的hexo可以插入图片把主页配置文件 _config.yml 里的post_asset_folder:这个选项设置为true在你的hexo目录下执行npm install hexo-asset-image --save等待一小段时间后，再运行hexo n &quot;配置像我一样的hexo博客&quot;来生成md博文时，/source/_posts文件夹内除了配置像我一样的hexo博客.md文件还有一个同名的文件夹最后在配置像我一样的hexo博客.md中想引入图片时，先把图片复制到配置像我一样的hexo博客这个文件夹中，然后只需要在配置像我一样的hexo博客.md中按照markdown的格式引入图片,注意使用相对路径：![你想输入的替代文字](配置像我一样的hexo博客/图片名.jpg)附：加速npm配置npm镜像npm config set registry https://registry.npm.taobao.org查看npm镜像npm config get registry引用用Hexo + github搭建自己的博客 — 再也不用羡慕别人了！npm太慢， 淘宝npm镜像使用方法]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>建站教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq集群的各种运维操作]]></title>
    <url>%2Ffirst-blog%2F</url>
    <content type="text"><![CDATA[摘要：在rabbitmq集群操作或者搭建的时候，常常会因为对于集群的不熟练而导致各种异常错误，常见的有绑定了浮动ip没有绑定实体ip导致，页面上操作mq完全没有问题，但是一到程序操作就出现问题，我们一起来学习下，rabbitmq集群的正确操作一、rabbitmq集群必要条件1.1. 绑定实体ip，即ifconfig所能查询到的绑定到网卡上的ip,以下是绑定方法1.2. 配置域名映射到实体ip二、启动停止2.1 停止2.2 启动三、重建集群3.1 使用2.1方法停止所有机器3.2 移除rabbitmq配置记录与存储文件3.3 按2.2方法启动所有机器3.4 停止被加入集群节点app3.5 建立集群3.6 启动集群3.7 检查集群状态3.8 添加集群配置 （见第四）四 添加集群配置4.1 创建用户4.2 打开15672网页管理端，访问mq4.3 在底部导入.json后缀的配置文件即可以下操作都以三节点集群为例，机器名标记为机器A、机器B、机器C，如果为双节点忽略机器C，如果为各多节点则与机器C操作相同一、rabbitmq集群必要条件1.1. 绑定实体ip，即ifconfig所能查询到的绑定到网卡上的ip,以下是绑定方法12#编辑配置路径 /etc/rabbitmq/rabbitmq-env.confNODE_IP_ADDRESS=172.16.136.1331.2. 配置域名映射到实体ip123456789101112131415161718192021#配置文件1所在路径 /etc/rabbitmq/rabbitmq.config (如果是集群，每台机器都需要修改这个绑定本机实体ip)#其中rabbit@master是创建集群时所配置的参数，@后面的参数为主机名，示例中为master[ &#123;rabbit, [ &#123;cluster_nodes, &#123;[&apos;rabbit@master&apos;], disc&#125;&#125;, &#123;cluster_partition_handling, ignore&#125;, &#123;default_user, &lt;&lt;&quot;guest&quot;&gt;&gt;&#125;, &#123;default_pass, &lt;&lt;&quot;guest&quot;&gt;&gt;&#125;, &#123;tcp_listen_options, [binary, &#123;packet, raw&#125;, &#123;reuseaddr, true&#125;, &#123;backlog, 128&#125;, &#123;nodelay, true&#125;, &#123;exit_on_close, false&#125;, &#123;keepalive, true&#125;]&#125; ]&#125;, &#123;kernel, [ &#123;inet_dist_listen_max, 44001&#125;, &#123;inet_dist_listen_min, 44001&#125; ]&#125;].1234#配置文件2 所在路径 /etc/hosts (如果是集群，每台机器都需要修改这个绑定本机实体ip，而且hosts文件的映射不得重复，如果重复linux系统为以最下面一条记录为准)172.16.136.133 master172.16.136.134 venus172.16.136.135 venus2二、启动停止2.1 停止123456789#机器Aservice rabbitmq-server stopepmd -kill#机器Bservice rabbitmq-server stopepmd -kill#机器Cservice rabbitmq-server stopepmd -kill2.2 启动123456#机器Aservice rabbitmq-server start#机器Bservice rabbitmq-server start#机器Cservice rabbitmq-server start三、重建集群注1：此处的mq集群重建是比较快速和有效的方法，面向的是初次安装或者可以接受mq中所存有的数据丢失的情况下，必须先有mq的.json后缀的配置文件或者有把握写入集群中exchange、queue等配置。注2:如果是运行中的mq出现问题，需要在保存数据和配置的情况下恢复集群时，请跳到rabbitmq集群恢复3.1 使用2.1方法停止所有机器3.2 移除rabbitmq配置记录与存储文件12#位于 /var/lib/rabbitmq/mensiamv /var/lib/rabbitmq/mensia /var/lib/rabbitmq/mensia.bak3.3 按2.2方法启动所有机器3.4 停止被加入集群节点app比如A、B、C三台机器，将B和C加入到A中去，需要执行以下命令1234#机器Brabbitmqctl stop_app#机器Crabbitmqctl stop_app3.5 建立集群注意此处master为唯一没有执行rabbitmqctl stop_app的机器1234#机器Brabbitmqctl join_cluster rabbit@master#机器Crabbitmqctl join_cluster rabbit@master3.6 启动集群1234#机器Brabbitmqctl start_app#机器Crabbitmqctl start_app3.7 检查集群状态在任意一台机器上执行rabbitmqctl cluster_status命令即可检查，输出包含集群中的节点与运行中的节点，兼以主机名标志3.8 添加集群配置 （见第四）四 添加集群配置4.1 创建用户例子中创建了两个用户添加用户add_user,设置角色set_user_tags,添加rabbitmq虚拟主机add_vhost，设置访问权限set_permissions,以下是详细用法123456789# 创建第一个用户/usr/sbin/rabbitmqctl add_user 用户名 密码/usr/sbin/rabbitmqctl set_user_tags 用户名 administrator/usr/sbin/rabbitmqctl set_permissions -p / 用户名 ".*" ".*" ".*"# 创建第二个用户/usr/sbin/rabbitmqctl add_user 用户名2 密码/usr/sbin/rabbitmqctl set_user_tags 用户名2 management /usr/sbin/rabbitmqctl add_vhost sip_ext /usr/sbin/rabbitmqctl set_permissions -p sip_ext 用户名2 '.*' '.*' '.*'备注：RabbitMQ 虚拟主机，RabbitMQ 通过虚拟主机（vhost）来分发消息。拥有自己独立的权限控制，不同的vhost之间是隔离的，单独的。 权限控制的基本单位：vhost。 用户只能访问与之绑定的vhost。 vhost是AMQP中唯一无法通过协议来创建的基元。只能通过rabbitmqctl工具来创建。 4.2 打开15672网页管理端，访问mq/usr/sbin/rabbitmq-plugins enable rabbitmq_management备注：如果发现命令执行完毕没有打开此服务，15672端口没有监听，则是由于没有重启mq导致的4.3 在底部导入.json后缀的配置文件即可如果覆盖了用户需要使用以下命令修改mq用户密码/usr/sbin/rabbitmqctl change_password 用户名 密码]]></content>
      <categories>
        <category>rabbitmq</category>
      </categories>
      <tags>
        <tag>rabbitmq集群</tag>
        <tag>rabbitmq教程</tag>
      </tags>
  </entry>
</search>
